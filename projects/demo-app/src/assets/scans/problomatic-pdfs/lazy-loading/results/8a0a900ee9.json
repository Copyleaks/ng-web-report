{"statistics":{"identical":95,"minorChanges":0,"relatedMeaning":50},"text":{"comparison":{"identical":{"source":{"chars":{"starts":[51396,56292,59641,74370,77595,78977,80105],"lengths":[27,25,23,23,23,31,31]},"words":{"starts":[8511,9545,10298,13735,14573,14914,15144],"lengths":[13,12,11,11,11,15,15]}},"suspected":{"chars":{"starts":[5361,5751,5787,6015,6244,6333,6707],"lengths":[27,25,23,23,23,31,31]},"words":{"starts":[854,982,1000,1071,1144,1182,1268],"lengths":[13,12,11,11,11,15,15]}}},"minorChanges":{"source":{"chars":{"starts":[],"lengths":[]},"words":{"starts":[],"lengths":[]}},"suspected":{"chars":{"starts":[],"lengths":[]},"words":{"starts":[],"lengths":[]}}},"relatedMeaning":{"source":{"chars":{"starts":[62669,62807],"lengths":[137,83]},"words":{"starts":[11002,11029],"lengths":[26,22]}},"suspected":{"chars":{"starts":[5405,6451],"lengths":[83,74]},"words":{"starts":[875,1220],"lengths":[30,14]}}}},"value":"Voronoi tessellation-based lifting\nscheme in bounded regions\n\nFatih Gezer\nDepartment of Statistics\nUniversity of Leeds\n\nSubmitted in accordance with the requirements for the degree of\nDoctor of Philosophy\nDecember 2021\n\nDeclaration\nThe candidate confirms that the work submitted is his own, except\nwhere work which has formed part of jointly authored publications has\nbeen included. The contribution of the candidate and the other authors\nto this work has been explicitly indicated below. The candidate confirms\nthat appropriate credit has been given within the thesis where reference\nhas been made to the work of others.\nThe findings in Chapter 2 of the thesis are published as Gezer, F.,\nAykroyd, R. G., & Barber, S. (2021). “Statistical properties of PoissonVoronoi tessellation cells in bounded regions”. Journal of Statistical\nComputation and Simulation, 91(5), 915-933.\nThe motivation of the investigation of Poisson-Voronoi tessellation cells\nand their statistical properties in bounded regions come from discussions\nwith Stuart Barber and Robert Aykroyd. Fatih Gezer (1) conducted\nthe simulation study, (2) demonstrated the differences in the statistical\nproperties of Voronoi cells for the infinite plane, unit square, and convex hull boundary cases, and (3) approximated the distributions of cell\nproperties by parametric distributions.\nThis copy has been supplied on the understanding that it is copyright\nmaterial and that no quotation from the thesis may be published without\nproper acknowledgement.\nThe right of Fatih Gezer to be identified as Author of this work has been\nasserted by Fatih Gezer in accordance with the Copyright, Designs and\nPatents Act 1988.\n\nTo my mother and father, for their endless and unconditional support...\n\nAcknowledgements\nI would like to express my sincere gratitude and appreciation to my\nsupervisors Dr. Stuart Barber and Dr. Robert G. Aykroyd for their\ncontinuous support and guidance with their immense knowledge during\nmy PhD. I am grateful for their kindness and approachability whenever\nI needed their help. It was an exceptional experience for me to work\nwith them. I am also grateful for the scholarship from the Ministry of\nNational Education, Republic of Turkey during my PhD.\nI would like to thank Dr. John Paul Gosling and Prof. Charles Taylor\nfor their useful comments at my annual reviews, and Dr. Matthew\nAldridge and Prof. Janine Illian for being my PhD viva examiners. I\nam also thankful to Prof. Peter Diggle for providing suggestions for a\nfuture use of the methods we devised. Special thanks to all my PGR\nfriends who made this journey meaningful and enjoyable. I would also\nlike to thank the staff of the School of Mathematics at the University of\nLeeds for making things easier and quick for us.\nLast but not least, I would like to thank my mother Saliha, my father\nBayram and my sisters Betül and Tuba, and all my family members\nand friends for their unconditional and emotional support during my\n7.5 years being far from them. Despite the distances, I always felt their\npresence as they are beside me. Finally, I would like to thank my wife\nAyşenur who joined me at the middle of this journey and gave me an\nincredible support. Words are not enough to describe her kindness and\npatience. I will always be in debt to her.\n\nAbstract\nWe study the Voronoi tessellation-based lifting scheme in two-dimensional\nregions where the spatial data is available in a finite and bounded twodimensional region. The lifting scheme is a second-generation wavelet\nmethod that is used for the analysis of spatial data which we model\nas being an underlying ‘true’ surface corrupted by noise. On the other\nhand, Voronoi tessellation is a standard technique to partition the space\ninto smaller sub-regions called Voronoi cells that are used as an ingredient in the lifting scheme.\nWe investigate the statistical properties of Voronoi cells for homogeneous\nPoisson points in the infinite plane and bounded regions. The properties\nare the cell area, perimeter, and the number of cell edges. Our findings\nshow that the distributions of cell properties differ substantially when\nboundaries are imposed. These differences are affected by proximity.\nWe emphasize the consequences of the boundaries on the Voronoi cells,\nand we devise a method that treats the spatial data in the finite region\nas if it is a subset of a larger region or an infinite plane. This approach\npredicts the true cell area that is actually clipped by a boundary line\nusing regression-based models. The models are updated for general data\ncases, and have an overall promising performance.\nLifting scheme uses the features of Voronoi tessellation and the information obtained from the Voronoi cells. The ultimate goal of this thesis\nis to implement the devised method, which adjusts the cell area near\nboundaries, into the lifting scheme framework and compare its performance to the standard approaches. Various configurations are considered; standard and proposed weight methods, noisy test functions with\ndifferent spatial characteristics, and randomly distributed, regular, and\nclustered point patterns. The proposed approach over-perform the existing options and even gives better performance over the standard spatial\nprediction techniques such as kriging in certain cases.\n\nContents\n1 Introduction\n\n1\n\n1.1\n\nOverview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n1\n\n1.2\n1.3\n\nVoronoi tessellation . . . . . . . . . . . . . . . . . . . . . . . . . . .\nA motivating example for boundaries . . . . . . . . . . . . . . . . .\n\n2\n2\n\n1.4\n\nSpatial point patterns and Poisson point process . . . . . . . . . . .\n\n4\n\n1.5\n\nRegular and clustered points . . . . . . . . . . . . . . . . . . . . . .\n\n6\n\n1.6\n1.7\n\nLifting scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\nThesis structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n7\n8\n\n2 Statistical properties of Voronoi tessellations in bounded regions 12\n2.1\n\nObjective of of the study . . . . . . . . . . . . . . . . . . . . . . . .\n\n12\n\n2.2\n\nVoronoi and Poisson Voronoi tessellation . . . . . . . . . . . . . . .\n\n13\n\n2.3\n2.4\n\nBackground and previous work . . . . . . . . . . . . . . . . . . . .\nDesign of the simulation . . . . . . . . . . . . . . . . . . . . . . . .\n\n14\n19\n\n2.5\n\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n20\n\n2.5.1\n\nVoronoi tessellation in the infinite plane . . . . . . . . . . .\n\n20\n\n2.5.2\n2.5.3\n\nVoronoi tessellation using unit square boundary . . . . . . .\nVoronoi tessellation using convex hull boundary . . . . . . .\n\n23\n25\n\n2.6\n\nComparisons of different boundary cases and the previous work . .\n\n28\n\n2.7\n\nPVT for different intensities . . . . . . . . . . . . . . . . . . . . . .\n\n41\n\n2.8\n\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n43\n\n3 Prediction of Voronoi tessellation cell area\n3.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n45\n45\n\n3.2\n\nBoundary issues . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n46\n\n3.3\n\nDescription of variables . . . . . . . . . . . . . . . . . . . . . . . . .\n\n47\n\n3.4\n\nArea prediction for Voronoi tessellation cells . . . . . . . . . . . . .\n3.4.1 The generalized additive model . . . . . . . . . . . . . . . .\n\n49\n50\n\n3.4.2\n\n53\n\nStudy design . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nv\n\nCONTENTS\n\n3.4.2.1\n\nDescription of training data . . . . . . . . . . . . .\n\n53\n\n3.4.2.2\n\nDescription of validation–1 data . . . . . . . . . . .\n\n55\n\n3.4.2.3\n3.4.2.4\n\nInfluential points . . . . . . . . . . . . . . . . . . .\nDescription of validation–2 data . . . . . . . . . . .\n\n55\n56\n\nResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n57\n\n3.5.1\n\nUnit square boundary case . . . . . . . . . . . . . . . . . . .\n\n57\n\n3.5.1.1\n3.5.1.2\n\nTraining base models . . . . . . . . . . . . . . . . .\nTraining augmented models . . . . . . . . . . . . .\n\n57\n59\n\nUnknown Boundary case . . . . . . . . . . . . . . . . . . . .\n\n71\n\n3.6\n\nClassification of boundary-affected points . . . . . . . . . . . . . . .\n\n78\n\n3.7\n3.8\n\nAlternative data scenarios . . . . . . . . . . . . . . . . . . . . . . .\nConclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n79\n81\n\n3.5\n\n3.5.2\n\n4 Robustness of area prediction\n\n82\n\n4.1\n\nMisspecification of intensity . . . . . . . . . . . . . . . . . . . . . .\n\n82\n\n4.2\n\nRegular and clustered point patterns . . . . . . . . . . . . . . . . .\n\n85\n\n4.3\n\nThe prediction of Voronoi cell area based on regular and clustered\npoints . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n86\n\n4.3.1\n\nResults for simulated data . . . . . . . . . . . . . . . . . . .\n\n89\n\n4.3.2\n\nResults for real data . . . . . . . . . . . . . . . . . . . . . .\n\n90\n\nConclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n98\n\n4.4\n\n5 Lifting scheme\n99\n5.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n5.2\n\nDiscrete wavelet transform . . . . . . . . . . . . . . . . . . . . . . . 100\n\n5.3\n\nLifting in two dimensions . . . . . . . . . . . . . . . . . . . . . . . . 102\n\n5.4\n\n5.3.1\n5.3.2\n\nSteps of the lifting transform . . . . . . . . . . . . . . . . . . 103\nMethods of prediction . . . . . . . . . . . . . . . . . . . . . 106\n\n5.3.3\n\nDerivation of transform matrix . . . . . . . . . . . . . . . . 107\n\n5.3.4\n\nImplementation of 2D lifting in R . . . . . . . . . . . . . . . 111\n\nShrinkage in lifting . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n5.4.1 Hard and soft thresholding . . . . . . . . . . . . . . . . . . . 113\n5.4.2\n\n5.5\n\nEmpirical Bayesian thresholding . . . . . . . . . . . . . . . . 114\n\nExample . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116\n\n6 Lifting results for homogeneous data\n122\n6.1 Test functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 122\n6.2\n\nWeight methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\n\nvi\n\nCONTENTS\n\n6.3\n\nDesign of the simulation . . . . . . . . . . . . . . . . . . . . . . . . 124\n\n6.4\n\nResults for simulated homogeneous data . . . . . . . . . . . . . . . 125\n\n6.5\n\n6.4.1\n6.4.2\n\nDoppler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\nHeavisine . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n\n6.4.3\n\nBlocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131\n\n6.4.4\n\nBumps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\n\n6.4.5 Maartenfunc . . . . . . . . . . . . . . . . . . . . . . . . . . . 133\nConclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n\n7 Lifting results for regular, clustered and real data examples\n\n137\n\n7.1\n\nLifting for regular and clustered data . . . . . . . . . . . . . . . . . 137\n\n7.2\n\nResults for simulated data . . . . . . . . . . . . . . . . . . . . . . . 138\n7.2.1\n7.2.2\n\nDoppler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\nHeavisine . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n\n7.2.3\n\nBlocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 142\n\n7.2.4\n\nBumps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144\n\n7.3\n\n7.2.5 Maartenfunc . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\nComparison of lifting estimates with kriging . . . . . . . . . . . . . 146\n\n7.4\n\nReal data application of lifting . . . . . . . . . . . . . . . . . . . . . 149\n\n7.5\n\nConclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n\n8 Discussion\n\n156\n\nA Extra plots and tables\n\n160\n\nB Test functions and R Codes\n162\nB.1 Test functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 162\nB.2 Example code for statistical properties of Poisson Voronoi cells . . . 165\nC Tables of MSE values for regular and clustered data\n\n170\n\nReferences\n\n184\n\nvii\n\nList of Figures\n1.1\n\nChanges of the Voronoi cell shapes when a boundary is imposed . .\n\n3\n\n2.1\n\nExamples of Voronoi tessellation of points . . . . . . . . . . . . . .\n\n14\n\n2.2\n\nPoint shifting example . . . . . . . . . . . . . . . . . . . . . . . . .\n\n19\n\n2.3\n2.4\n\nVoronoi tessellation of points using boundaries . . . . . . . . . . . .\nHistogram and surface plot of infinite plane cell area . . . . . . . .\n\n20\n21\n\n2.5\n\nHistogram and surface plot of infinite plane cell perimeter . . . . .\n\n22\n\n2.6\n\nHistogram and surface plot of infinite plane cell edges . . . . . . . .\n\n22\n\n2.7\n2.8\n\nTransect used in the line plots . . . . . . . . . . . . . . . . . . . . .\nSurface and line plots of unit square cell area . . . . . . . . . . . . .\n\n24\n24\n\n2.9\n\nSurface and line plots of unit square cell perimeter . . . . . . . . . .\n\n25\n\n2.10 Surface and line plots of unit square cell edges . . . . . . . . . . . .\n\n26\n\n2.11 Surface and line plots of convex hull cell area . . . . . . . . . . . . .\n2.12 Surface and line plots of convex hull cell perimeter . . . . . . . . . .\n\n26\n27\n\n2.13 Surface and line plots of convex hull cell edges . . . . . . . . . . . .\n\n27\n\n2.14 Estimated density lines for gamma distribution . . . . . . . . . . .\n\n30\n\n2.15 Estimated gamma density lines of cell area based on the number of\nedges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n33\n\n2.16 Estimated gamma density lines of cell perimeter based on the number\nof edges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n34\n\n2.17 Histograms and surface plots of area reduction . . . . . . . . . . . .\n2.18 Histograms and surface plots of perimeter reduction . . . . . . . . .\n\n35\n36\n\n2.19 Histograms and surface plots of edge reduction . . . . . . . . . . . .\n\n37\n\n2.20 Histograms and surface plots of area ratio . . . . . . . . . . . . . .\n\n38\n\n2.21 Histograms and surface plots of perimeter ratio . . . . . . . . . . .\n2.22 Histograms and surface plots of edge ratio . . . . . . . . . . . . . .\n\n39\n40\n\n2.23 Histogram and fitted density lines for area reduction\n\n. . . . . . . .\n\n40\n\n2.24 Transect line plots of cell properties for different intensities . . . . .\n\n42\n\n2.25 Proportion of boundary-affected cells . . . . . . . . . . . . . . . . .\n\n43\n\nviii\n\nLIST OF FIGURES\n\n3.1\n\nScatterplots of selected variables . . . . . . . . . . . . . . . . . . . .\n\n49\n\n3.2\n\nBox plots of cell area based on cell type and number of cell edges .\n\n50\n\n3.3\n\nVariables and interaction terms, and how many times selected in the\nbase models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n58\n\n3.4\n3.5\n3.6\n3.7\n3.8\n3.9\n\nEstimated smooth components of the GAMs in the individual base\nand augmented models . . . . . . . . . . . . . . . . . . . . . . . . .\n\n59\n\nThe normal quantile-quantile plot of residuals versus fitted values in\nbase models in gray lines, and the averaged values as the black line.\n\n60\n\nIndex of the influential points and how many times they are identified\nas influential. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n61\n\nSelected variables in the unit square boundary models and the number of times each term is selected . . . . . . . . . . . . . . . . . . .\n\n64\n\nThe normal quantile-quantile plot of residuals versus fitted values in\naugmented models . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n64\n\nMSE image plots for base and augmented models, and observed areas\nusing unit square boundary . . . . . . . . . . . . . . . . . . . . . .\n\n66\n\n3.10 The boxplots of MSE for predictions from individual base models\n(blue) and augmented models (red) and observed areas using unit\nsquare boundary . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n3.11 Mean error image plots for base and augmented models, and observed\n\n68\n\nareas using unit square boundary . . . . . . . . . . . . . . . . . . .\n\n69\n\n3.12 The boxplots of mean error for predictions from individual base models (blue) and augmented models (red) and observed areas using unit\nsquare boundary . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n70\n\n3.13 Selected variables in the unknown boundary models and the number\nof times each term is selected . . . . . . . . . . . . . . . . . . . . .\n\n71\n\n3.14 Index of the influential points and how many times they are identified\nas influential. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n72\n\n3.15 Estimated smooth components of the GAMs in the individual base\nand augmented models . . . . . . . . . . . . . . . . . . . . . . . . .\n\n73\n\n3.16 The normal quantile-quantile plot of residuals versus fitted values for\nindividual base (left) and augmented models (right). . . . . . . . . .\n\n73\n\n3.17 MSE image plots for base and augmented models, and observed areas\nfor unknown boundary . . . . . . . . . . . . . . . . . . . . . . . . .\n\n74\n\n3.18 The boxplots of MSE for predictions from individual base models\n(blue) and augmented models (red) and observed areas for unknown\nboundary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\nix\n\n75\n\nLIST OF FIGURES\n\n3.19 Mean error image plots for base and augmented models, and observed\nareas for unknown boundary . . . . . . . . . . . . . . . . . . . . . .\n\n76\n\n3.20 The boxplots of mean error for predictions from individual base models (blue) and augmented models (red) and observed areas for unknown boundary . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n77\n\n3.21 ROC curve created from the confusion matrices for the test data. .\n\n80\n\n4.1\n\nSimulated points from Geyer’s saturation process based on different\nparameter values . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n87\n\n4.2\n\nKernel smoothed intensity of the point patterns. . . . . . . . . . . .\n\n88\n\n4.3\n4.4\n\nConfidence intervals for MSE values . . . . . . . . . . . . . . . . . .\nLocations of the data points in the real data sets . . . . . . . . . . .\n\n92\n95\n\n4.5\n\nRipley’s K function plots for simulated and real data sets . . . . . .\n\n95\n\n?\n\n4.6\n\nThe adjustment pattern on the cell area using base B models . . .\n\n96\n\n4.7\n\nThe adjustment pattern on the cell area using augmented Ag ? models 97\n\n5.1\n\nAn illustration of the neighbourhood structure of a selected point\n\n5.2\n\nand its neighbours . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\nAn illustration of the neighbourhood structure of a selected point\nand the change in the cells of the neighbours the selected point is\nremoved . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n\n5.3\n\nAn illustration of the calculation of weights based on partitioned cell\nof the removed point. . . . . . . . . . . . . . . . . . . . . . . . . . . 107\n\n5.4\n\nVoronoi tessellation of a set of uniform random points with and without boundaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n\n5.5\n\nVoronoi tessellation of uniform random points and noisy function\nvalues at the locations . . . . . . . . . . . . . . . . . . . . . . . . . 117\n\n5.6\n\nZoomed in plot of Voronoi tessellation of the lifted point and its\nneighbours, and partition of its cell . . . . . . . . . . . . . . . . . . 118\n\n5.7\n5.8\n\nProgression of the lifting transform . . . . . . . . . . . . . . . . . . 120\nDetail coefficients, and estimated function values . . . . . . . . . . . 120\n\n6.1\n\nTest functions: (a) Doppler, (b) Heavisine, (c) Blocks, (d) Bumps,\n\n6.2\n\n(e) Maartenfunc. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123\nZoomed in bottom left corner of the unit square divided into a 50×50\ngrid of square bins, showing how the points fall into the first few. . 126\n\n6.3\n\nMSE line plots for the Doppler test function at different transects . 128\n\n6.4\n\nMSE line plots for the Heavisine test function at different transects\n\nx\n\n130\n\nLIST OF FIGURES\n\n6.5\n\nMSE line plots for the Blocks test function at different transects . . 131\n\n6.6\n\nMSE line plots for the Bumps test function at different transects . . 134\n\n6.7\n\nMSE line plots for the Maartenfunc test function at different transects135\n\n7.1\n\nLifting MSE results for Doppler test function . . . . . . . . . . . . . 140\n\n7.2\n\nLifting MSE results for Heavisine test function . . . . . . . . . . . . 142\n\n7.3\n7.4\n\nLifting MSE results for Blocks test function . . . . . . . . . . . . . 143\nLifting MSE results for Bumps test function . . . . . . . . . . . . . 145\n\n7.5\n\nLifting MSE results for Maartenfunc test function . . . . . . . . . . 146\n\n7.6\n\nLifting results for real data sets . . . . . . . . . . . . . . . . . . . . 151\n\nA.1 Estimated density lines for Gamma, Weibull and log-normal distributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nA.2 Selected variables in the unit square boundary models and the number of times each term is selected . . . . . . . . . . . . . . . . . . . 161\n\nxi\n\nList of Tables\n2.1\n2.2\n\nEstimated gamma parameters of cell area in Tanemura (2003) . . .\nEstimated gamma parameters of cell area in Koufos & Dettmann\n\n17\n\n(2019) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n18\n\n2.3\n\nNumber of cell edges in the infinite plane and the occurrences observed. 23\n\n2.4\n2.5\n\nSummary statistics for cell properties . . . . . . . . . . . . . . . . .\nParameter estimations for two-parameter gamma distribution . . .\n\n28\n31\n\n2.6\n\nParameter estimations for three-parameter gamma distribution . . .\n\n32\n\n3.1\n3.2\n\nA list of the variables used in the modeling . . . . . . . . . . . . . .\nSample sizes of training, and validation sets. The numbers refer to\n\n48\n\nthe number of randomly sampled cells from independent realisations.\n\n53\n\n3.3\n\nSummary statistics of the variables in the validation data. The first\nrow for each variable panel is the results for all points and the second\nrow for the influential points coloured in blue. . . . . . . . . . . . .\n\n3.4\n\n62\n\nProportion of the cell types (0: interior, 2: edge, 3: corner, 4: corner+) for all points, and the influential points.\n\n. . . . . . . . . . .\n\n63\n\nProportion of the cells located on the convex hull (0: No, 1: Yes) for\nall points, and the influential points. . . . . . . . . . . . . . . . . .\n\n63\n\n3.6\n\nMSE for full and reduced models . . . . . . . . . . . . . . . . . . .\n\n67\n\n3.7\n\nThe confusion matrix table. . . . . . . . . . . . . . . . . . . . . . .\n\n79\n\n4.1\n\nMSE of area prediction for base and augmented models for misspec-\n\n3.5\n\n4.2\n4.3\n\nified point intensities . . . . . . . . . . . . . . . . . . . . . . . . . .\n\n84\n\nMSE of area prediction using the weighted average of the lower and\nhigher intensity models . . . . . . . . . . . . . . . . . . . . . . . . .\n\n85\n\nMSE of the predicted area using base B, augmented Ag, and updated\nB ? and Ag ? models based on different γ\n\n4.4\n\n. . . . . . . . . . . . . . .\n\n91\n\nStandard error of the MSE values from Table 4.3 . . . . . . . . . .\n\n91\n\nxii\n\nLIST OF TABLES\n\n4.5\n\nData set name, number of points n, estimated parameter γ̂, sampling\nregion Ω, and the description of the data sets. . . . . . . . . . . . .\n\n93\n\n6.1\n\nTable of MSE values for the Doppler test function at different transects129\n\n6.2\n\nTable of MSE values for the Heavisine test function at different transects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n\n6.3\n6.4\n\nTable of MSE values for the Blocks test function at different transects132\nTable of MSE values for the Bumps test function at different transects134\n\n6.5\n\nTable of MSE values for the Maartenfunc test function at different\ntransects . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 135\n\n7.1\n\nMSE and SE values for lifting estimates using Ag ? and kriging . . . 149\n\nC.1 MSE for the lifting estimations for regular and clustered points when\nγ = 0, 0.25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 171\nC.2 MSE for the lifting estimations for regular and clustered points when\nγ = 0.5, 0.75 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172\nC.3 MSE for the lifting estimations for regular and clustered points when\nγ = 1, 1.25 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 173\nC.4 MSE for the lifting estimations for regular and clustered points when\nγ = 1.5, 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174\nC.5 MSE for the lifting estimations for regular and clustered points when\nγ = 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 175\n\nxiii\n\nChapter 1\nIntroduction\n1.1\n\nOverview\n\nIn this thesis, we study Voronoi tessellations and the lifting scheme, and how these\ntwo topics combine in situations where the estimation of an underlying function\nfrom noisy spatial data is disrupted by artificially imposed boundaries. The idea\nof Voronoi tessellation is the division of the space into smaller sub-regions called\nVoronoi cells, and the lifting scheme is used for denoising irregularly spaced data in\nmultidimensions. The focus of this thesis is to study the Voronoi tessellation-based\nlifting scheme in two-dimensional space, and investigate what happens if the infinite\nplane is disrupted by a boundary.\nWhen the spatial data is constrained by the boundaries, there are certain limiting\ncircumstances since the boundary act as a cutoff point of the data. Boundaries\nalso change the shapes of the Voronoi cells. Hence, it is important to investigate\nand understand the effects of the boundaries on the Voronoi cells. Furthermore,\nwe propose methods that behave as if there is an infinite plane which the bounded\nregion is a subset of. For the spatial data observed within a bounded region, this\napproach has an implicit assumption that the bounded region in which we observe\ndata is actually a subset of a larger region or an infinite plane. Therefore, the data\nin a finite region is treated as if there is no boundary.\nVoronoi tessellation has a wide usage in many disciplines as well as its links to\nlifting scheme. The lifting framework in (Jansen et al., 2009) uses the cell area and\nthe neighbourhood structure provided by the Voronoi tessellation of data locations.\nThe lifting scheme is a denoising method for irregularly spaced data and has advantages over conventional wavelet methods, and other well-known spatial prediction\n\n1\n\n1.2 Voronoi tessellation\n\nmethods. The flexibility and applicability of lifting on general data situations is one\nof its key strengths. Also the theoretical properties of lifting allow it to deal with\nfunctions that are smooth or with discontinuities, or even in the case of uncertainty\nof either cases.\n\n1.2\n\nVoronoi tessellation\n\nVoronoi tessellation is a standard space subdivision method. In one dimension, the\nreal line is divided into intervals, whereas the two-dimensional space is divided into\nnon-overlapping convex cells or polygons in two-dimensional case, and three dimensional case is also possible that the partitions are referred to as the polyhedron. In\nthis thesis, the particular focus is on the two-dimensional case where we investigate\nthe statistical properties of Voronoi cells in the absence and presence of boundaries.\nThere is a vast literature on Voronoi tessellation regarding its theoretical aspects\nand its applications to many different areas which will be discussed in Chapter 2.\nConsider a set of n finite number of points x1 , x2 , . . . , xn ∈ R2 within some finite\nregion Ω ⊂ R2 where Ω is a suitable region that contains all the points, Voronoi\ntessellation subdivides the two-dimensional Euclidean space into a collection of nonoverlapping convex polygons or mosaics V = {Vi ; i = 1, . . . , n} called Voronoi cells.\nThis is done by associating each point xi with all the closest points x in that space\nbased on the Euclidean distance. Each Voronoi cell Vi associated with the point xi\nis defined as\nVi = {x ∈ R2 kx − xi k ≤ kx − xj k for j = 1, 2, ..., i − 1, i + 1, ..., n}\n\n(1.1)\n\nwhere k.k denotes Euclidean distance. Each cell Vi is defined to be that segment\nof Ω which is closer to the corresponding point xi than any other point. The\nedges of the Voronoi cells may consists of line segments, half lines or infinite lines.\nWe consider the line that separates two cells has nearly zero thickness hence the\nT\nintersection of two cells Vi Vj is nearly nonempty. Therefore, Voronoi cells satisfy\nn\nn\nS\nT\nΩ=\nVi and\nVi = ∅ up to a measure zero and the statement can be generalized\ni=1\n\ni=1\n\nfor d−dimensional cases as explained in Okabe et al. (2000) and Møller (2012).\n\n1.3\n\nA motivating example for boundaries\n\nGiven a set of points, Voronoi tessellation can be constructed based on the locations\nof the points. A simple example is illustrated in Figure 1.1. The left plot shows\n\n2\n\n1.3 A motivating example for boundaries\n\nthe Voronoi tessellation of randomly distributed points in a continuous region but\nonly a part of the region is shown. The solid lines continue outside the window\nbased on the locations of other points. The geometric structure created by Voronoi\ntessellation is non-overlapping convex polygons or the Voronoi cells where each cell\nedge is the perpendicular bisector between two points.\n\nFigure 1.1: A zoomed in version of Voronoi tessellation of points with ρ = 200 in an\ninfinite plane. Gray lines are the original tessellation lines before any boundary is\nused. Dashed lines are the tessellation lines after the vertical solid line is imposed as\na boundary to the points on the right side. Cells with the ( ) had an intersected the\nboundary, and cells with (•) did not intersect the boundary but also had a changes\nin their shapes. Gray and black circle points are the points of remaining cells.\nIn this thesis, we are also interested in the cases where the spatial region is disrupted\nby a boundary. The right plot shows an example of this situation. Voronoi tessellation of the same set of points subject to a boundary line is given. The objective of\nthis illustration is to demonstrate the consequences of the boundaries. Consider the\nvertical solid line is an imposed boundary, and the Voronoi tessellation of points on\nthe right side is performed again. Changes on the shape of cells are observed for the\ncells that are closer to the boundary and most of the cells far from the boundary\nremains the same.\nThere are interesting features of the cells in the presence and absence of the boundary. The cells with a ( ) point have vertices on the boundary line and the boundary\nline clipped a part of the cell at the top. The remaining two cells with ( ) are both\nclipped and expanded after the boundary is imposed. More importantly, although\nsome cells with (•) point did not have a vertex on the boundary, their shapes are\n\n3\n\n1.4 Spatial point patterns and Poisson point process\n\nalso affected by the boundary. Therefore, a statement ‘only the cells that have a\nvertex on the boundary are likely to be affected by the boundary’ be inefficient.\nThe boundaries and the boundary types is an important context in spatial statistics\nand a clear explanation of their functionality and effect is necessary. A boundary is\na real or artificial line or point based on the dimension of the space that separates\ntwo things or acts as an end point of an existing space. In Figure 1.1 (right),\nwe visualised the Voronoi tessellation of points and draw an artificial boundary\nline. The vertical line acts as an end point of the tessellation and has an effect\non the existing structure. However, the dashed lines can also be considered as\nthe boundaries of polygons which are perpendicular bisectors that separates two\nneighbour points. In this thesis, we focus on boundaries for simulated data, and\nreal data examples where the boundary is an artificially imposed boundary or a\nstudy region.\nIn the real life, physical boundaries occur due to the existence of a natural factor,\nfor instance, a coastline, river, or the starting point of a desert. The political\nboundaries are another example of real boundaries such as the border between two\nstates or countries. Occasionally, the political boundaries are determined based\non the natural factors such as a river may be referenced to separate the states.\nAlso, a smaller sampling regions may be defined on a large geographical region to\nstudy the features of the plants or the soil. The defined sampling region may be\na suitable rectangular window and acts as the boundary. The natural boundaries\nhave different effects compared to the boundaries such as the sampling regions.\nFor instance, proximity to a natural boundary may have a negative effect on the\nfertility of the soil or the existence of the trees. However, if a rectangular region is\nsampled from a larger region, the observations in the sampled region are related to\nthe ones outside the boundary, hence it is important to consider ways to understand\nand reduce the bias near the boundaries. Throughout the thesis, we give examples\nof these kind of induced boundary types, discuss the issues that may occur, and\npropose ways to reduce the boundary effects.\n\n1.4\n\nSpatial point patterns and Poisson point process\n\nA spatial point pattern is a set of randomly located points on a specified region\nthat is designated as the two-dimensional Euclidean space in this thesis but one\nand three-dimensional cases are also likely. The locations of trees in a forest, cell\n\n4\n\n1.4 Spatial point patterns and Poisson point process\n\nnuclei of a tissue, earthquake centres, bird nests, particles, and the positions of the\ngalaxies in the universe are the examples of point patterns. The locations of points\nare also referred to as the events, and the information carried at the locations are\ncalled marks such as the tree diameter. These type of data are called the marked\npoint pattern data.\nOn the other hand, the point processes are stochastic mechanisms, and are useful\nto understand, describe and analyse the point patterns. It is mostly used for the\nidentification of the short-range relationship between the points that characterizes\nwhether a spatial randomness, regularity, or clustering exist. Our aim is not to\ngive a complete treatment about point process statistics, it is rather to explain\nthe methods which are useful to implement the Voronoi tessellation and the lifting\nframework that are the main focus of this thesis.\nThe point patterns are assumed to have an underlying mechanism that can be\nformulated by point processes. Voronoi tessellation explained in Section 1.2 require\npoints x1 , x2 , . . . , xn ∈ R2 , however, have not mentioned yet whether the points rely\non a mathematical concept.\nIn this thesis, we consider several types of geometrical structures of the point patterns that are realisations from point processes. Hence, we use the point processes\nto generate point patterns that obey the parameters of certain point processes. A\npoint process N is described as a random counting measure or a function that is operating on sets in (Illian et al., 2008). For instance, for any bounded region B ∈ R2 ,\nN(B) stands for the number of events or points within the region B.\nOne of the simplest but a fundamental point process is the homogeneous Poisson\nprocess whose realisations exhibit complete spatial randomness. A point process N\nis a homogeneous Poisson process if it has the following properties:\ni The number of points in any bounded region B follows a Poisson distribution\nwith mean ρ|B|. In the formal way,\n(ρ|B|)n −ρ|B|\nPr{N(B) = n} =\ne\nn!\nwhere |B| denotes the area of B,\nii and given n points xi , those points form an independent random sample with a\nuniform distribution on B.\n\n5\n\n1.5 Regular and clustered points\n\nThe parameter ρ in (i) is called the intensity that refers to the mean number of\npoints per unit area. The second property constitutes the complete spatial randomness of the points. We will use the homogeneous Poisson points essentially in\nChapter 2 and occasionally in the other chapters.\n\n1.5\n\nRegular and clustered points\n\nThe regularity and clustering of points or events happen towards departure from\ncomplete spatial randomness. In the parts of the thesis, we are interested in the\nusage of examples of regular and clustered points, in addition to the homogeneous\nPoisson points. The reason of the consideration of regular and clustered points\nis to see how the core methods we develop throughout the thesis that rely on\npoint patterns are affected by the departure from complete spatial randomness. In\nother words, the intention is to examine the performance of the methods when the\nregularity and clustering approaches to its extreme forms.\nRegularly spaced and clustered data locations are the two important cases we consider. These point patterns are frequently seen in real life data. Regular and\nclustered point processes are also called inhibition or repulsion, and clumping or\nattraction respectively in the literature. These two processes can be expanded to\nexamples where different levels of regularity or clustering is observed such as departures from homogeneity to highly clustered and regular points.\nThere is a convenience of generating point patterns that are examples of clustering and regularity and control this process with a single parameter. The saturation\nprocess by Geyer (1999) permits both the attraction and repulsion processes for spatial data. Geyer’s saturation process is an extension to the Strauss process Strauss\n(1975) that is a method for repulsion within a fixed radius. The saturation process\nof Geyer modifies the Strauss process by constraining the overall contribution of\neach point to a maximum value (Goldstein et al., 2015). The probability density of\nthe saturation model is\nf (x; β, γ, r, s) = cβ\n\nn\n\nn\nY\n\nγ\n\nmin(\n\nP\n\nj6=i\n\n1kxi −xj k≤r ,s)\n\n(1.2)\n\ni=1\n\nwhere c is a constant, β, γ, r, s are the parameters,\n\nP\n\n1kxi −xj k≤r denotes the number\n\nj6=i\n\nof neighbours of the point xi within a distance r. The saturation threshold s ≥ 0\nprevents each term in the product from being larger than γ s and hence the product\nis never larger than γ sn . This prohibits the attraction from becoming very strong,\n\n6\n\n1.6 Lifting scheme\n\nwhich discourages highly clustered patterns. If s = 0, the model becomes a Poisson\npoint process, if s > 0, the interaction parameter γ can take any values such that\nγ > 1 indicates attraction or clustering and γ < 1 indicates repulsion or inhibition.\nAlso, in the case of s = ∞, the model reduces to the Strauss process.\nThis model will also be used for generating realisations of regular and clustered point\npatterns as is aimed for the Poisson point process. The method is useful in terms\nof controlling the departure from homogeneous pattern with a single parameter γ.\nHence, it allows flexibility to decide on the degree of clustering and regularity which\nwe are especially interested in.\n\n1.6\n\nLifting scheme\n\nThe lifting scheme transforms a noisy function at irregularly spaced data locations\ninto the lifting domain where the data is represented by a set of coefficients. Then\nthe coefficients are modified by a thresholding rule that aims to separate the noise\nand preserve the important features in the data such as the step changes or spikes in\na function. The inverse transform of the thresholded coefficients gives an estimate of\nthe true function. We adopt the lifting one coefficient at a time technique proposed\nby Jansen et al. (2009), which iteratively transforms the data into coefficients in\nthe lifting domain by starting with localised or fine-scale details and working up to\nbroader or coarse-scale patterns.\nWithin the stages of the lifting transform, the weights, which are obtained from\nthe areas of Voronoi cells are used. This is the part where the Voronoi tessellation\nand the lifting scheme merge. Voronoi tessellation is used for the detection of the\nneighbourhood of the points, and the cell area is used for the calculation lifting\ncoefficients. The standard choice for the weights is using the observed cell area that\nis calculated using the boundaries. We alternatively use adjusted cell area as the\nweights that are attained from the a method we will devise later.\nThe noise is a usual and an unavoidable issue which happens during data collection\nor the recording of the data by measurement tools. In a general sense, noisy data\nin real life case may be an image of a person or an ultrasound image that contain\nnoise, or a recorded noisy signal from sound. The underlying true patterns are the\ntrue functions in this case which we would like to estimate by separating the noise\nfrom the data. In this thesis, we are more interested in developing some aspects\nof existing denoising methods and propose alternative ways that can improve the\nestimation of the underlying true patterns. Therefore, we use the two-dimensional\n\n7\n\n1.7 Thesis structure\n\nanalogues of some well known functions and artificially add noise. The functions\nwhich are treated as the true functions are explained in Section 6.1. The noiseadded test functions are the noisy functions which we apply the lifting scheme to\nseparate the noise from the data.\nFunction estimation using the lifting scheme has the standard model yi = f (xi ) + \u000fi\nwhere we consider {xi }ni=1 ∈ R2 as the irregularly spaced data locations, yi are the\nobserved noisy data, f (xi ) are the values of some underlying true function corrupted\nby independent and identically distributed Gaussian noise such that \u000fi ∼ N (0, σ 2 ).\nHowever, we are interested in the estimation of the function fi when only the noisy\nobservations yi are available. In this situation, we use the lifting scheme to obtain\nan estimate of fˆi .\nVoronoi tessellation-based lifting scheme aims to estimate the underlying true function fi using the Voronoi tessellation cell area as weights during the process. Lifting\nis a linear transformation that transforms the observed noisy data yi into the lifting\ndomain, and the transform can be represented by a transform matrix L. Hence,\nthe resulting representation of the noisy data can be shown as d = Ly where the\nvector d consists of lifting coefficients. These coefficients in d are usually a sparse\nrepresentation of the observed data y that explains the data by a small number of\nnon-zero coefficients. The zero or small coefficients indicate small deviations in the\ndata that are due to the noise and the larger coefficients are attributed to the real\nfeatures in the function. The transform matrix L is independent of the observed\ndata values and only depends on the data locations hence it has a reusable feature\nfor other data observed at the same locations. The process of estimation of fˆi includes the adjustment or thresholding of the coefficients in d that aims to shrink the\nsmall coefficients to zero, and keep the larger ones, obtaining a vector of adjusted\ncoefficients d0 . Finally, the inverse transform is performed on the thresholded coefficients to estimate the underlying true function as f̂ = L−1 d0 that is separated from\nnoise. The transform can be inverted by both using the inverse of the transform\nmatrix L or following the steps of the lifting in an inverse way. The steps of the\nlifting scheme will be explained in detail in Chapter 5.\n\n1.7\n\nThesis structure\n\nVoronoi cells have geometrical properties such as the cell area, perimeter, number\nof edges, interior angles of cells etc., that have been widely studied. Currently the\n\n8\n\n1.7 Thesis structure\n\nliterature include the analytic derivation of the mean cell properties and numerical approximations using appropriately selected parametric distributions based on\ncomputer experiments for particular point pattern types.\nWe extend the study on the statistical properties of Voronoi cells by considering Voronoi cells in the infinite plane and finite regions using different types of\nboundaries, and demonstrate the differences in the cell properties in the presence of\nboundaries in Chapter 2. This separate part of the study contributes significantly\nto the Voronoi tessellation literature since little attention has been given to how\nthese properties change when a boundary is imposed. A better understanding of\nthe statistical properties of Voronoi cells in bounded regions is especially important\ndue to the usage of such properties on the next topic that we focus on, the lifting\nscheme.\nThe consideration of various boundary types and their effects is also important. A\nconvex hull of points is the smallest polygon that includes all points and can be\ndrawn for any type of point patterns. On the other hand, a suitable window i.e.,\na rectangular window can be used as the boundary. However, different boundary\ntypes are likely to have distinct impact on cell properties. For instance, consider a\nset of uniform random points X = {x1 , . . . , xn } generated in a unit square. Hence,\nthe finite region is defined as Ωu = [0, 1]2 which we can consider as a boundary.\nThen we perform Voronoi tessellation of points and record the cell area for each cell\nVi . If the convex hull of points is used for the same set of sampled points, unless\nthere are points precisely at the corners of the unit square, the convex hull will be a\nsubset of the unit square such that Ωc ⊂ Ωu . Therefore, the observed cell area will\nbe different for the cells close to the boundary with the usage of these two types of\nboundaries. Although the choice of the boundary could be expanded, we consider a\nlimited number of boundary types that demonstrates the important consequences\ncaused by the boundaries.\nWe conduct a simulation study in Chapter 2 to investigate the statistical properties\nof Poisson Voronoi cells in the infinite plane, and when unit square and convex\nhull boundaries are imposed. The chapter discusses the exploratory analysis of the\ncell properties for different boundary cases, the fitting of parametric distributions\nfor cell area, perimeter and number of cell edges, and explores the changes in cell\nproperties when the boundaries are imposed. It also presents the results for different\nintensities of points. Findings in Chapter 2 open an important discussion about the\nreduction of issues caused by the boundaries which is discussed in Chapter 3.\n\n9\n\n1.7 Thesis structure\n\nAnother major contribution of this thesis is to propose a method that treats the\ndata in a finite bounded region as if it is a subset of a larger region or an infinite\nplane. This part of the thesis aims to reduce the unfavourable consequences of the\nboundaries and is a transition between the Voronoi tessellation study and the lifting\nscheme. In Chapter 3, we use the data obtained from the simulation in Chapter 2\nthat is a large data set containing many variables which are characteristic information such as the cell area (infinite plane, unit square, convex hull), perimeter,\nnumber of cell edges, cell type (interior, edge, corner), distance from the boundary\netc., for 106 cells that are sampled from individual realisations. Using the remaining\nvariables, we fit regression models to predict the true cell area that is the cell area\nin the absence of boundary. This is done by dividing the data obtained from simulation into training and validation sets, fitting regression models to the individual\ntraining sets which we call base models, and using an ensemble approach in the\nprediction of cell area in the validation set. We also identify influential points that\ncause large error in the validation set and add them to the training sets, and fit augmented models that are capable of predicting observations that are hard to predict.\nEvaluation of the performances of the base and augmented models, and implementation of this approach into the lifting framework is an important contribution of\nthe thesis.\nChapter 4 investigates the application of the method in Chapter 3 for general data\nsituations such as the departure from homogeneity. The models are used for the\narea prediction of Voronoi cells based on regular and clustered point patterns. The\npreliminary objective is to see the performances of the models on the violation of\nthe homogeneous patterns. Furthermore, we present a way to update the models\nhence they perform more efficiently for the regular and clustered data cases.\nNext, we explain the lifting scheme and how the Voronoi tessellation is used as an\ningredient in the method Chapter 5. Non-parametric regression is one of the classical concerns in statistics including the analysis of spatial data. The lifting scheme is\na relatively new method introduced by Sweldens (1998), and is an extension of the\nwavelet methods that are used for function estimation in non-parametric regression\nusing shrinkage schemes (Donoho & Johnstone, 1994; Donoho et al., 1995). While\nthe conventional wavelet methods require equally spaced data with size n = 2J\nfor some J ∈ N, the lifting scheme relaxes such restrictions as being applicable to\nany type of data structure regardless of the size n. We rely on the framework described in Jansen et al. (2009) when using Voronoi tessellation-based lifting scheme\nin two-dimensions.\n\n10\n\n1.7 Thesis structure\n\nChapter 5 is a background chapter for lifting scheme in two dimensions with some\ndiscussion on wavelet methods which the lifting scheme is built upon. The technical\ndetails about the steps of the lifting scheme and illustrative examples are given.\nThresholding methods are also explained.\nAs mentioned previously, the Voronoi cell area is used as the weights in the lifting\nscheme. These weights determine the calculation order of the coefficients in d, and\nthe update of function values in the stages of lifting. Hence, the choice of the weights\nhas a direct effect on the estimation of the underlying true function. For spatial data\nin a finite region, we consider observed weights such as the calculated cell area using\nboundaries. More importantly, we use the predicted cell area from the models we\ndeveloped, and use the predicted area as weights. Ultimately, we consider various\nweight methods, and evaluate their performances especially aiming our proposed\nmethod to reduce the boundary effects and improve the function estimation. Since\nwe target our new method to be used in general data situations, we take into\naccount various data location structures, such as randomly distributed, regularly\nspaced and clustered points and test the method using numerous test functions that\nhave different spatial characteristics.\nChapter 6 explains the types of weight methods, and two-dimensional test functions\nwe consider throughout the thesis. Then it presents the lifting results for simulated\nhomogeneous data locations with different configurations of weight methods. We\nfocus on the local information to check and identify the differences between weight\nmethods in the estimation of test functions. Lifting results for regular and clustered\ndata from simulations, and real data sets are presented in Chapter 7 where we\nevaluate the weight methods in the case of departure from homogeneity and use the\nsuggested method for the real data examples. The weight methods we consider in\nthis chapter also include the area prediction with local intensity-based scaling since\nwe use regular and clustered points that have local features.\nFinally, we give an overall summary of the thesis, discuss the meaning and importance of our findings, and talk about the potential future work in Chapter 8.\n\n11\n\nChapter 2\nStatistical properties of Voronoi\ntessellations in bounded regions\n2.1\n\nObjective of of the study\n\nVoronoi tessellation is a standard space subdivision method that has wide application areas such as seismology, astronomy, ecology, meteorology, metallurgy, material\nscience, and architecture. Also, the structures obtained from Voronoi tessellation\nare used as an auxiliary tool in the analysis of spatial data. For instance, the partitions obtained from the Voronoi tessellation are used as a curvature parameter in\nspline methods as discussed in Ripley (2005) or as the weights in spatio-temporal\nanalysis methods, and for the intensity estimation and efficient computation algorithms Illian et al. (2008).\nAlthough Voronoi tessellation is a wide topic on its own, it is also used in conjunction\nwith other methods such as the lifting scheme which we aim to develop some aspects\nof in this thesis. The lifting scheme discussed in Jansen et al. (2009) uses Voronoi\ntessellation as a key ingredient in the algorithm. The neighbourhood structure\ndetermined from the Voronoi tessellation, and the properties of Voronoi cells such\nas the cell area are used in the steps of the lifting scheme. Therefore, this thesis\naims to give a good understanding of the Voronoi tessellation and investigate the\naspects which are important but has not been studied thoroughly.\nThis chapter discusses the statistical properties of Voronoi tessellations based on\nhomogeneous Poisson points in the infinite plane and in the bounded regions. The\ndescription and examples of Poisson Voronoi tessellation is given in Section 2.2.\nGeometrical and statistical characteristics of Voronoi cells have been investigated\n\n12\n\n2.2 Voronoi and Poisson Voronoi tessellation\n\ntheoretically and numerically for decades and applied to a range of data types. Over\nthe years, properties of the Voronoi cells such as the mean cell area, perimeter,\nnumbers of edges or vertices and vertex angles have been explored. These cell\nproperties are used in different context in many disciplines. Hence, the relevant\nliterature was reviewed and findings are discussed in Section 2.3.\nThe techniques and approaches used in the previous work provide us guidance how\nthe experiments on point patterns were conducted for particular cases. However, in\nthis chapter, new perspectives are considered which will be a significant contribution\nto improve the current approaches. Point patterns are generally considered to be\nin an infinite plane so that Voronoi cells are surrounded by the neighbour cells.\nThis chapter mainly focuses on Voronoi tessellation in two-dimensional space, and\nexplores the characteristics of the cells when boundaries are imposed on the point\npatterns. Section 2.4 explains the design of the simulation study we perform and\nSection 2.5 summarizes our results. This chapter is published as a journal article\nin Gezer et al. (2021).\n\n2.2\n\nVoronoi and Poisson Voronoi tessellation\n\nRevisiting the definition (1.1) in Section 1.2, Voronoi tessellation partitions the\ntwo-dimensional space into disjoint regions Vi called Voronoi cells, given a set of\npoints xi ∈ R2 , i = 1, 2, ..., n. Each Vi is associated with a point xi and cells are\ndetermined by the perpendicular bisectors between the point and its neighbours.\nIn the formal way, it can be expressed as\nVi = {x ∈ R2 kx − xi k ≤ kx − xj k for j = 1, 2, ..., i − 1, i + 1, ..., n}\nwhere k.k is the Euclidean distance (Møller, 2012; Okabe et al., 2000).\nThe formal definition of the Poisson point process is given in Section 1.4. When\nthe number of randomly generated points n in R2 follow a Poisson distribution\nwith a finite and constant intensity ρ > 0, this standard point pattern is called a\nhomogeneous Poisson point process. Therefore, the Voronoi tessellation based on\nhomogeneous Poisson points is called the Poisson Voronoi tessellation.\nConsider the set of points {xi }ni=1 ∈ Ω and let Ω ⊂ R2 be a convenient region\nin the space which contains all the points. Therefore, for n homogeneous Poisson\npoints we shall denote n ∼ P o(ρ|Ω|) where |Ω| is the area of the region Ω. The\nregion may be a suitable rectangle, the convex hull of the points, or some other\n\n13\n\n2.3 Background and previous work\n\n(a)\n\n(b)\n\n(c)\n\nFigure 2.1: Examples of Voronoi tessellation of points with intensity (a) ρ = 20,\n(b) ρ = 100 and ρ = 200 in a unit square bounded region Ω = [0, 1]2 .\nspecified region. Realizations of Poisson Voronoi tessellations (PVT) with intensities\nρ = {20, 100, 200} respectively in a unit square Ω = [0, 1]2 are given in Figure 2.1\nwhere solid points represent the generated points in each realization. Points are\ngenerated uniformly in a unit square and the number of points follows a Poisson\ndistribution with mean ρ. Perpendicular bisectors between points are shown with\ndashed lines which generate the cells. These lines are called the cell edges and two\npoints are considered as neighbours if they have a common edge.\n\n2.3\n\nBackground and previous work\n\nThe importance of space subdivision methods to investigate spatial splines, and\nexamples of different spatial point patterns for both simulated and real data to\nrelate the subject to the estimation of distributions of the locations within a region\nusing the Voronoi tessellation are discussed in Ripley (2005), Illian et al. (2008) and\nOkabe et al. (2000). The Voronoi tessellation has been applied in different sciences\nsuch as in seismology (Schoenberg et al., 2009) to find the distribution of the cell\nareas of Voronoi tessellations based on the locations of earthquakes in Southern\nCalifornia; astronomy (Icke & Weygaert, 1987; Ramella et al., 2001; Yoshioka &\nIkeuchi, 1989) to discover how galaxies are distributed in space; to investigate the\nconditions of the habitat of animals when they are establishing territories (Tanemura & Hasegawa, 1980); in agriculture for maximal weed suppression to plant\ncrops (Fischer & Miles, 1973) and to study atomic crystals (Mackay, 1972), liquids\n(Finney, 1970), glasses (Luchnikov et al., 2000), and wireless networks (Baccelli\n& Blaszczyszyn, 2001; Koufos & Dettmann, 2019). An application of constrained\nVoronoi tessellation is used in micro-structure modeling (Xu & Li, 2009) where a\n\n14\n\n2.3 Background and previous work\n\nnew space subdivision method is introduced using reverse Monte Carlo based on\nconditions such as moving the randomly placed points until their geometric features\nobey a particular distribution.\nPreliminary studies (Gilbert, 1962; Meijering, 1953) investigated the mean of interface area, edge length, and number of faces for an aggregate of crystals that are\nconsidered as the points. Let N denote the number of cell edges, P the perimeter\nand A the area of a Voronoi cell. Meijering (1953) presented the initial theoretical\nresults of the Voronoi cells and showed that the mean cell area perimeter and the\nnumber of cell edges are\nE(N ) = 6,\n\nE(P ) = 4ρ−1/2 ,\n\nE(A) =\n\n1\nρ\n\n(2.1)\n\nwhere ρ is the unit intensity of the points as explained in Section 1.4. Even though\nthe distributions of the cell properties has been investigated empirically, no exact representative distribution has yet been found. However, various authors have\nrecommended that appropriate approximations can be made using the Gamma distribution with appropriately chosen parameters.\nKiang (1966) proposed an appropriate fit for the length of Voronoi line segments in\none-dimension, the area of cells in two-dimensions and the volume of polyhedrons in\nthree-dimensions. The length distribution of the Voronoi segments in one dimension\nis derived analytically and Monte Carlo experiments are performed to estimate the\ndistribution of areas and volumes in two and three-dimensional spaces. A fixed\nnumber of points are randomly distributed on a square lattice and the cell areas are\nrecorded for all points. To avoid boundary effects, the coordinates of the points on\nthe opposite end of the region is translated. This process is repeated independently\nfor many realizations to increase the sample size.\nStandardized measures are obtained through (2.1). For instance, the standardized\n√\ncell area and perimeter are derived as s = A × ρ and p = ρ/4 × P , therefore,\nE(s) = E(p) = 1 for the standardized measures and number of edges E(N ) is taken\nas the same since it is independent of ρ, (Crain, 1978). This makes the comparison\nof the parameter estimation results accurate when different studies used different\nρ. The two-parameter gamma distribution has density function\nf (s|b, c) =\n\nbc (c−1) −bs\ns\ne ,\nΓ(c)\n\n0 < s < ∞,\n\nb, c > 0\n\n(2.2)\n\nwhere b is the shape parameter, c is the rate parameter and s is the standardized\n\n15\n\n2.3 Background and previous work\n\ncell area. Kiang (1966) found that (2.2) can explain the observed histograms of the\nstandardized cell areas. The cell area A is standardized and denoted as s instead of a\ndue to simplicity of the expression of the Gamma distribution with three parameter\nwhich we will use later.\nAnalytic derivation of the distribution of cell area for Voronoi tessellation cells\nin two dimensions in Weaire et al. (1986) approximated the shape parameter as\nb = 3.63. Both in Kiang (1966) and Weaire et al. (1986), it is assumed that b = c\nbased on the similarity of the parameter estimation results.\nAnother study by Kumar & Kurtz (1993) was based on Poisson-Voronoi tessellation\nwith intensity ρ = 100 were one of the points was always placed at (0.5, 0.5) and the\nremaining points are randomly placed within a unit square. Then, the distributions\nof the area, perimeter, length of the each side of the cell and the numbers of the\nsides were investigated for the centered point. Kumar & Kurtz (1993) also found\nthat the two-parameter gamma distribution gave an accurate fit for the observed\nhistograms of the cell properties. The shape and rate parameters are estimated\nb = 3.7176 and c = 3.7174, respectively.\nHinde & Miles (1980) carried a simulation based on homogeneous Poisson point\nprocess with intensity ρ = 100 and recorded the properties of the point that is closest\nto the centre of the unit square region based on many independent simulations.\nThe observed shape of cell area and perimeter distributions suggested a uni-modal\ndensity function dominated by an exponential and controlled by a simple power in\n(0, ∞). Therefore, the three-parameter generalized gamma distribution in (2.3) is\nused.\nThe three-parameter gamma distribution by Stacy (1962) has density function\nabc/a (c−1) −bsa\nf (s|a, b, c) =\ns\ne\n,\nΓ(c/a)\n\n0<s<∞\n\na, b, c > 0.\n\n(2.3)\n\nNote that (2.3) is a two-parameter gamma distribution as in (2.2) when the shift\nparameter a = 1. Parameters of the generalized gamma distribution should ideally\nbe estimated by maximum likelihood estimations as stated in (Stacy & Mihram,\n1965). However, two other computationally simpler methods gave very similar\nresults.\nStatistical distributions of the Voronoi cells from homogeneous Poisson points in\ntwo and three dimensions are studied by Tanemura (2003) in an extensive manner.\nA set of homogeneous Poisson points with intensity ρ = 200 is generated in a\n\n16\n\n2.3 Background and previous work\n\ntwo-dimensional space where the unit square is used as the sampling region. To\nconstruct an independent sample of Voronoi cells, a point shifting method is used.\nThe procedure is based on random selection of a point and to move it to the centre\nof the sampling region Ω = [0, 1]2 that is (0.5, 0.5) while the relative positions of\nthe other points are kept. This procedure will be explained in Section 2.4.\nThe intensity of the points and the numbers of the realizations were modified in\nTanemura (2003) to check whether any difference exists but the results for the\nstandardized cell properties were very similar. Three-parameter gamma distribution\nis fitted for the observed histograms using the maximum likelihood estimations. The\nlog-likelihood function l(a, b, c|s) for the standardized areas is derived as\nl(a, b, c|s) = log\n\nr\nY\n\nf (si |a, b, c)\n\ni=1\n\n=\n\nr\nX\n\nlog f (si |a, b, c)\n\ni=1\n\n=\n\nr\nX\n\n{log\n\ni=1\n\nabc/a\n+ (c − 1) log si − bsai }\nΓ(c/a)\n\nr\nr\nX\nX\nabc/a\n= r log\n+ (c − 1)\nlog si − b\nsai .\nΓ(c/a)\ni=1\ni=1\n\n(2.4)\n\nTanemura’s approach was to approximately maximize the log-likelihood function\ngiven in (2.4). The same distribution for the perimeter, as well as for discrete\nmeasures such as the numbers of the edges is used even though the generalized\ngamma distribution is a continuous distribution. Estimated parameters in Table 2.1\nshows that a 6= 1, and b and c are not as similar in multi-dimensions as is seen in\nthe previous work that relied on two-parameter gamma distribution.\nDimension\n1\n2\n3\n\na\n\nb\n\nc\n\n1.0\n1.07950\n1.16788\n\n2.0\n3.03226\n4.04039\n\n2.0\n3.31122\n4.79803\n\nTable 2.1: Generalized gamma distribution parameter estimates for dimensions\nd = 1, 2, 3 in Tanemura (2003).\nArvanitakis (2014) overlaid the density lines of the estimated parameters of the\ngeneralized gamma distribution by Tanemura (2003) over the two-parameter gamma\n\n17\n\n2.3 Background and previous work\n\ndensity fitted by Tanemura (2005) for the same data. However, two densities did\nnot show a meaningful difference on fitting the histograms.\nIn a recent paper, Koufos & Dettmann (2019) conducted research on the distribution\nof bounded Poisson Voronoi cell areas. An integral based method from Brakke\n(1987) is extended to calculate the mean cell area. The method is to consider\nPVT over a quadrant in two-dimensional space and calculate the mean cell area for\nthe points located at the edge and corner of the region, and in the bulk which is\ninterior part of the region. The parameters are estimated by two-parameter gamma\ndistribution in (2.2) from the first two moments of the cell area as\nc=\n\nE(s)2\nV ar(s)\n\nand b =\n\nE(s)\n.\nc\n\nThe mean and variance for area of the cells located at the corner, edge and bulk are\ngiven in Table 2.2. A corner cell is a Voronoi cell that has more than two vertices\nlocated on the boundary, the edge cell has exactly two vertices, and a bulk cell has\nno vertices on the boundary. The standardized mean cell area is found less than 1\nat the corner and edges of the region. Also, the parameter estimates are different\nin these two cases. Under these considerations, computer simulations verify the\nresults from the integral based method.\nType\n\nE(s)\n\nV ar(s)\n\nb̂\n\n1/ĉ\n\nCorner 0.36351 0.10567 1.25052 0.29069\nEdge\n0.61082 0.17198 2.16935 0.28157\nBulk\n1\n0.28018 3.56918 0.28018\nTable 2.2: Mean cell area, variance and parameter estimations for two-parameter\ngamma distribution by Koufos & Dettmann (2019) for corner, edge and bulk cells.\nIn this section, a discussion of previous studies and the directions that they followed\nare given. The main purpose of the previous work was to find suitable distributions\nto estimate the properties of Voronoi cells. Analytic derivations of the cell properties\nare verified as the performance of programs for statistical computing increased. In\nthe remainder of this chapter, investigation of Voronoi cells in two dimensional\nspace will be extended to the cases of regions with imposed boundaries.\n\n18\n\n2.4 Design of the simulation\n\n2.4\n\nDesign of the simulation\n\nIn the entire experiment, the intensity of the points is set to ρ = 200 for r = 106\nrealizations. To generate independent samples of Voronoi cells, a technique briefly\nmentioned in Section 2.3 is used. We generate uniform random points with the\nspecified intensity and perform the Voronoi tessellation. Next, one cell is selected\nat random and moved to the centre of the region. The relative positions of the\nother points are kept the same using periodic boundary conditions. Finally, the\nproperties of the selected cell are calculated. The procedure is repeated for a new\nset of points, for a total of r = 106 realizations. An illustration of this the point\nshifting process for a randomly selected point is given in Figure 2.2 for ρ = 100 for\nvisual clarity.\n\nFigure 2.2: A randomly sampled cell with solid black point along with its neighbour\npoints as red triangles is shown (left). The arrow shows the direction of the sampled\npoint to the centre of the region where all other points keep their relative positions.\nCell moved to the centre of the region (right), where new points from the opposite\nend of the region form the new neighbourhood.\nIt is known that the properties of Poisson Voronoi cells do not change by conditioning on the location of a point in an infinite plane for the homogeneous Poisson points\n(Koufos & Dettmann, 2019). This first step of the experiment focuses on PVT in\nan infinite plane and adopts the shifting illustrated in Figure 2.2 to generate independent samples of cells in the infinite plane. Additionally, two different boundary\ncases (the unit square and the convex hull of points) are used as in Figure 2.3 to\ninvestigate how the imposed boundaries affect the cells.\n\n19\n\n2.5 Results\n\nFigure 2.3: Voronoi tessellation of points with ρ = 100 bounded with a unit square\n(left). The convex hull of points shown with red lines and points which are on the\nconvex hull are in blue (right).\n\n2.5\n\nResults\n\nThis section presents the simulation results based on the three cases; infinite plane,\nunit square, and convex hull bounded cells, fitting of parametric distributions, and\na discussion of the effects of the boundaries on the cell properties. We present the\nresults for three cases separately.\n\n2.5.1\n\nVoronoi tessellation in the infinite plane\n\nIn the first part of the experiment, Voronoi tessellation of points in the absence of\nthe boundary is considered. Initially, homogeneous Poisson points with ρ = 200 are\nsimulated within the unit square region Ω = [0, 1]2 domain, and a point is sampled\nrandomly. Then the sampled point is moved to the centre of the region (0.5, 0.5) by\nkeeping the relative positions of all other points as described in Figure 2.2. Finally,\nthe Voronoi tessellation of the shifted points is performed, and the cell properties\nof the sampled point is recorded. This temporary process allows us to eliminate a\npossible boundary effect on the sampled cell. Therefore, the recorded cell property\nreflects as if there is no boundary.\nHistogram of the cell area of one million Poisson Voronoi cells each of which is\nrandomly sampled from one million realisations of point patterns is presented in\nFigure 2.4 (left). A uni-modal left skewed distribution is observed in the histogram.\nMean cell area over pixel bins is sown as a image plot (right) that summarizes the\ninformation over the two-dimensional surface. The region is divided into equal size\nof bins and the mean cell area is visualized based on the cell area observed at the\n\n20\n\n2.5 Results\n\nsampled points in each bin. The initial positions of the sampled points before the\nshifting process are used when creating the image plots.\n\nFigure 2.4: Area of Voronoi cells in the infinite plane (left). Surface plot of cell area\nin the infinite plane (right). The unit square is divided into a 50 × 50 grid of bins\nand the mean of the observations falling in each bin is plotted.\nThe histogram and the surface plot summarizes the results from all realizations\nin the experiment. Hence, each pixel bin of the image contains a number of data\npoints from different realizations. The surface plot therefore shows the spatial\npatterns of cell area at different locations. We observe an unstructured pattern\nin Figure 2.4 since the data is based on the cells in the infinite plane where the\nunstructured pattern is expected. However, there are interesting spatial features\nwhen the boundaries are imposed which will be discussed later. Although the\nright plot in Figure 2.4 appears to be a trivial example, it is included since the\nequivalent plots will be presented in the following sections. The surface plots in the\nresults section and throughout the other chapters of the thesis are created using the\nfunctions in the ggplot2 package by Wickham (2016). The observed unstructured\npattern in Figure 2.4 implies that the cell area takes value around the expectation\n1/ρ, and does not change over the infinite plane, that supports the statement that\nthe characteristics of Poisson Voronoi cells are independent of the location.\nSimilarly, the perimeter of each cell in one million realizations is calculated and\nthe histogram and the surface plot is shown in Figure 2.5. The histogram of the\nperimeter (left) is less skewed and even has a symmetric-like shape compared to\nthe area. However, the observed cell perimeter over the infinite plane (right) shows\nunstructured characteristics as in the cell area.\n\n21\n\n2.5 Results\n\nThe bar chart of the number of Voronoi cell edges along with the surface plot is\nshown in Figure 2.6. It is likely for the cells to have 6 edges as expected and\nno anomalies over different parts observed. Numbers of the cell edges and their\noccurrences in the infinite plane is shown in Table 2.3 which ranges from 3 to 15\nand has very small number of observations for N > 9.\n\nFigure 2.5: Perimeter of Voronoi cells in the infinite plane (left). Surface plot of\ncell perimeter in the infinite plane (right).\n\nFigure 2.6: Number of cell edges in the infinite plane (left). Surface plot of number\nof cell edges in the infinite plane (right).\n\n22\n\n2.5 Results\n\n# of edges\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n11360\n\n106358\n\n260419\n\n293821\n\n199110\n\n90317\n\n29523\n\n# of edges\n\n10\n\n11\n\n12\n\n13\n\n14\n\n15\n\nCounts\n\n7312\n\n1495\n\n252\n\n27\n\n5\n\n1\n\nCounts\n\nTable 2.3: Number of cell edges in the infinite plane and the occurrences observed.\n\n2.5.2\n\nVoronoi tessellation using unit square boundary\n\nIn this section, we consider imposing boundaries for homogeneous Poisson points in\nthe infinite plane, particularly, the unit square boundary. Similar to the Figure 2.4,\ncell areas are calculated for one million Poisson Voronoi cells bounded with the unit\nsquare and the surface (left) and line (right) plots are produced in Figure 2.8. The\nlines are created based on the points that are sampled from different transects of\nthe region that are explained in Figure 2.7. This allows us to investigate the local\ndetails of the surface plots.\nWe select three transects and create line plots based on the cell area of the points\nlocated on these transects. In Figure 2.8 (right) the line plots are created for the\ntransects based on the image (left). The transects are shown in Figure 2.7 using\nthe arrows with the same colours of lines in Figure 2.8. First, we chose a diagonal\ntransect from bottom-left corner to the top-right corner of the region and showed in\nred colour. Note that this can be achieved in four different ways. Second, another\ntransect called middle that is horizontal and located at the y = 0.5, and shown\nin a green colour. The vertical middle transect can also be used which will have\nsymmetrical properties with the horizontal transect. Lastly, an edge transect is\nselected and shown in blue. This can also be done for any four edges of the region.\nUsing these transects, we average the data over the pixel bins in that transect to\ncreate the line plots in Figure 2.8 (right).\nIn the surface plot in Figure 2.8, it is seen that the Voronoi cells are likely to behave\ndifferently depending on their location. Points that are very close to any edge have\nsmaller cell areas (region coloured in blue) than the ones close to the centre of the\nregion which are not affected by the boundary. The red parts that are relatively\nclose to the boundaries show that the cell area is higher than the mean cell area\nat these regions. This is the case when the point associated with a Voronoi cell is\nfar from the boundary but the cell has a vertex on the boundary. Hence the size of\nthe cell become large. Cells which are not affected by the boundary, which can be\n\n23\n\n2.5 Results\n\nFigure 2.7: Transects used in the line plots in Figure 2.8.\nthought as the ones located interior to the region shows the same characteristics as\nin the infinite plane case.\n\nFigure 2.8: Surface plot of cell area in a unit square (left). Averaged cell area over\nthe grids against the scaled distance of the direction being followed on the region\n(right). Different directions are shown in Figure 2.7.\nIn conclusion, restriction of the infinite plane with a regular rectangle boundary,\nnamely the unit square, causes cells to have different sizes conditioning on the\nlocation. For instance, Voronoi cells located very close to the corner of the boundary\nare found to be very small in size as it deviates from location to location as specified\nfrom blue to red colour in the surface plot in Figure 2.8. Line plots also demonstrate\nthe changes in the cell area proximity to the boundary.\nAnother measure, cell perimeter, is similarly visualized in Figure 2.9. Cells intersect\nthe unit square boundary, have perimeter partly constitute a small part of the\n\n24\n\n2.5 Results\n\nFigure 2.9: Surface plot of cell perimeter in a unit square (left). Averaged cell\nperimeter over the grids against the scaled distance of the direction being followed\non the region (right).\nboundary. We observe similar patterns in both surface and line plots as the cell\narea but the gap between the cell perimeter for cells laying on the boundary is not\ntoo wide than the mean cell perimeter that means the cells close to the boundary\ndo not have very small perimeter. Only the corner cells have noticeably small\nperimeter.\nNumber of cell edges as a surface plot for unit square boundary is given in Figure 2.10. Considering the expected number of cell edges E(N ) = 6 over the infinite\nplane, cells affected with a regular rectangle boundary are likely to have smaller\nnumber of edges than the mean cell edges and the number of cell edges gets smaller\nas it gets closer to the boundary. Diagonal and middle transects from the surface\nplot shows a similar pattern in terms of the mean cell edges. However, if we walk\nfrom one corner to the next corner, namely on the edges of the region, a significant\nreduction in the mean cell edge is observed. More importantly there is no location\nwhere the number of cell edges are observed to be greater than 6.\n\n2.5.3\n\nVoronoi tessellation using convex hull boundary\n\nIn this section, the properties of Voronoi cells within the convex hull of points will\nbe investigated. In this case, homogeneous Poisson points generated in Ω = [0, 1]2\nare restricted using the convex hull of the points. Hence, the convex hull stands for\nthe boundary that restricts the Voronoi cells. Convex hull is the smallest convex\npolygon which contains all the generated points. For the sets of points we generated\nin the realizations of of the simulation, we use the convex hull as the boundary and\n\n25\n\n2.5 Results\n\nFigure 2.10: Surface plot of number of cell edges in a unit square (left). Averaged\nnumber of cell edges over the grids against the scaled distance of the direction being\nfollowed on the region (right).\ncalculate the cell properties for the cells that are restricted by the convex hull. The\nresults for each property are presented and discussed respectively.\n\nFigure 2.11: Surface plot of cell area in convex hull (left). Averaged cell area over\nthe grids against the scaled distance of the direction being followed on the region\n(right).\nSurface plots are given in Figure 2.11, 2.12, and 2.13 for the area, perimeter and\nnumber of cell edges respectively with line plots as described in Figure 2.7. Recall\nthe cells in a unit square which intersect the boundary with point falls apart from\nthe boundary were likely to be larger than the mean cell area. Area of such cells are\nobserved in the levels of the red colour. On contrary to the unit square boundary,\nit is seen from Figure 2.11, 2.12, and 2.13 that the area of cells close to the\nconvex hull are usually smaller than the expected cell area. Also, cells affected\n\n26\n\n2.5 Results\n\nby the convex hull are likely to have smaller area and perimeter compared to the\nones affected by the unit square and such measures take the smallest values at the\ncorners of the region. Number of cell edges will be larger in the convex hull case\nthan the unit square for the cells close to the boundary which is possibly because of\nhaving irregular shapes in the convex hull compared to the unit square where the\nboundary has four straight lines which reduces the number of cell edges intersecting\nthe boundary.\n\nFigure 2.12: Surface plot of cell perimeter in convex hull (left). Averaged cell\nperimeter over the grids against the scaled distance of the direction being followed\non the region (right).\n\nFigure 2.13: Surface plot of number of cell edges in convex hull (left). Averaged\nnumber of cell edges over the grids against the scaled distance of the direction being\nfollowed on the region (right).\n\n27\n\n2.6 Comparisons of different boundary cases and the previous work\n\n2.6\n\nComparisons of different boundary cases and\nthe previous work\n\nIn this section, the aim is to discuss the results from Section 2.5.1, 2.5.2, and 2.5.3\nin a comparative manner. The same standardization method with the previous\nwork is used and the results are presented for standardized area s, standardized\nperimeter p, and the number of cell edges N . Summary statistics are given in\nTable 2.4 for the measures of Voronoi cells for three different cases. Mean cell area\ns̄I and perimeter p̄I are calculated for cells in the infinite plane are found very\nclose to the analytically calculated expectations E(s) = E(p) = 1, however, the\nmean cell area and perimeter for the unit square boundary case are calculated as\ns̄U = 1.137 and p̄U = 1.097. For the convex hull case, it is calculated as s̄C = 0.886\nand p̄C = 0.951. Therefore, cells bounded with unit square will have larger area\nand perimeter than the cells in the infinite plane, and convex hull bounded cells will\nhave the smallest area and perimeter. Here, when calculating the measures for cells\nin the unit square and convex hull, only the cells affected by the boundary are taken\nand the ones interior to the region are avoided which carries the same information\n\nCase\n\nMean\n\nSD\n\nSkewness\n\nKurtosis\n\nArea\n\nInfinite plane\nUnit square\nConvex hull\n\n1.004\n1.137\n0.886\n\n0.531\n0.654\n0.599\n\n1.022\n1.164\n1.186\n\n1.525\n2.247\n1.957\n\nPerim.\n\nInfinite plane\nUnit square\nConvex hull\n\n1.002\n1.097\n0.951\n\n0.244\n0.294\n0.307\n\n0.190\n0.240\n0.146\n\n-0.025\n0.106\n-0.141\n\nEdges\n\nfrom the infinite plane case. Hence, the summary statistics are calculated for the\ncells given they are affected by the boundary.\n\nInfinite plane\nUnit square\nConvex hull\n\n6\n5.364\n5.432\n\n1.334\n1.203\n1.185\n\n0.432\n0.497\n0.509\n\n0.204\n0.278\n0.321\n\nTable 2.4: Mean, standard deviation, skewness and kurtosis of standardized area,\nperimeter and number of edges of Poisson Voronoi cells in infinite plane, and for\nunit square and convex hull boundaries.\nTwo and three-parameter gamma distributions are fitted for the standardized cell\narea, perimeter and number of edges. It is discussed in the previous work that\nthe gamma distribution gives the best approximation for these measures. Although\nthe number of cell edges takes integer values in the rage of [3, 15] and hence has\n\n28\n\n2.6 Comparisons of different boundary cases and the previous work\n\na discrete distribution, the gamma distribution is still used and the parameters\nof the gamma distribution are estimated only using the observed integer values as\nsuggested in Hinde & Miles (1980). We used the gamma distribution for the number\nof cell edges to have a comparison of the estimated parameters with the previous\nwork Hinde & Miles (1980) and Tanemura (2003). An appropriate alternative could\nbe the Poisson distribution.\nFigure 2.14 shows the mid points of the histogram bins for the observed measures\n)\nwith solid points (•) and fitted two and three-parameter gamma densities (\nand (\n\n) with estimated parameters from Table 2.5 and 2.6 respectively. Plots\n\nin the first column are the results for the infinite plane, second row for the unit\nsquare, and the bottom row are for the convex hull cases. Three-parameter gamma\ndistribution with blue lines shows a great performance to fit the measures, however,\neven though two-parameter gamma performs well in many cases, it cannot fit the\ncell perimeter as good as the three-parameter. In addition to the gamma distribution, several others, Weibull and log-normal distributions are checked but their\nperformances were not satisfactory as can be seen in Figure A.1 in Appendix A for\nthe standardized areas in the infinite plane.\nDisparities on the statistical properties are discovered through the surface plots,\nand summary statistics for three cases of Voronoi cells are verified by the estimated\nparameters of two and three-parameter gamma distributions in Table 2.5 and 2.6\nrespectively. We can conclude that the measures of Voronoi cells can be estimated\nvia three-parameter gamma distribution with appropriately chosen parameters. Parameters estimated for the cells in the infinite plane shows a great agreement with\n(Hinde & Miles, 1980) and (Tanemura, 2003) for three-parameter gamma distribution, and similar parameters are estimated for two-parameter gamma case.\nIn addition to the parameter estimates in Table 2.5 and 2.6, we also calculated\nthe 95% confidence intervals for the parameter estimates. The parameters are\nestimated by maximizing the log-likelihood function in (2.4). The second-order\npartial derivative of the log-likelihood function evaluated at the maximum creates\nthe Hessian matrix\n\n∂2l\n\n∂2l\n∂a∂b\n\n∂2l\n∂a∂c\n\n\n2\n∂ l\nH=\n∂b∂a\n\n\n∂2l\n∂b2\n\n∂2l\n.\n∂b∂c\n\n∂2l\n∂c∂a\n\n∂2l\n∂c∂b\n\n∂a2\n\n\n(2.5)\n\n\n∂2l\n∂c2\n\nThe diagonal elements of the inverse of negative Hessian matrix (−H−1 ) are the\nestimated variances (σ̂a2 σ̂b2 σ̂c2 ) for the parameters. Then a confidence interval for\n\n29\n\n2.7 PVT for different intensities\n\n2.7\n\nPVT for different intensities\n\nIn this section, Poisson Voronoi tessellation for different intensities is explored. The\nentire experiment discussed in Section 2.5 was based on PVT with intensity ρ = 200,\nhowever, it is important to discover whether the behaviour of cells vary as the ρ\nchanges. Hence, another experiment is performed considering ρ = {30, 50, 100, 300}\nfor r = 106 realizations and results are compared with the ones obtained before.\nAgain, surface plots are aimed to be created to see the patterns of cell measures over\nthe surface as in Section 2.5.2 and 2.5.3. Since the mean cell area depends on the\nintensity of points as E(s) = 1/ρ, the same standardization method is applied for\nall experiments based on different ρ values to make results comparable. Line plots\nare produced in Figure 2.24 from the surface plots following a horizontal direction\nat the middle of the region as described in Figure 2.7. Each ρ value is assigned to\na colour and put together for i.e standardized area for unit square bounded region\n(top left), number of edges for convex hull bounded region (bottom right) and so\non.\nStandardization of measures is useful for the comparison of different cases. A general comment from the plots is the occurrence of different patterns over a surface\ndepending on the location. However, the variability is mostly observed on the cells\ncloser to the boundary of the region. There is not a big difference on the cell measures when moved closer to the centre of the region which are possibly the cells that\nare not affected by the boundary for all cases except when ρ = 30. The initial intuition for ρ = 30 is that the number of the points generated for such intensity could\nsometimes be very smaller than 30. Hence, a randomly sampled cell could still be\naffected by the boundary even though it is located very far from the boundary.\nProbability of a cell being affected by the boundary for lower intensities lead us to\nunderstand the circumstances of having smaller number of points and the pattern\nthat they generate on the surface. Particularly, unit square or convex hull boundary\nmay still affect the characteristics of a Voronoi cell whose associated point is very\nfar from the boundary. Another simulation is performed to find the probability of a\nrandomly sampled cell being affected by the boundary when the number of points\nare m = {10, 15, 20, 30, 50}. Number of points are fixed in each simulation instead\nof fixing the intensity to avoid the variability on the number of points generated for\neach specified intensity especially when the ρ is very small, and the simulation is\nrun for r = 105 realizations.\n\n41\n\n2.7 PVT for different intensities\n\nUnit Square\n\nConvex Hull\n\nFigure 2.24: Standardized PVT properties for intensities ρ = 30, 50, 100, 200, 300\nfor points across the centre of the region. Each ρ is assigned to a colour that show\nthe pattern standardized cell properties. First and second column of the plots show\nthe results for unit square and convex hull bounded cells, and rows panels are for\ncell area, perimeter and number of edges respectively.\n\n42\n\n2.8 Conclusion\n\nProportions of number of cells affected by the unit square and convex hull boundaries based on the number of points is presented in Figure 2.25. Almost every\nrandomly sampled cell among 10 points is affected by the boundary and the proportion decreases as the number of points increases. The proportion of cells affected\nby the convex hull is slightly higher than the unit square case but the proportions\nconverge at higher number of points. The different pattern observed in Figure 2.24\nfor ρ = 30 is also highlighted here for lower intensities in general.\n\nFigure 2.25: Proportion of boundary-affected cells in 106 realizations for varying\nnumbers of points n ∈ {10, 15, 20, 30, 50, 100, 200, 300}\n\n2.8\n\nConclusion\n\nTo sum up, this chapter investigated Poisson Voronoi cells in two-dimensional space\nand their statistical properties extensively. Simulations are performed for intensity\nρ = 200 for r = 106 realizations. Poisson Voronoi cells are initially considered in\nthe infinite plane where the results related to distribution fitting showed a great\nassociation with the relevant literature. More importantly, boundaries are imposed\nto the homogeneous Poisson points. Using the unit square and the convex hull\nof points as boundaries, cell area, perimeter, and the number of cell edges can be\nestimated using the three-parameter gamma distribution. The difference in the\nfitted distributions raises the importance of taking into account these boundary\neffects in the analysis of spatial data which usually comes with its own boundary\ncase.\nAlthough the cell properties do not change over the infinite plane, it is not the case\nwhen the boundaries are imposed. The distance and location of the points from\n\n43\n\n2.8 Conclusion\n\nfrom the boundary play a key role on the statistical properties of the cells. Also,\nwe observe similar properties of cells at the edges and the corners. For instance\nthe cell properties show symmetric properties when the unit square is folded with\nrespect to the axes x = 0.5 or y = 0.5. In Gezer et al. (2021), the image plots are\nconstructed using the folded regions to increase the sample sizes in the pixel bins.\nThe cells affected by both boundaries do not have identical properties. Hence, in\npractice, further study considering other specifically determined boundaries or real\nboundaries such as state borders or coastlines may need to be performed for any\ngiven data set.\nWe have only considered the homogeneous Poisson points in a unit square region,\nhence further work may investigate how he results change for other point patterns\nand boundary types. In the previous work, Schoenberg et al. (2009) studied the\ndistributions of the Voronoi tessellation cell area and perimeter of the locations of\nearthquakes in Southern California. The data is based on the epicentres of 7567\nearthquakes that had magnitude over 3.0 between 1984 and 2007 in Southern California. The cells intersecting the various boundary options are excluded from the\nstudy due to their biased values. The study found that the tapered Pareto distribution is a suitable distribution to model the area and perimeters of the Voronoi cells.\nThe same distribution is also used to approximate the seismic moments. This study\nis an example of the application of Voronoi tessellation to a real data set which has\nan irregular boundary type and clustered earthquake epicentres. We learn from this\nstudy that the Voronoi cells that are obtained from data types with different spatial\ncharacteristics can be modeled through different distributions than the gamma that\nis mostly suitable for data locations with spatial randomness.\n\n44\n\nChapter 3\nPrediction of Voronoi tessellation\ncell area\n3.1\n\nOverview\n\nThe statistical properties of Voronoi tessellation cells of homogeneous Poisson points\nin two dimensions is studied in Chapter 2. Careful attention is necessary when\nboundaries are imposed since they change the geometric structure of the Voronoi\ncells and hence the statistical properties, such as the cell area, perimeter, number\nof cell edges. Figure 1.1 was an excellent illustration of the changes on the Voronoi\ncells when a boundary line is drawn.\nPrevious chapters clarified that Voronoi tessellations subject to boundaries may not\nreflect the true cell properties since the boundary has a constraining effect. In this\nchapter, we aim to propose and develop ways that treats the spatial data available\nin a finite bounded region as if there is a larger region or an infinite plane which\nthe data in the finite region is a subset of. Hence, the boundary effects in the data\nare aimed to be reduced.\nOne approach to accomplish this would be to predict the true cell area using\nregression-based models. We will give a detailed explanation and examples of conducting this approach throughout this chapter. This process can be thought as\ncreating models that adjusts the cell area especially for the cells near the boundary.\nThe predictors of the model is the observed cell properties within a boundary and\nthe response is the true cell area (as if there is no boundary). To fit the regression\nmodels, we need a data set. The simulation study in Chapter 2 generated large data\nset of many cell properties. Splitting this data into training and validation sets, we\n\n45\n\n3.2 Boundary issues\n\naim to fit regression models using the training set and evaluate and improve the\nperformances of the fitted models in the validation sets.\nThe issues that the boundaries cause in the analysis of spatial data is briefly discussed in Chapter 3.2. Then, we described the data set and variables that we are\ngoing to use in the modeling in Section 3.3. The methodology we use, the steps\nof the model fitting process such as the division of the data into the training and\nvalidation sets, and model selection criterion etc. are explained in Section 3.4. The\nresults of model fitting are presented in Section 3.5 and discussed extensively. Section 3.6 mentions an approach to classify the boundary-effected cells respectively.\nFinally, the suggestions for alternative scenarios are given in Section 3.7.\n\n3.2\n\nBoundary issues\n\nSpatial data usually come with its own boundary structure. This can be a regular\nor irregular boundary of any sort. In some situations, a suitable rectangle can be\ndefined as the boundary for a set of data points in two-dimensional space. This is\nsuitable if the points are generated within the constraint of a rectangular domain.\nHowever, it is always possible to take the convex hull of points whether or not a\nrectangular boundary is useful. Consider a data set that contain data locations in\na two-dimensional space but we do not know the exact boundary of the data. In\nsuch cases, we use the sampling region, or we can always draw the convex hull of\ndata points and consider the convex hull as the boundary.\nIn the simulation study in Chapter 2, the sampling region was defined as the unit\nsquare hence we used both the unit square and convex hull as two boundary types\nand investigated the cell properties based on them. We may generalize the definition\nas ‘known’ boundary and ‘unknown’ boundary for these two cases respectively.\nKnown boundary stands for the case where we are given the boundary information\nsuch as the sampling region. Otherwise, we can use the convex hull in the unknown\nboundary case.\nThere are properties that can be calculated for Voronoi cells such as the cell area,\nperimeter, number of cell edges, cell type, or the shortest distance from a boundary.\nThe relationship between the cell properties may help us to understand the changes\nin the cell area which we are interested in. For each boundary type, unit square\nknown and convex hull unknown, these properties are calculated differently. For\ninstance, the closest distance from a data point to the unit square, and to the convex\n\n46\n\n3.3 Description of variables\n\nhull boundary is likely to be different. Hence, we define these two distances as\nseparate variables. In Section 3.3, we give a detailed explanation of these variables.\n\n3.3\n\nDescription of variables\n\nThe variables are the properties of the Voronoi cells which we are able to measure.\nIn Chapter 2 we presented how the distributions of cell area, cell perimeter and\nthe number of cell edges change in the presence and absence of the boundaries\nand only focused on these three cell properties. However, in this chapter, we are\ninterested in the prediction of true cell area (in the absence of the boundary) using\nthe information obtained from the cells in the presence of a boundary. This approach\nhas an implicit act such that the Voronoi cells are treated in a continuum rather\nthan a restricted region.\nTo create models that predicts the true cell area, that is the outcome, we rely on\nregression methods that require independent variables which are the other measured\nproperties of the Voronoi cells. We consider measuring all possible meaningful cell\nproperties that is likely to have an effect on the cell area and use them as predictor\nvariables. Hence the collection of variables are the ones which are likely to have a\ncasual relationship with the outcome. Since a variable selection procedure for the\nchoice of best model is also taken into account when fitting the models, the number\nof variables we initially consider is not restricted and we included as many variables\nas possible.\nThe list of candidate variables are presented in Table 3.1. These variables are obtained from the simulation data in Chapter 2 and the code to perform a single\nrealization of the simulation is given in Appendix B. We repeatedly perform the\nsimulation to obtain independent realizations of data sets. First, a set of homogeneous Poisson points with a specified intensity within a unit square region is\ngenerated and a point is selected at random. Voronoi tessellation of the points is\ndone when the boundary is the original unit square boundary and the convex hull\nof the generated points. Then, the cell properties listed in Table 3.1 are recorded\nfor the sampled cell. Some cell properties are calculated differently based on the\nboundary type that is why the first and the last columns are created separately. Finally, the sampled point is moved to the centre of the unit square by translating the\nrelative positions of the other points and the cell area is recorded which indicates\nthe outcome variable, true cell area.\n\n47\n\n3.3 Description of variables\n\nThe separate sets of variables are considered for the unit square and convex hull\nboundary cases. There are (i) variables in common, such as the point coordinates\nx1 , x2 , (ii) similar variables such as the cell area that is calculated differently based\non the boundary type such as the unit square area x3 , and the convex hull area z3\nrespectively, and (iii) some variables which are only possible to calculate for one\nboundary type such as the cell type x11 . As it is not obvious how to define the\ncorner cells for irregular boundaries.\nUnit Square Boundary\n\nType\n\ny: inf.area\nCN\nx1 , x2 : x.coord & y.coord CN\nx3 , x4 , x5 : unit.area\nCN, CN, DN\nunit.per, unit.edge\nx6 : on.chull\nC\nx7 : m\nDN\nx8 : dist.edge\nCN\nx9 : dist.vert\nCN\nx10 : dist.cent\n\nCN\n\nx11 : type\n\nDN\n\nDefinition\n\nUnknown Boundary\n\nCell area without boundary.\nCoordinates of data points.\nArea, perimeter and number of edges.\n\ny: inf.area\nz3 , z4 , z5 : chull.area\nchull.per, chull.edge\nWhether a point is located on the convex hull.\nz6 : on.chull\nNumber of points generated.\nDistance of the point from the nearest boundary. z8 : dist.edge2\nMinimum distance between the boundary\nz9 : dist.vert2\nand the cell vertices.\nDistance of the point from\nthe centre of the finite region.\nCell type based on how many points of the cell\nsegments intersect the boundary (0, 2, 3, 4).\n\nTable 3.1: A list of the variables for both the unit square and the unknown boundaries. The variable labels x and z refer to the variables obtained using the unit\nsquare and convex hull boundaries respectively. Variable names and notations with\ncorresponding definitions are given. Variable types are labeled as CN: continuous numerical, DN: discrete numerical, and C: categorical. Dashes represent the\nunavailability of the usage of a particular variable.\nThere are several variables shown in Table 3.1 that have direct or indirect relationships in some fashion. This situation is called collinearity or multicollinearity\nwhich happens when an independent variable can be linearly predicted by other\nvariable or multiple variables. A more general term for this case is concurvity that\nhappens when a smooth term is predicted by other smooth terms is the model, i.e.\nthe generalized additive model (Morlini, 2006). Collinearity and concurvity cause\ninterpretation issues and unstable predictions that may lead large errors.\nThe variables x1 and x2 are the coordinates of the points, so the distance from\nthe boundary x8 and the distance from the centre x10 use x1 and x2 to calculate\nthe Euclidean distances. Hence, there is a situation where one of the variable is\nbeing a function of the other variables. However, we do not have any collinearity\nissues for the unknown boundary case since we do not calculate the distance from\nthe boundary or the centre and use them as variables in the model. The available\nvariables when the boundary is unknown are given at the right column of Table 3.1.\nTheir existence in the models will be discussed in Section 3.4.\n\n48\n\n3.4 Area prediction for Voronoi tessellation cells\n\n0.015\n\n0.000\n\n0.010\n\n0.015\n0.000\n\ninf.area\n\n0.4 0.000\n\nunit.area\n\n0.015 0.1\n\nunit.per\n\n0.40.000\n\nchull.area\n\n0.1\n\nchull.per\n\n0.10 0.00 0.10\n\ndist.edge\n\n0.35\n\n0.35\n\ndist.cent\n\n0.55\n\n0.55 0.00\n\ndist.edge2\n\n0.000\n\n0.010\n\n0.000\n\n0.010\n\n0.0200.1\n\n0.3\n\n0.5\n\n0.000\n\n0.010\n\n0.0200.1\n\n0.3\n\n0.50.00\n\n0.10\n\n0.00\n\n0.10\n\n0.35\n\n0.50\n\n0.65\n\nFigure 3.1: Scatterplots of selected variables at the lower triangular part of the\nscatterplot matrix. Variable labels are given in the boxes on the diagonal panel.\nThe scatterplot matrix in Figure 3.1 shows the correlations among some variables\nwe are interested in. We aimed to inspect the data and see what kind of patterns are\nobserved among the variables. Therefore, this would help us to decide on the model\nwe would like to use. The plots are created using a randomly chosen sub-sample\nfrom the entire data since the visualisation would not be clear for 106 observations.\nThe levels of categorical variables, cell type, being located on the convex hull and\nnumber of cell edges are separately checked. In Figure 3.2, the cell area based on\nthe data points that are affected by the unit square and convex hull boundaries\nare shown based on cell type, being located on the convex hull, and number of cell\nedges. The cell area gets larger as the number of cell edges increases for both unit\nsquare and convex hull boundaries. The cell type has a slight effect on the unit\nsquare boundary area where the smallest median cell area is observed for the edge\ncells. Being on the convex hull causes the cells to have a smaller area than the\notherwise case.\n\n3.4\n\nArea prediction for Voronoi tessellation cells\n\nIn this section, the study design and methodology of the area prediction is explained.\nArea prediction procedure involves model fitting using regression models based on\n\n49\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\nchull.area[chull.change != 0]\n\nchull.area\n0.02 0.03\n0.01\n0.00\n\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n2 3\ntype\n\n0.04\n\n0\n\n●\n\nunit.area[area.change != 0]\n\n●\n●\n\n0.00\n\n0.01\n\nunit.area\n0.02 0.03\n\n0.04\n\n3.4 Area prediction for Voronoi tessellation cells\n\n4\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n0\n1\non.chull\n\n3\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n3\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n4\n\n5\n\n6\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n5\n\n6\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n4\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n7\n8\nunit.edge\nunit.area\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n7\n8\nchull.edge\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n\n●\n\n●\n\n9\n\n10\n\n11\n\n12\n\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n\n●\n●\n●\n●\n●\n\n●\n●\n●\n\n9\n\n10\n\n11\n\n12\n\nFigure 3.2: Unit square area and convex hull area based on cell type, convex hull\npoints and the number of cell edges.\nthe variables given in Table 3.1. The infinite plane area labeled as inf.area is\nthe response variable we aim to predict, and the remaining variables are used as\ncovariates.\nConsidering the presence of non-linear relationships between the covariates and the\nresponse, there is a need for appropriate regression methods. The linear regression\nwould not capture the non-linear pattern in the data. The techniques such as\nthe polynomial regression that models the response variable by adding k−th degree\npolynomials of the covariates might be useful, however, it may only capture a certain\namount of non-linearity. Hence, the usage of a more flexible method such as spline\nregression is essential.\n\n3.4.1\n\nThe generalized additive model\n\nGeneralized Additive Models (GAMs), a generalized form of the linear and generalized linear models, is a flexible non-parametric regression method that models\nP\nnon-linear relationship using the sum of smooth functions\nfj (θj ) of the predictors {θj }pj=1 (Hastie & Tibshirani, 1990; Wood, 2017) by a replacement of the linear\nP\ncomponents\nβj θj in the multiple linear regression. The advantage of GAMs is\n\n50\n\n3.4 Area prediction for Voronoi tessellation cells\n\nthe flexibility of the non-linear smooth functions fj that are calculated for each θj\nand added together. The general form of the additive model is expressed as\nY i = β0 +\n\np\nX\n\nfj (θij ) + εi\n\n(3.1)\n\nj=1\n\nwhere fj denote smooth, non-parametric functions that can take various shapes, p\nis the total number of covariates, and εi ∼ N (0, σ 2 ) is the error term. The smooth\nfunctions are generated by many smaller functions called basis functions that can\nbe expanded as\nfj (θj ) =\n\nKj\nX\n\nβjk bjk (θj )\n\n(3.2)\n\nk=1\n\nwhere bjk (θj ) denote the basis functions that construct the smooth components,\nand βjk are the coefficients to be estimated during the model fitting. The Kj is the\ndimension of the basis function for the component fj where an optimal choice is\nnecessary since it decides on the wiggliness of the smooth component fj . A small\nnumber of basis functions is likely to miss the wiggly patterns of the data as known\nas under-fitting, whereas very large number of basis functions results over-fitting\nthat captures very fine details. The sum of the basis functions constructs the smooth\nfunctions as shown in (3.2).\nWhen a spline basis is used, we obtain a form of linear model that has a penalty\nterm which can be written as\nβ + εi\nYi = Bβ\n\n(3.3)\n\nwhere the matrix B is created stacking the columns of a basis matrix for each covariate together. Hence the matrix B evaluates the basis functions for each observation.\nThen the model fit is achieved by choosing the vector of β that minimizes\nβ )> (y-Bβ\nβ ) + β > Pβ\nβ\n(y-Bβ\n\n(3.4)\n\nwhere P is the penalty matrix in a block-diagonal form. The penalty matrix is\nobtained from individual components of the model such that the j−th component is\nbeing λj D>\nj Dj , where Dj is a differencing matrix. Therefore, the following solution\nβ is obtained as\nfor the estimate of the vector of weights β̂\nβ = (B> B + P)−1 B> y.\nβ̂\n\n(3.5)\n\nThe components fj are identifiable only when a constraint is imposed. We require\n\n51\n\n3.4 Area prediction for Voronoi tessellation cells\n\nthe sum-to-zero constraints\n\nP\n\ni fj (θij ) = 0 so that the addition of a constant to f1\n\nwhereas it is subtracted from f2 without changing the prediction Claire & Neocleous\n(2019).\nThe estimation of the smoothing parameter can be done using maximum likelihood\n(ML), and restricted maximum likelihood (REML) that uses the random effects\nto estimate smoothing parameters. Wood (2011) showed that their performance is\nbetter than other methods such as generalized cross validation (GCV) and AIC.\nThe generalized additive can also be expressed in the form of\ng(μi ) = β0 +\n\np\nX\n\nfj (θij )\n\n(3.6)\n\nj=1\n\nwhere μ = E(y|θ1 , . . . , θp ) is the mean and g(.) is a link function function such that\nηi = g(μi ). The possible choices for the distribution of the response include the\nnormal, gamma, Poisson, binomial, inverse Gaussian, negative binomial and quasi\ndistributions and the fitting of the model for different link functions can be done\nusing a local scoring procedure or penalized iteratively re-weighted least squares\nHastie & Tibshirani (1990); Wood (2017).\nBased on the variables listed in Table 3.1, we denote the response variable true area\nthat we aim to predict as Âi for i = 1, . . . , n, and the full set of covariates is denoted\nas θ = {x1 , x2 , . . . , z3 , z4 , . . . }, hence we have {θj }pj=1 are the vectors of covariates\nsuch that θ1 = x1 , θ2 = x2 , . . . , θp = z9 for all j = 1, 2, . . . , p. We will use the θj\nnotation in the expression of the model for simplicity, and xj , zk will be used as\nlabels of the variables when necessary. Then the full model that we consider takes\nthe form\nÂi = β0 +\n\np\nX\n\nfj (θij ) + εi\n\nj=1\n\n= β0 + f1 (θi1 ) + f2 (θi2 ) + · · · + fp (θip ) + εi ,\n\n(3.7)\n\nSome low-dimensional interaction terms can be added into the model. For instance\nthe interaction function is denoted as fp+1 (θk , θm ) where k, m ∈ [1, · · · , p] and\nk 6= m. The fp+1 (θk , θm ) term indicates a two dimensional spline for the interacting\ncovariates (James et al., 2013). However, this is only valid when both of the covariates are numerical variables. If one of the variables is categorical, then smooth\nfunctions of the numerical variables are separately determined based on each level\nof the categorical variable.\n\n52\n\n3.4 Area prediction for Voronoi tessellation cells\n\n3.4.2\n\nStudy design\n\nThe data set obtained from Chapter 2 is for the statistical properties of n = 106 cells\nthat are sampled from independent realizations. This large data set can be split\ninto training, and validation sets with specifically determined sizes to fit the models\nand evaluate the performance of the models. The sample sizes for the partitioned\ndata sets are given in Table 3.2. The training and validation sets are independent.\n\nSize\n\nTraining\n\nValidation–1\n\nValidation–2\n\n5 × 105\n\n105\n\n105\n\nTable 3.2: Sample sizes of training, and validation sets. The numbers refer to the\nnumber of randomly sampled cells from independent realisations.\nThe training set is used for fitting additive models that we call ‘base models’. The\nValidation–1 is used to test these models and identify influential data points that\nlead to large prediction error. Then, some influential points are added to the training\ndata and the models are fitted again which we call these models ‘augmented models’.\nTherefore, the augmented models are aimed to be capable of predicting the ‘hard to\npredict’ cells better than the base models. This is checked in the separate ‘left alone’\ndata set, Validation–2. We evaluate the performances of the base, and augmented\nmodels in Validation–2 and highlight in which situations these models perform\nbetter. These steps will be explained in detail in the following sections.\n3.4.2.1\n\nDescription of training data\n\nIt is aimed to fit initial base models to the training data based on the model assumption in (3.7). Due to the high number of predictors, we consider the trade-off\nbetween the goodness of fit and the model complexity to decide the model that has\nthe most useful variables and leaves the insignificant ones out. Two approaches\nare tried for variable selection. The first one, is achieved by starting with an intercept only model yi = β0 and iteratively adding and removing variables, namely\nthe stepwise model selection that optimizes the goodness of fit. That is performing a model selection based on the Akaike information criterion (Akaike, 1987),\nAIC = 2p − 2 ln(L̂), where L̂ is the maximum value of the likelihood function of\nthe model may be an appropriate approach. There is an option to achieve this in\nthe gam package (Hastie, 2020) in R using the step.Gam() function.\nThe second approach is called the double penalty approach, introduced by Marra &\nWood (2011) and found to perform significantly better along with another proposed\n\n53\n\n3.4 Area prediction for Voronoi tessellation cells\n\nmethod that is shrinkage-based than the competing methods in a comparative study.\nThe space of a spline basis is expressed as a sum of two components where the first\nterm is based on the functions in the penalty null space and the second is based on\nthe penalty range space. Marra & Wood (2011) explains the approach as follows;\nfunctions in the range space can be shrunk to zero via a high penalization by the\nsmoothing penalty but the function component in the null space is unchanged.\nTherefore, penalization of the null space is required in order to shrink the entire\nspline component to zero. Their double penalty approach applies penalty for the null\nspace hence the smooth component can be eliminated. Double penalty is applied\nfor each smooth function and the functions with smoothing parameters approach\nto infinity (such as the straight lines), will be removed the model. The R package\nmgcv introduced in Wood (2015) has functions to fit additive models with an option\nof variable selection by double penalization as described in Marra & Wood (2011).\nThis approach has significantly lower computational cost compared to the stepwise\nmodel selection hence it is adopted for variable selection.\nThe model fitting and variable selection in the training data is performed using available functions in the mgcv package. The model is defined as gam(y ∼ s(theta 1)\n+ s(theta 2) + · · · ) in the function where s(·) denotes the smooth terms. Also,\nsome interaction terms can be added as f (θ3 , θ8 ) for the interaction of (unit square\narea) and (the distance from the nearest edge) that fits a two-dimensional surface.\nThe interaction of the (convex hull area) and a categorical variable (being located\non the convex hull) takes binary values f (θ12 , θ15 ) can also be added. These interactions are defined in the function by writing s(theta 3, theta 8) and s(theta 12,\nby = theta 15) respectively.\nInstead of using the entire training set at once, we divide it into training subsets\nsince the size of the training set permits this flexibility. Therefore, we may fit model\nusing each training subset and these models can be used in an ensemble learning\napproach. We first randomly sample smaller training subsets with equal size ntrain =\n5000 without replacement that gives 100 training subsets. Then the GAMs are\nfitted to each training subset and these models are used for area prediction in the\nValidation–1. We check the individual and combined performances of the models\nin the Validation–1. Having numerous models from independent training sets for\nthe same prediction purpose is also useful to reduce the sampling bias and the\nvariability in the predictions.\n\n54\n\n3.4 Area prediction for Voronoi tessellation cells\n\n3.4.2.2\n\nDescription of validation–1 data\n\nThe initially fitted base models in the training subsets are used for the prediction\nof the cell area in the Validation–1, and an unbiased evaluation of the model fit is\nmade. Next, we investigate the influential points that we define as the observations\nthat cause large prediction error. These observations are the ones that the model\nis not able predict well. One could suspect that this is due to the lack of the data\nwith similar characteristics in the training set. Hence we identify the influential\npoints and investigate whether they have characteristics in common (such as all\nbeing corner cell, or extremely large cell, etc.). Moreover, we consider enriching\nthe training sets by adding the influential points and fitting previously mentioned\naugmented models using the augmented training sets. The augmented models are\naimed to improve the accuracy of the base models.\nThe predicted cell area can be denoted as Âi for i = 1, 2, . . . , nV1 where nV1 is the\nsize of the Validation–1. Since the area prediction is made using 100 individual\nmodels, the predicted area can be denoted as Âit where t = 1, 2, . . . , 100 indicate\neach training subset. We may represent the predicted area using all models in a\nmatrix since there will be a practical use of it later. The matrix of predicted cell\nareas Â can be expressed as\n\n\nÂ11\nÂ12\nÂ13\nÂ\nÂ22\nÂ23\n21\nÂ =   ..\n..\n..\n.\n.\n.\nÂnV1 1 ÂnV1 2 ÂnV1 3\n\n\nÂ1,100\nÂ2,100\n\n..   ,\n.\n· · · ÂnV1 100\n···\n···\n..\n.\n\n(3.8)\n\nwhere the columns indicate the vector of predicted area using the t-th model for the\nValidation–1. Each row is the predicted area for the observations of Validation–1.\nFor instance, the first row Â11 , Â12 , . . . , Â1,100 indicates the prediction of area for\nthe 1st observation of Validation–1 using individual base models which are 100 in\ntotal.\n3.4.2.3\n\nInfluential points\n\nA classical modelling approach that fits models in the training data and evaluate\nthe performances of the models in a test or validation data is not our sole target.\nWe would also like to create improved versions of the initial models that have\na better predictive performance. To achieve this, we propose a way that takes\nhard to predict observation in the Validation–1 data into account. Hard to predict\nobservations refer to the area of Voronoi cells which the model is the least capable of\n\n55\n\n3.4 Area prediction for Voronoi tessellation cells\n\npredicting. We identify these data points by checking the observations that causes\nthe largest absolute prediction error. Since the individual models are applied to the\nValidation–1 data separately, each model has its own set of hardly predicted data\npoints. Therefore, we call this sets of points as the influential points.\nThe separate sets of influential points, chosen by individual models, are going to\nbe combined with the training data sets which constructs the augmented training\ndata sets. Then the new models are fitted using the augmented data which we\ncall augmented models, and keep the previously fitted base models. The augmented\nmodels are aimed to be the updated versions of the base models. These type\nof modeling approaches are discussed in Hofner et al. (2014) where a model-based\nboosting method is introduced and used to fit boosted additive models by optimizing\nthe general risk functions using penalized least squares estimates as base-learners.\nAlso the ensemble learning approach we use is similar to the random forest technique\nfor classification and regression introduced by Ho (1995) and Liaw et al. (2002).\nThe columns of Â in (3.8) indicate the predicted cell area using the initially fitted\nmodels on Validation–1. Each column is obtained using a separate model. Therefore, for each column a set of influential points can be identified. Here, we define the\nset of influential points as being the data points which has the largest 500 absolute\nprediction error. By choosing this threshold, we aim to select the most influential\npoints.\nThe next stage will be using the base and augmented models in our second validation data Validation–2 that is going to be discussed in Section 3.4.2.4 and their\nperformances are going to be compared in the results section. We also investigate\nwhat is special with these influential points and check whether they have common\nfeatures. Hence, we can have a better understanding of what type of cells cause\nlargest absolute prediction error.\n3.4.2.4\n\nDescription of validation–2 data\n\nA hold out data set with the same size as Validation–1 is created for further inference about the base and augmented models. This is the stage that the two\nmodeling approaches are compared and the most appropriate model is suggested.\nArea prediction in Validation–2 is made using individual base and augmented models, and the individual performances, and their overall performances are aimed to\nbe evaluated. Besides the design of the training, Validation–1 and Validation–2, the\nspecification of the distribution family of the residuals and link functions has an importance. As the default choice, the residual distribution has a Gaussian family and\n\n56\n\n3.5 Results\n\nidentity link function. Hence, it is worth considering the alternative assumptions\nas well.\n\n3.5\n\nResults\n\nThis section presents and discusses the results of base model fitting, identification\nof influential points using Validation–1, data augmentation, fitting the augmented\nmodels, and eventually the comparison and evaluation of the base and augmented\nmodels in Validation–2. We consider fitting models and presenting the results for\ntwo types of boundary scenarios separately: the unit square boundary, and the\nunknown boundary where the convex hull of points is used as the boundary in\nSections 3.5.1 and 3.5.2 respectively.\n\n3.5.1\n\nUnit square boundary case\n\nThis section presents the results for the unit square boundary case which is used as\nthe sampling region for the data we use from Chapter 2.\n3.5.1.1\n\nTraining base models\n\nThe additive model in (3.7) is fitted to 100 randomly sampled training data sets,\nand hence we obtained individual models for each training subsets. Model fitting is\nperformed by taking the variable selection into account. We are interested in seeing\nhow frequently the variables are chosen based on separate models. Then we may\nhave an idea about the importance of the frequently selected variables.\nIn Figure 3.3, the occurrences of the variables based on 100 base models are listed.\nNote that (:) stands for the interaction of variables. We use the x and y labels of the\nvariables referring to Table 3.1. The ranking of the selected variables highlights some\nimportant variables in the base models. For instance, (unit square boundary area)\nand its interaction with the (cell type), that is x3 : x11 , are selected in all models.\nSimilarly the interaction of (cell type) and the (distance from the boundary) that\nis x8 : x11 . Also, z8 (distance from the convex hull boundary), x4 (unit square\nboundary perimeter), the interaction of (convex hull boundary area) and (being on\nthe convex hull) that is z3 : z6 , and z4 (convex hull boundary perimeter) are selected\nin most of the models. On the other hand, the least important variables are x10\n(distance from the centre), x8 (distance from the unit square boundary), x1 and x2\nthat are (the point coordinates), and x7 (the number of points).\n\n57\n\n3.5 Results\nTraining Data\n\nAugmented Tra\n\nunit.area:type\nunit.area\ndist.edge:type\ndist.edge2\nunit.per\nchull.area:on.chull\nchull.per\nchull.area:dist.edge2\nunit.area:dist.edge\nchull.area\nunit.edge\nunit.vert\nchull.edge\nchull.vert\nm\ny.coord\nx.coord\ndist.edge\ndist.cent\n\n+13\n+2\n+1\n-4\n+3\n+4\n-1\n-3\n-2\n-9\n+5\n+1\n-2\n-4\n\nunit.area:type\nm\ndist.edge:type\nchull.area:on.chull\nunit.per\nchull.per\nunit.area\nunit.edge\nchull.edge\nunit.area:dist.edge\nchull.area:dist.edge2\nchull.area\ndist.edge2\ndist.cent\ny.coord\nunit.vert\nchull.vert\n0\n\n20 40 60 80\nTimes selected\n\n100\n\nFigure 3.3: Variables and interaction terms, and how many times selected in the\nbase models.\nEach base model can be expressed as a function as shown in the model in (3.7) that\ncontains all variables and some interactions. The useless variables in the models\nwill still appear in the equations, but since their coefficients are penalized to zero,\nthey have no effect in the model.\nTo see the trajectories of the individual models in the prediction of cell area, we\nmay visualize the estimated smooth components. The smooth components for the\ncovariates are given in Figure 3.4. Each plot is associated with a covariate that is\nlabeled in the title and many curves appear in each plot. The curves or straight\nlines are the estimated smooth components for individual models. The smooth\ncomponents with gray colour are obtained from the base models, and the black\nlines are for the augmented models which will be discussed in Section 3.5.1.2. The\ny-axis always shows the values of the response variable and the x-axis is for the value\nrange of the covariate. Hence, we will be able to compare the smooth components\nobtained from the base and augmented models.\nFlat lines in particular variables, such as the point coordinates and the number of\npoints, indicates the penalized covariate that has no effect on the response variable.\nThe smooth components that show different characteristics have the major effects in\nthe base models. The smooth components that have the curvy shapes are the ones\nthat are frequently selected in the models whereas the least selected variables are\nthe flat lines. It is also observed that some individual models behave very differently\n\n58\n\n0\n\n20 40 6\nTimes sel\n\n4\n\n6\n\n8\n\n10 12 14\n\n0.0\n\n0.2\n\n0.4\n\n0.0\n\n0.2\n\n0.4\n\n4\n\n6\n\n8\n\n10 12 14\n\nchull.vert\n\n0.000 0.002 0.004\n\nunit.vert\n\n0.8\n\n0.0\n\n0.2\n\n0.4\n\n0.005 0.010 0.015 0.020\n\n0.000 0.002 0.004\n-0.004\n\n0.000\n\n0.010\n\n0.020\n\n0.030\n\ndist.cent\n\n0.0\n\n0.2\n\n0.4\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\nm\n\n0.000 0.002 0.004\n\n0.6\n\nchull.per\n\n-0.004\n\n0.4\n\n-0.005\n\n0.2\n\nchull.area\n\n-0.004\n\n0.0\n\n-0.004\n\ndist.edge2\n\n-0.004\n\n-0.004\n0.04\n\n-0.008\n\n0.02\n\n-0.004\n\n0.000 0.002 0.004\n\ndist.edge\n\n0.00\n\n0.000 0.002 0.004\n\n0.8\n\nunit.edge\n\n0.000 0.002 0.004\n\n0.000\n\n0.03\n0.02\n0.01\n0.4\n\n-0.004\n\n-0.004\n\ninf.area\n0.000 0.002 0.004\n\nchull.edge\n\n0.0\n\n0.000 0.002 0.004\n\n0.8\n\n-0.004\n\n0.4\n\n0.00\n\n-0.004\n\n-0.004\n0.0\n\nunit.per\n\n0.000 0.002 0.004\n\nunit.area\n\n0.004\n\ny.coord\n\n0.000 0.002 0.004\n\ninf.area\n0.000 0.002 0.004\n\nx.coord\n\n0.04\n\n3.5 Results\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n140\n\n180\n\n220\n\n260\n\nFigure 3.4: Estimated smooth components of the GAMs in the individual base and\naugmented models. Gray lines are the smooth components for the base models that\nare overlaid for 100 models, and the smooth components for the augmented models\nare shown in black.\nto the majority of the other models. It is possible to make this conclusion based on\nthe smooth lines that do not follow the pattern of the majority.\nThe GAM is a generalization of the linear model, hence, it is natural to have a\nmixture of smooth and linear components in a model even though our models do not\ninclude any linear terms explicitly. The x8 (distance from the unit square boundary)\nshows an example of this case where the lines are nearly straight in all models. Note\nthat there are only the numeric variables and no interaction terms shown in this\nfigure since the other types of interactions should be presented in different ways.\nFor instance, the interaction of two numeric variables in the model generates a\ntwo-dimensional spline, and the interaction of a numeric and a categorical variables\nestimates separate smooths based on each category level.\nLastly, let us check the residual patterns of the base models. Figure 3.5 shows\nthe normal quantile plot of base models in grey lines. The variation is high at\nthe lower and higher values of the residuals within models, but the lines get closer\naround zero. Also, there are many values close to zero in the sample quantiles, that\nis because many points that are close to the centre of the spatial region are not\naffected by the boundaries and the prediction is very accurate.\n3.5.1.2\n\nTraining augmented models\n\nIn this section, we explain the procedure of training the augmented models. The\naugmented models are a continuation of the base models which are fitted using\n\n59\n\n3.5 Results\n\nFigure 3.5: The normal quantile-quantile plot of residuals versus fitted values in\nbase models in gray lines, and the averaged values as the black line.\nthe training subsets and used in Validation–1 to identify the influential points that\nis explained in Section 3.4.2.3. Augmented models are fitted using the augmented\ntraining data which is obtained adding the influential points identified in Validation–\n1. We first explain how to identify the influential points, and summarize their\ncharacteristics, and explain how the augmented models are fitted.\nRecall the matrix Â of predicted cell area in equation (3.8) where each column is\nthe vector of predictions using individual base models. To identify the influential\npoints, we base on the criterion of absolute error that can be denoted as |Ait − Âit |\nfor observations i = 1, 2, . . . , nV1 in Validation–1, and for models t = 1, 2, . . . , 100.\nFor each column t, we select from the points that cause the largest absolute errors.\nUsing each base model in Validation–1, we identify 500 points that cause the largest\nabsolute error. Therefore, each base model identifies a set of points that are the\nmost influential. Each set of influential points can be denoted as as I1 , I2 , . . . , I100 .\nIn this case, I1 is the set that contains influential points identified using the first\nbase model and would contain some points such as I1 = {x125 , x431 , x1480 , . . . }.\nIt is important to note that sets I1 , I2 , . . . , I100 are independently generated, therefore, these sets are likely to contain influential points in common. Hence, we can\ncheck the influential points in terms of the number of sets in which they are observed. The union of separate sets can be denoted as I = I1 ∪ I2 ∪ · · · ∪ I100 where I\nis the set that has all influential points. Figure 3.6 shows the number of times each\ninfluential point is identified in the y-axis, and the x-axis shows the index numbers\nn\n\nV1\nof influential points. Note that it is not the index numbers i in {xi }i=1\n, we sorted\n\nthe influential points based on the number of base models they are identified by, and\n\n60\n\n0\n\n20\n\nTimes identified\n40\n60\n\n80\n\n100\n\n3.5 Results\n\n1\n\n250\n\n500\n750\n1000\nInfluential point index\n\n1250\n\n1500\n\nFigure 3.6: Index of the influential points and how many times they are identified\nas influential.\ngave new index numbers k ∈ 1, 2, . . . , K. Therefore, x-axis in Figure 3.6 denotes\nthe k. The unique number of influential points in I is approximately K ≈ 1500,\nand around 200 points are identified as influential point by all base models.\nIt is important to check what is special with these influential points. The summary\nstatistics of the influential points and all other points are presented for comparison\nin Table 3.3. The first row for each variable panel shows the summary statistics\nof all points and the second row shows the influential points coloured in blue.\nMany comparisons can be made based on this table. The mean properties are\nsignificantly different for the influential and all points. The distance between the\ninfluential points to the boundaries are much smaller compared to the all points,\nwhich demonstrates the influential points are located closer to the boundaries. The\narea and perimeter of influential points are larger than the and points and influential\npoints have fewer cell edges. Also the standard deviations of these properties are\nsubstantially different.\nCell types of the influential points show differences compared to all points. The\nproportions of cell types are calculated for influential, and all points are shown in\nTable 3.4. The dominance of type–2 (edge cells) is obvious in the influential points.\nAlso, the proportions of type–3 (corner) and type–4 (corner+) cells which are close\nto the corner but has two cell vertices lying on each perpendicular boundary is\nhigher. Only a small number (≈ 1%) of interior cells are identified as influential.\nSimilar differences on the proportions of convex hull points are seen in Table 3.5.\nMost of the influential points are located on the convex hull boundary whereas it\nis not the case when all points are considered.\n\n61\n\n3.5 Results\n\nMin\n\nMax\n\nMean\n\nSD\n\nRange\n\ninf.area\n\n0.0001 0.0197 0.0050 0.0026 0.0196\n0.0006 0.0250 0.0115 0.0039 0.0244\n\nunit.area\n\n0.0000 0.0197 0.0050 0.0027 0.0196\n0.0006 0.0284 0.0062 0.0035 0.0278\n\nunit.per\n\n0.0222 0.6085 0.2845 0.0723 0.5863\n0.1203 0.6696 0.3404 0.0828 0.5493\n\nunit.edge\n\n3\n3\n\ndist.cent\n\n0.0020 0.7056 0.3793 0.1412 0.7036\n0.4258 0.6978 0.5566 0.0696 0.2720\n\ndist.edge\n\n0\n0.0002\n\n0.4981 0.1690 0.1175 0.4981\n0.0986 0.0190 0.0161 0.0985\n\nm\n\n139\n157\n\n264\n239\n\nchull.area\n\n0.0000 0.0197 0.0047 0.0026 0.0197\n0.0001 0.0147 0.0041 0.0027 0.0146\n\nchull.per\n\n0.0100 0.5965 0.2744 0.0730 0.5865\n0.0583 0.4970 0.2791 0.0876 0.4387\n\nchull.edge\n\n3\n3\n\n13\n8\n\ndist.edge2\n\n0\n0\n\n0.4932 0.1564 0.1191 0.4932\n0.0757 0.0068 0.0117 0.0757\n\n13\n8\n\n5.759\n4.923\n\n1.306\n1.044\n\n10\n5\n\n200.05 14.036 125\n193.50 14.254 82\n\n5.7826\n5\n\n1.294\n1.144\n\n10\n5\n\nTable 3.3: Summary statistics of the variables in the validation data. The first\nrow for each variable panel is the results for all points and the second row for the\ninfluential points coloured in blue.\nSets of influential points I1 , I2 , . . . , I100 identified by the base models are moved from\nValidation–1 to the corresponding training data sets Dtr1 , Dtr2 , . . . , Dtr100 . Then\nGAMs are refit using these augmented training data that contain the initial training\ndata sets and the influential points. The new models, namely the augmented models\nare expected to improve some features of the base models.\nThe variable selection results are given for the augmented models in Figure 3.7.\nThese results are an extension to the results in Figure 3.3 with the variable selection\nresults for the augmented models given on the right. Some variables keep their\nposition in the ranking, and some changed position. For instance, x7 (number of\npoints) was one of the least selected variables in the base models, however, it was\n\n62\n\n3.5 Results\n\ntype\n\n0\n\n2\n\n3\n\n4\n\nAll points\nInfluential points\n\n0.761 0.219 0.020 0.0006\n0.008 0.603 0.350 0.0400\n\nTable 3.4: Proportion of the cell types (0: interior, 2: edge, 3: corner, 4: corner+)\nfor all points, and the influential points.\non.chull\n\n0\n\n1\n\nAll points\nInfluential points\n\n0.933 0.067\n0.351 0.649\n\nTable 3.5: Proportion of the cells located on the convex hull (0: No, 1: Yes) for all\npoints, and the influential points.\nselected by all augmented models. Also, one of the variables selected in all base\nmodels x3 (unit square area), and z8 (distance from the convex hull boundary) are\nless important variables in the augmented models. The interaction terms x8 , x11\n(interaction of the distance from the unit square boundary and type), x3 , x11 (unit\nsquare area and cell type) are included in all base and augmented models so they\nhave importance in both methods.\nThe overlaid smooth components for single variables from individual augmented\nmodels are also shown in Figure 3.4 where the estimated components for the base\nmodels are given in the background in gray. Most black curves lie between the\nminimum and maximum value of the x-axis. This means the larger observations of\nthe variables are available in the augmented data. The gray lines do not always lie\nwithin the same range of x-axis because the initial training data did not have that\nobservations, but addition of the influential points to the training data provided\nthese observations. This means while the base models do extrapolation, augmented\nmodels are doing interpolation for the influential points. The gray and black curves\nfor a specific variable, the unit square, shows that the lengths of many of the curves\nin the base models are different. The issue is reduced in the augmented training data\nby the placement of the influential points because the influential points enriched\nthe training data with infrequently seen observations.\nThere are slight changes on the patterns of the estimated smooth curves for the\nbase and augmented models. Also, penalized variables are closer to a straight line\nat zero in the augmented models where they were slightly off from being flat in the\nbase models. The residual pattern of the augmented models using the augmented\n\n63\n\n3.5 Results\nTraining Data\n\nAugmented Training Data\n\nunit.area:type\nunit.area\ndist.edge:type\ndist.edge2\nunit.per\nchull.area:on.chull\nchull.per\nchull.area:dist.edge2\nunit.area:dist.edge\nchull.area\nunit.edge\nunit.vert\nchull.edge\nchull.vert\nm\ny.coord\nx.coord\ndist.edge\ndist.cent\n\n+13\n+2\n+1\n-4\n+3\n+4\n-1\n-3\n-2\n-9\n+5\n+1\n-2\n-4\n\nunit.area:type\nm\ndist.edge:type\nchull.area:on.chull\nunit.per\nchull.per\nunit.area\nunit.edge\nchull.edge\nunit.area:dist.edge\nchull.area:dist.edge2\nchull.area\ndist.edge2\ndist.cent\ny.coord\nunit.vert\nchull.vert\n0\n\n20 40 60 80\nTimes selected\n\n100\n\n0\n\n20 40 60 80\nTimes selected\n\n100\n\nFigure 3.7: Selected variables in the unit square boundary models and the number\nof times each term is selected. Results are given for the base models (left) and\naugmented models (right). The change in the ranking for each term is highlighted\nin blue and the total number of times selected is given in parentheses.\ntraining data is given in Figure 3.8 which shows some difference from the base\nmodels.\n\nFigure 3.8: The normal quantile-quantile plot of residuals versus fitted values in\naugmented models in gray lines, and the averaged values as the black line.\nWe now move on to the final evaluation of base and augmented models in an independent data set. The Validation–1 data is only used for the base models to\nidentify the influential points which are moved to the training data for augmentation. Hence, Validation–1 is lacking some important data points (influential) and\ncan no longer be used. As an independent data set, Validation–2, will be used\n\n64\n\n3.5 Results\n\nfor the further evaluation and inference on the base and augmented models. The\nValidation–2 has a size 105 .\nAn ensemble approach is used for the area prediction in Validation–2 to obtain a\nbetter predictive performance of the individual models. The 100 individual base\nand augmented models are used to predict the area, and the ensemble prediction\nis calculated by averaging the estimates over data points. Using general notations,\nthe matrices for the prediction in the Validation–2 data be denoted as Ŷ 0 and Ŷ ∗\nfor base and augmented models respectively\n0\nŷ11\nŷ 0\n21\n0\nŶ =   ..\n.\n\n\n0\nŷ12\n0\nŷ22\n..\n.\n\n0\nŷ13\n0\nŷ23\n..\n.\n\n···\n···\n..\n.\n\n\n0\nŷ1t\n0\nŷ2t\n\n..   ,\n.\n\n∗\nŷ11\nŷ ∗\n21\n∗\nŶ =   ..\n.\n\n\nŷn0 v2 1 ŷn0 v2 2 ŷn0 v2 3 · · · ŷn0 v2 t\n\n∗\nŷ12\n∗\nŷ22\n..\n.\n\n∗\nŷ13\n∗\nŷ23\n..\n.\n\n···\n···\n..\n.\n\n\n∗\nŷ1t\n∗\nŷ2t\n\n..\n.\n\nŷn∗ v2 1 ŷn∗ v2 2 ŷn∗ v2 3 · · · ŷn∗ v2 t\n(3.9)\nfor points i = 1, . . . , nv2 and models j = 1, . . . , t, where nv2 = 105 and t = 100.\nThe ensemble prediction is calculated as\n0\n\nỸ =\nỸ ∗ =\n\nt\n\u0010 1 X\n\n100 j=1\n100\n\u0010 1 X\n\n100 j=1\n\nt\n\nt\n\n0\nŷ1t\n\n1 X 0\n1 X 0 \u0011>\nŷ2t · · ·\nŷ\n100 j=1\n100 j=1 nv2 t\n\n∗\nŷ1t\n\n1 X ∗\n1 X ∗ \u0011>\nŷ2t · · ·\nŷ\n100 j=1\n100 j=1 nv2 t\n\nt\n\n(3.10)\n\nt\n\n(3.11)\n\nwhere Ỹ 0 and Ỹ ∗ are the vectors of predictions for base and augmented models\nrespectively.\nTo compare the performance of prediction, the prediction error over the spatial\nregion is a good way to see where these methods perform well and badly. Area\nprediction is made for a total of nv2 = 105 data points. The squared error at each\npoint is calculated as SE 0 = (yi − Ỹ 0 )2 and SE ∗ = (yi − Ỹ ∗ )2 for base and augmented\nensemble predictions. The mean squared error (MSE) is calculated by averaging\nthe squared error over pixel bins and it is visualized in Figure 3.9. The unit square\nregion has symmetric properties on each quadrant, so the spatial region is folded to\nincrease the data points in each bin using the data points at the relevant bin.\nThe high MSE occurs near the boundaries and highest at the corners for both\nbase and augmented models in (a) and (b) in Figure 3.9. The pixel bins in (a)\nhave lighter colour compared to (b) near the edges that means the MSE is smaller\nfor base models. The ensemble predictions perform very well if the points are\nlocated far from the boundary. In (c) and (d), the squared error is calculated as\n\n65\n\n3.5 Results\n\n(a)\n\n(b)\nSq. Error\n\nSq. Error\n\n2.0e-05\n\n2.0e-05\n\n1.5e-05\n\n1.5e-05\n\n1.0e-05\n\n1.0e-05\n\n5.0e-06\n\n5.0e-06\n\n0.0e+00\n\n0.0e+00\n\n(c)\n\n(d)\nSq. Error\n\n2.0e-05\n\nSq. Error\n\n2.0e-05\n\n1.5e-05\n\n1.5e-05\n\n1.0e-05\n\n1.0e-05\n\n5.0e-06\n\n5.0e-06\n\n0.0e+00\n\n0.0e+00\n\nFigure 3.9: The squared error of infinite plane area and predicted area averaged\nover pixel bins for (a): base models, (b): augmented models, (c) unit square area,\nand (d) convex hull area.\nthe difference between the infinite plane area, and the observed area due to both\nboundary scenarios. The MSE in (c) and (d) seems higher compared to (a) and (b)\nthat shows the predicted area is closer to the truth. The darkest colour is at the\nedges and the corner of the observed area due to the convex hull boundary in (d).\nWe may also check the numeric values of the MSE. Table 3.6 shows the MSE calculated globally, and for interior and edge parts of the region. The first two columns\nare results for the base models and the last two columns are for the augmented\nmodels. For these two modelling types, we obtained the results for the full models\nand reduced models separately. The full model considers all variables listed in Table 3.1. On the other hand, the reduced model excludes the variables coordinates\nx1 and x2 , and the distance from the centre x10 that have dependence with distance\nfrom the edge x8 that is kept in the model.\nWe observe from Table 3.6 that the results for the full models and reduced models\n\n66\n\n3.5 Results\n\nare extremely similar and even almost identical for the base models. For the augmented models, the reduced models gave just slightly higher MSE. We also checked\nthe ranking of variables based on how many times they appear in the reduced models and showed in Figure A.2 and compared to the full models from Figure 3.7.\nThe rankings are very similar and only a few minor differences are seen. Since the\nvariable selection method also penalized the correlated variables in the full model,\nsimilar results are expected. However, the collinearity is an essential issue that\nshould be considered in modelling.\nBase\n\nAugmented\n\nFull Models\n\nReduced Models\n\nFull Models\n\nReduced Models\n\nGlobal\n\n0.5441\n\n0.5440\n\n0.8525\n\n0.8648\n\nInterior\n\n0.0033\n\n0.0033\n\n0.0418\n\n0.0449\n\nEdge\n\n1.9032\n\n1.9031\n\n2.9001\n\n2.9357\n\nTable 3.6: MSE values calculated in the Validation-2 data using the models that\ninclude all variables, and the models that do not contain the correlated variables.\nThe MSE values are multiplied as MSE×106 .\nThe surface plots are not completely informative since it is hard to evaluate different\nmethods by eye. Therefore, we check the MSE along different surface transects in\nFigure 3.10. Cross checking of the surface plots and the transect plots verifies\nthe previous conclusions and adds more details about the results. We see that\nthe ensemble predictions for both base and augmented models are in the first two\nquartiles of boxplots. More importantly, the base ensemble predictions always give\nsmaller MSE for all transects and it is the best among all individual models. When\nthe outlier points of the individual model prediction boxplots are checked, some of\nthe individual base models give extremely larger MSE than the augmented models.\nTherefore, the augmented model approach reduces the maximum errors in the data,\nbut the overall performance of the base models is better. In the edge transect,\nsome of the boxplots have unusual shapes since there are individual base models\ninsufficient to predict the cell area well. But the ensemble prediction performs\nsatisfactorily accurate.\nThe error plots are shown in Figure 3.11 and 3.12 where the transect boxplots show\nthat base ensemble predictions are spread around zero but the augmented ensemble\nmodels mostly predict values to be larger than the truth.\n\n67\n\n2.0e-05\n\n3.5 Results\n\nDiagonal\n\nBase Ensemble\nAugm. Ensemble\n\n1.5e-05\n\nUnit Square\n\n0.0e+00 5.0e-06\n\nMSE\n1.0e-05\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n2.0e-05\n\nDistance\n\nEdge\n\nBase Ensemble\nAugm. Ensemble\n\n1.5e-05\n\nUnit Square\n\n0.0e+00 5.0e-06\n\nMSE\n1.0e-05\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n2.0e-05\n\nDistance\n\nCentre\n\nBase Ensemble\nAugm. Ensemble\n\n1.5e-05\n\nUnit Square\n\n0.0e+00 5.0e-06\n\nMSE\n1.0e-05\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\nDistance\n\nFigure 3.10: The boxplots of MSE for predictions from individual base models\n(blue) and augmented models (red) over pixel bins along different transects. The\nensemble predictions are shown with the solid points in the same colour at each\ntransect bin. The MSE for unit square and convex hull area are shown with square\nand diamond shaped points.\n\n68\n\n3.5 Results\n\n(a)\n\n(b)\nError\n\nError\n\n0.004\n\n0.004\n\n0.002\n\n0.002\n\n0.000\n\n0.000\n\n-0.002\n\n-0.002\n\n-0.004\n\n-0.004\n\n(c)\n\n(d)\nError\n\nError\n\n0.004\n\n0.004\n\n0.002\n\n0.002\n\n0.000\n\n0.000\n\n-0.002\n\n-0.002\n\n-0.004\n\n-0.004\n\nFigure 3.11: The error of infinite plane area and predicted area averaged over pixel\nbins for (a): base models, (b): augmented models, (c) unit square area, and (d)\nconvex hull area.\n\n69\n\n0.004\n\n3.5 Results\n\nDiagonal\n\nBase Ensemble\nAugm. Ensemble\n\n0.002\n\nUnit Square\n\n-0.004\n\n-0.002\n\nError\n0.000\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.004\n\nDistance\n\nEdge\n\nBase Ensemble\nAugm. Ensemble\n\n0.002\n\nUnit Square\n\n-0.004\n\n-0.002\n\nError\n0.000\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.004\n\nDistance\n\nCentre\n\nBase Ensemble\nAugm. Ensemble\n\n0.002\n\nUnit Square\n\n-0.004\n\n-0.002\n\nError\n0.000\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\nDistance\n\nFigure 3.12: The boxplots of mean error for predictions from individual base models\n(blue) and augmented models (red) over pixel bins along different transects. The\nensemble predictions are shown with the solid points in the same colour at each\ntransect bin. The MSE for unit square and convex hull area are shown with square\nand diamond shaped points.\n\n70\n\n3.5 Results\n\n3.5.2\n\nUnknown Boundary case\n\nThis section considers the case where no boundary information is available. In this\ncase, we take the convex hull of the points and use it as the boundary. The model\nfitting procedure is performed for the unknown boundary case using the available\nvariables from Table 3.1 (right column). A similar strategy is followed to fit the\nbase models as in Section 3.5.1. Then these models are used to predict the cell area\nin Validation–1 and influential points are identified. Finally, the influential points\nare added to the training data sets to fit augmented models.\nThe variable selection results for the base and augmented models are given in Figure 3.13. z4 (convex hull perimeter), z5 (convex hull number of edges), z3 : z6 (the\ninteraction of convex hull area and being on the convex hull), z3 : z8 (the interaction of convex hull area and distance from the convex hull boundary) are the most\nselected variables in the base models. z4 and z5 kept its position in the ranking\nfor the augmented models and the interaction terms slightly went down although\nthe number of times selected increased. There is a significant jump for x7 (the\nnumber of points) which was also the same for unit square boundary results, and\nz8 (distance from the boundary) are the least important terms in the augmented\nmodels.\nBase models\n\nAugmented models\n\nchull.per\n\n( 99 )\n\nx.coord\n\n( 100 )\n\nchull.edge\n\n( 99 )\n\nm\n\n( 100 )\n\n-\n\nchull.area:on.chull\n\n( 92 )\n\nchull.per\n\n( 100 )\n\n+6\n\nchull.area:dist.edge2\n\n( 85 )\n\nchull.edge\n\n( 100 )\n\n+2\n\ndist.edge2\n\n( 77 )\n\nchull.area:dist.edge2\n\n( 99 )\n\n-1\n\nx.coord\n\n( 67 )\n\ny.coord\n\n( 98 )\n\n+1\n\ny.coord\n\n( 60 )\n\nchull.area:on.chull\n\n( 93 )\n\n-4\n\nchull.area\n\n( 41 )\n\nchull.area\n\n( 22 )\n\n-\n\nm\n\n( 30 )\n\nchull.vert\n\n( 15 )\n\n+1\n\nchull.vert\n\n( 23 )\n\ndist.edge2\n\n(6)\n\n-5\n\n0\n\n20 40 60 80\nTimes selected\n\n100\n\n0\n\n-\n\n20 40 60 80\nTimes selected\n\n100\n\nFigure 3.13: Selected variables in the unknown boundary models and the number\nof times each term is selected. Results are given for the base models (left) and\naugmented models (right). The change in the ranking for each term is highlighted\nin blue and the total number of times selected is given in parentheses.\nThe equivalent process of the identification of the influential points from Section 3.4.2.3 is done for the unknown boundary case models as well. Figure 3.14\n\n71\n\n3.5 Results\n\nshows the number of points which were identified as influential by all models was\naround 250 which is very close to the previous results in the unit square boundary\nin Figure 3.6. Figure 3.6 and 3.14 summarizes that there are many influential points\nwhich are commonly identified by all 100 models. This shows that influential points\nmay have features in common and the data augmentation process we perform has\nan importance to deal with the predictive performances of the base models. There\nare approximately 1000 influential points, and 816 of them are identified as influential in the unit square base models. This demonstrates that mostly the same points\nare identified as influential. Therefore, another analysis on the influential points is\n\n0\n\n20\n\nTimes identified\n40\n60\n\n80\n\n100\n\nnot necessary.\n\n1\n\n250\n\n500\nInfluential point index\n\n750\n\nFigure 3.14: Index of the influential points and how many times they are identified\nas influential.\nThe estimated smooth components in Figure 3.15 shows that curves for base models in gray and augmented models in black which shows some differences. The\ndata augmentation here also provides the training data to have some infrequently\nobserved data points. Hence the augmented models are trained to estimate such\npoints more accurately. The residuals patterns are very similar to the unit square\nboundary results.\nThe spatial patterns of the MSE in Figure 3.17 and at the transects in Figure 3.18\nhave some overall similarities but also differences in specific parts. For instance, the\nbase and augmented models almost perform equally at the corner. This is noticeable\nin Figure 3.17 (a) and (b) near the corner, and top panel in Figure 3.18 where the\nfirst few boxplots are for the pixels near the corner and they overlap. There are no\nunusual boxplots for either of the base and augmented ensemble predictions so there\nare no extreme errors. Apart from the corners, the base models perform better than\n\n72\n\n1.0\n\nchull.edge\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\ndist.edge2\n\n4\n\n6\n\n8\n\n10\n\n12\n\n14\n\n0.004\n0.000\n-0.004\n\n0.000\n\n0.010\n\n0.020\n\n0.030\n\nchull.vert\n\n0.000\n-0.004\n\n-0.010\n\n-0.004\n\n0.000\n\n0.000\n\n0.004\n\n0.0\n\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n-0.008\n\n0.8\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\nm\n\n-0.004\n\n0.6\n\n0.004\n\n0.4\n\n0.010\n\n0.2\n\nchull.per\n\n0.004\n\n-0.004\n\n-0.004\n0.0\n\nchull.area\n\ninf.area\n0.000\n\n0.004\n\ny.coord\n\n0.000\n\ninf.area\n0.000\n\n0.004\n\nx.coord\n\n0.00 0.01 0.02 0.03 0.04\n\n3.5 Results\n\n0.0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n140\n\n180\n\n220\n\n260\n\nFigure 3.15: Estimated smooth components of the GAMs in the individual base\nand augmented models. Black lines are the estimated smooth components for the\naugmented models that are overlaid for 100 models, and the estimated components\nfor the base models are shown in the background in gray lines.\n\nFigure 3.16: The normal quantile-quantile plot of residuals versus fitted values for\nindividual base (left) and augmented models (right).\nthe augmented models. The spatial patterns of the errors are shown in Figure 3.19\nand 3.20. The unusual appearance of the augmented models also exists here.\n\n73\n\n3.5 Results\n\n(a)\n\n(b)\nSq. Error\n\nSq. Error\n\n2.0e-05\n\n2.0e-05\n\n1.5e-05\n\n1.5e-05\n\n1.0e-05\n\n1.0e-05\n\n5.0e-06\n\n5.0e-06\n\n0.0e+00\n\n0.0e+00\n\n(c)\n\n(d)\nSq. Error\n\nSq. Error\n\n2.0e-05\n\n2.0e-05\n\n1.5e-05\n\n1.5e-05\n\n1.0e-05\n\n1.0e-05\n\n5.0e-06\n\n5.0e-06\n\n0.0e+00\n\n0.0e+00\n\nFigure 3.17: The squared error of infinite plane area and predicted area averaged\nover pixel bins for (a): base models, (b): augmented models, (c) unit square area,\nand (d) convex hull area.\n\n74\n\n2.0e-05\n\n3.5 Results\n\nDiagonal\n\nBase Ensemble\nAugm. Ensemble\n\n1.5e-05\n\nUnit Square\n\n0.0e+00 5.0e-06\n\nMSE\n1.0e-05\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n2.0e-05\n\nDistance\n\nEdge\n\nBase Ensemble\nAugm. Ensemble\n\n1.5e-05\n\nUnit Square\n\n0.0e+00 5.0e-06\n\nMSE\n1.0e-05\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n2.0e-05\n\nDistance\n\nCentre\n\nBase Ensemble\nAugm. Ensemble\n\n1.5e-05\n\nUnit Square\n\n0.0e+00 5.0e-06\n\nMSE\n1.0e-05\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\nDistance\n\nFigure 3.18: The boxplots of MSE for predictions from individual base models\n(blue) and augmented models (red) over pixel bins along different transects. The\nensemble predictions are shown with the solid points in the same colour at each\ntransect bin. The MSE for unit square and convex hull area are shown with square\nand diamond shaped points.\n\n75\n\n3.5 Results\n\n(a)\n\n(b)\nError\n\nError\n\n0.004\n\n0.004\n\n0.002\n\n0.002\n\n0.000\n\n0.000\n\n-0.002\n\n-0.002\n\n-0.004\n\n-0.004\n\n(c)\n\n(d)\nError\n\nError\n\n0.004\n\n0.004\n\n0.002\n\n0.002\n\n0.000\n\n0.000\n\n-0.002\n\n-0.002\n\n-0.004\n\n-0.004\n\nFigure 3.19: The error of infinite plane area and predicted area averaged over pixel\nbins for (a): base models, (b): augmented models, (c) unit square area, and (d)\nconvex hull area.\n\n76\n\n0.004\n\n3.5 Results\n\nDiagonal\n\nBase Ensemble\nAugm. Ensemble\n\n0.002\n\nUnit Square\n\n-0.004\n\n-0.002\n\nError\n0.000\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.004\n\nDistance\n\nEdge\n\nBase Ensemble\nAugm. Ensemble\n\n0.002\n\nUnit Square\n\n-0.004\n\n-0.002\n\nError\n0.000\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\n0.004\n\nDistance\n\nCentre\n\nBase Ensemble\nAugm. Ensemble\n\n0.002\n\nUnit Square\n\n-0.004\n\n-0.002\n\nError\n0.000\n\nConvex Hull\n\n0\n\n0.1\n\n0.2\n\n0.3\n\n0.4\n\n0.5\n\nDistance\n\nFigure 3.20: The boxplots of mean error for predictions from individual base models\n(blue) and augmented models (red) over pixel bins along different transects. The\nensemble predictions are shown with the solid points in the same colour at each\ntransect bin. The MSE for unit square and convex hull area are shown with square\nand diamond shaped points.\n\n77\n\n3.6 Classification of boundary-affected points\n\n3.6\n\nClassification of boundary-affected points\n\nArea prediction for Voronoi cells discussed in Section 3.4 is useful for the area\nprediction of both the interior and edge cells. The area prediction results show that\nthe predicted area differs from what is observed especially near boundaries. This\ngives an idea that the clipped cell area does not reflect the true cell area since the\ncells are affected by the boundaries. On the other hand, the interior points that are\ncloser to the centre of the region are less likely to be affected by the boundaries.\nThe boundary-affected points constitute approximately 28% of a test data with\nsize ntest = 3 × 105 which we will use for the classification of the boundary-affected\npoints. This section aims to give an idea on the classification of cells that are likely\nto be effected by the boundaries and performs a simple logistic regression.\nThe same training sets from Section 3.4 are used to create individual models for the\nprediction of the probabilities of being boundary-affected. The model in equation\n(3.7) is modified to a generalized linear model with Binomial family to fit logistic\nregression models. The binary response variable Y is denoted as p = P (Y =\n1|θ) that indicates the probability of a point to be boundary-affected given θ =\n{x1 , x2 , . . . , z3 , z4 , . . . , z9 } and the relationship between the predictors and log-odds\nis written as\nlog\n\np\n= β 0 + β 1 x1 + β 2 x2 + · · ·\n1−p\n\n(3.12)\n\nand the probabilities are recovered as\np=\n\neβ0 +β1 x1 +β2 x2 +···\n.\n1 + eβ0 +β1 x1 +β2 x2 +···\n\n(3.13)\n\nLet p̂ be the prediction matrix of probabilities calculated using individual logistic\nregression models that are fitted to the previously generated training subsets. Hence\nthe p̂ is created using the individual logistic regression models in the test set as\n\n\np̂11\np̂21\n\np̂ =   ..\n.\n\np̂12\np̂22\n..\n.\n\np̂13\np̂23\n..\n.\n\n···\n···\n..\n.\n\n\np̂1t\np̂2t\n\n..\n.\n\nntest = 1, . . . , 3 × 105 and t = 1, . . . , 100.\n\np̂nts 1 p̂nts 2 p̂nts 3 · · · p̂nts t\n\nThen the ensemble predictions of the probabilities in the test set are calculated as\np̃ =\n\nt\n\u0010 1 X\n\n100 j=1\n\nt\n\np̂1t\n\nt\n\n\u0011>\n1 X\n1 X\np̂2t · · ·\np̂n t .\n100 j=1\n100 j=1 ts\n\n78\n\n3.7 Alternative data scenarios\n\nThe prediction results in the test set are illustrated using the receiver operating\ncharacteristic (ROC) curves that present a useful evaluation of the performance of a\n\nPred.\n\nbinary classifier depending on the measures of sensitivity and specificity at different\nthresholds. The ROC curve is calculated based on the values on the confusion\n\nBoundary-affected\nUnaffected\n\nActual\nBoundary-affected Unaffected\nTrue Positives\nFalse Positives\nFalse Negatives\nTrue Negatives\n\nTable 3.7: The confusion matrix table.\nmatrices as in Table 3.7. We calculate the sensitivity and specificity to construct\nthe ROC curve for all possible thresholds.\nTrue Positive Rate = Sensitivity =\n\nTrue Positives\nTrue Positives + False Negatives\n\nFalse Positive Rate = (1 − Specificity) =\n\nFalse Positives\nFalse Positives + True Negatives\n\nThe ROC curve in Figure 3.21 summarizes all confusion matrices from all possible\nthreshold values. The varying threshold values are used to calculate the true positive\nand false positive rates for each threshold, and the calculated rates construct the\ncurve in Figure 3.21. The curve close to the top-left corner indicates the good\nperformance of the model to classify the boundary effected cells. The threshold\nthat gives the desired rates can be selected as the optimal threshold. A random\nclassifier that has equal true positive and false positive rates gives the diagonal line.\n\n3.7\n\nAlternative data scenarios\n\nWe considered two types of modeling strategies; in the case of known and unknown\nboundaries. These two types of conditions can cope with many important data\nstructures but cannot cover all possible data scenarios. It is not practical to go\nover all boundary scenarios, however, the methodology we use can be adopted for\nalternative scenarios possibly with appropriate modifications.\nThe are two main stages in our study; first, a large data set is created through\nthe simulation where the settings of the simulation such as which cell properties\nto calculate are carefully chosen for a specific point pattern type. Second, the\n\n79\n\n0.8\n0.6\n0.4\n0.2\n0.0\n\nTrue Positive Rate (Sensitivity)\n\n1.0\n\n3.7 Alternative data scenarios\n\n0.0\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1.0\n\nFalse Positive Rate (1- Specificity)\n\nFigure 3.21: ROC curve created from the confusion matrices for the test data.\nmethodology to predict the cell area or classify the boundary-affected cells should\nbe considered where we gave the major emphasis on the prediction of cell area. The\nmodels we created can be used for any type of point pattern and boundary cases,\nbut we have not checked the accuracy of the predictions in this chapter. Therefore,\nwe will investigate and explain the usage of the models for general data scenarios\nin Chapter 4.\nWe considered the set of n recorded points X ∈ Ω where the sampling region is a\nunit square Ω = [0, 1]2 and X follow a homogeneous Poisson process so n ∼ P o(ρ|Ω|)\nwhere |Ω| is the area of the region. The simulation study is designed to generate a\ndata set to learn more about the properties of homogeneous Poisson points. Then\nthe parts of this data set is used as training, and validation sets to fit and evaluate\nmodels.\nThe R code in Appendix B calculates the statistical properties of homogeneous\nPoisson points with intensity ρ = 200 for a single realization. Given a set of homogeneous Poisson points within a finite region Ω, it performs the Voronoi tessellation\nof points, randomly select a point and calculate the cell properties which are listed\nin Table 3.1. We repeated the process given in the code for one million independent\nrealizations, and by randomly sampling a point at each realization and recording\nthe cell properties, the entire data set is created. If the similar path is followed\nto repeat the simulation for a different data or boundary scenario, the data obtained from the simulation can be used for further purposes such as fitting models\n\n80\n\n3.8 Conclusions\n\nto predict a cell property or a classification case.\n\n3.8\n\nConclusions\n\nThis chapter discussed a thorough analysis of constrained Voronoi tessellation cell\narea due to imposed boundaries and provides ways to deal with the issues caused\nby boundaries. Since the data points in a finite region lack neighbour points beyond\nthe boundaries, the boundaries determine the characteristics of cells that lie on the\nboundary or close to the boundary. However, the base and augmented models we\ncreated treats the Voronoi cells as they are in a larger region or in an infinite plane.\nThe Voronoi tessellation has a wide use in spatial data analysis and we demonstrated how its statistical properties change near the boundaries. There are ways\nto reduce the problems that boundaries cause by fitting regression models that\npredict the cell area. The base and augmented ensemble models are the two approaches we proposed. For the general use, base models perform satisfactorily well.\nAugmented models on the other hand are able to improve some of the weaknesses\nof the base models such as reducing the extreme errors, but not have a sufficient\nglobal performance. Therefore, one could decide whether to achieve a good global\nperformance, or to minimize the largest errors. Based on accurate area prediction,\nwe suggest the use of base models due to its global performance.\nOne of the circumstances of this chapter is the consideration of particular boundary\ntypes and point patterns. Since it is not possible to cover all possible scenarios, we\nconsidered the most important cases that may have a general use. However, as in\nthe previous section, we explained how one can perform the simulation by adopting\nthe code we provided which can be modified easily for different boundary types and\npoint patterns.\n\n81\n\nChapter 4\nRobustness of area prediction\nThis chapter extends the study on the area prediction of Voronoi cells described in\nChapter 3 by considering homogeneous Poisson points with varying intensity cases,\nand the situations where the spatial data shows regular and clustered patterns.\nThe main objective of this chapter is to develop generalized versions of the previous\nmodels so they can be applied to a wide range of data cases that have different\nspatial characteristics. More importantly, our proposed approach aims to allow the\nmodels to be applicable to real data sets as well.\nFirst, we start by testing the models from Chapter 3 which were created using\ntraining sets of Voronoi cells from homogeneous Poisson points with a specific intensity. Section 4.1 explores the beahviour of these models on the test sets that\ncontain Voronoi cells of homogeneous Poisson points from different intensities. In\nSection 4.2, we describe regular and clustered points based on an existing method,\nand focus on the local intensities (i.e. the highly clustered parts) that may be\nalso different than the global intensity of the points. Then we propose a way that\nthe models created in Chapter 3 can be updated based on the local intensities to\nimprove the prediction performance. Finally, we use the updated versions of the\nmodels for area prediction for regular and clustered points in a simulation study\nand real data sets in Section 4.3, and an overall summary is given in Section 4.4.\n\n4.1\n\nMisspecification of intensity\n\nChapter 3 considered models that predict the true area of Voronoi cells given the\nother properties of the cells. These models are fitted using a training data that\ncontains properties of Voronoi cells from homogeneous Poisson points with point\n\n82\n\n4.1 Misspecification of intensity\n\nintensity ρ0 = 200. However, it is uncertain how the models perform for data sets\nwith varying point intensities. more specifically, we have not tested the model for\ndata sets that has ρ = 50 or ρ = 500. In this chapter, we consider the cases of\nvarying intensities which we call as misspecified intensity which is an often case in\nthe real data. Therefore, the aim is to check the robustness of the models that are\nfitted to the training data with intensity ρ0 on test data sets that have number of\npoints n ∼ P o(ρ) where ρ 6= ρ0 . The choices of different ρ values will be discussed\nlater.\nTo investigate the robustness of the models for misspecified intensities, the simulation study is extended to generate new training and test sets. A simulation is\nperformed which we generate data sets of homogeneous Poisson points with intensities ρ ∈ {50, 100, 200, . . . , 600}. First, we consider each data separately and fit\nadditive models for each case. Additionally, these models are used for area prediction for unrelated intensities. For instance, we fitted separate models for each of\nρ ∈ {50, 100, 200, . . . , 600} but we used the model fitted for ρ = 50 data on ρ = 600\ndata to see how the model with misspecified intensity performs. The purpose of this\napproach is to detect if any issues occur with misspecified intensity, and propose\nways to improve the predictions in such situations. Otherwise, relying on a single\nmodel for any data set may not be accurate.\nTo compare all cases, let ρ and ρ? indicate the intensity of the training and test data\nrespectively. Results are summarized in Table 4.1 in terms of the mean squared\nerrors. The predicted and true area are standardized as Âi = ρt Âi,ρ?t and A?i =\nρt Ai,ρ?t hence E(Âi ) = E(A?i ) = 1 for cell areas associated with points xi , i =\n1, . . . , n for all intensity cases t = 1, 2, . . . , 7, and the values in the table are MSE ×\nP\n102 . The mean squared error is calculated as MSEi = ni (Âi − A?i )2 /n. The rows\nin the table indicate the MSE for a specific data intensity ρt when different models\nthat has ρ?t are used. The top panel shows the results for base models and the\nbottom panel is for augmented models. The blue colour shows the smallest MSE\nachieved for each data. The most important conclusion from the table is that the\nsmallest MSE is always achieved using the models which are fitted to data which\nhave the same intensity as the data being analysed. Usage of models from different\nintensities, that is when ρt 6= ρ?t , gives a larger MSE. Hence, it is not appropriate to\nuse a model when there is a mismatch between the training and test set intensities\n(ρt and ρ?t ). There is a better performance of the base models as concluded in\nChapter 3.\n\n83\n\n4.1 Misspecification of intensity\n\nData\nintensity ρ\n50\n100\n200\n300\n400\n500\n600\n\n50\n4.48\n4.66\n3.22\n2.65\n2.27\n1.99\n1.83\n\n50\n100\n200\n300\n400\n500\n600\n\n6.21\n5.98\n4.87\n4.17\n3.75\n3.42\n3.07\n\n100\n7.81\n3.09\n3.01\n2.40\n2.06\n1.80\n1.65\n\nModel intensity, ρ?\n200\n300\n400\n500\n8.45 9.23 11.95 12.46\n4.79 5.09 5.73 6.56\n2.22 3.01 3.11 3.37\n2.25 1.81 2.24 2.31\n1.89 1.85 1.51 1.86\n1.65 1.60 1.56 1.31\n1.49 1.43 1.40 1.39\n\n600\n17.70\n7.29\n3.41\n2.44\n1.87\n1.57\n1.19\n\n10.94\n5.05\n5.14\n4.69\n4.14\n3.93\n3.62\n\n10.43\n5.90\n3.70\n3.54\n3.16\n2.99\n2.76\n\n12.07\n6.69\n3.84\n2.81\n2.31\n2.10\n1.75\n\n10.93\n6.39\n4.34\n3.02\n2.91\n2.77\n2.57\n\n11.09\n6.36\n3.96\n3.01\n2.25\n2.36\n2.19\n\n14.45\n6.61\n3.92\n2.95\n2.50\n2.09\n2.07\n\nTable 4.1: Mean squared errors of area prediction for base (top panel) and\naugmented models (bottom panel). The rows are for the data set intensities\nρ ∈ {50, 100, . . . , 600} and columns are for the models that is fitted for each\nρ? ∈ {50, 100, . . . , 600}. The case when ρt = ρ?t for t = 1, . . . , 7 indicates the\nusage of the same model fitted for the data that has intensity ρt , and ρt 6= ρ?t\nindicates the usage of models fitted from different data intensities.\nIn this experiment, another consideration might be to evaluate the joint performance\nof models from other models that has the lower and higher intensities. For instance,\nthe area prediction for ρt = ρ?t is Âi,ρ?t for a specific intensity. As an alternative\napproach, we take the weighted average (Âi,ρ?t−1 + Âi,ρ?t+1 )/2 to check how robust the\nprediction of Âi,ρ?t is to misspecified intensity. The MSE of the weighted average of\npredictions from models ρ?t−1 and ρ?t+1 are given in Table 4.2. The results show that\nthe weighted average is not very accurate. In particular, for augmented models,\na lower MSE is obtained by just using a prediction model based on the higher\nintensity ρ?t+1 . However, for base models, this is not true and averaging the two\npredictions gives marginally lower MSE.\nOne possible violation of the modelling assumptions is the misspecification of the\nintensity which can lead to problems in the area prediction. Some of the cell properties such as the cell area and perimeter depend on ρ. When the model is fitted\nusing a training set with ρ0 , and the test set has intensity ρ 6= ρ0 , the mean cell\narea and perimeter for the training and test data will be different. Therefore, the\nissue can be approached by modifying the variables (that depend on ρ) in the test\n\n84\n\n4.2 Regular and clustered point patterns\n\nData\nintensity ρ\n50\n100\n200\n300\n400\n500\n600\n\n50\n–\n–\n–\n–\n–\n–\n–\n\n100\n5.46\n4.60\n2.48\n2.38\n2.03\n1.78\n1.63\n\nModel intensity, ρ?\n200 300\n400\n500\n600\n8.26 10.02 22.16 17.48 –\n3.62 5.15 6.48 7.94 –\n2.94 2.46 3.14 3.23 –\n1.95 2.22 1.91 2.33 –\n1.92 1.60 1.84 1.60 –\n1.67 1.59 1.38 1.56 –\n1.51 1.43 1.40 1.24 –\n\n50\n100\n200\n300\n400\n500\n600\n\n–\n–\n–\n–\n–\n–\n–\n\n6.28\n5.60\n3.83\n3.73\n3.36\n3.13\n2.84\n\n8.99\n4.79\n4.50\n3.50\n3.34\n3.19\n2.95\n\n10.58\n5.97\n3.38\n3.17\n2.53\n2.61\n2.42\n\n12.09\n6.23\n4.02\n2.76\n2.65\n2.29\n2.27\n\n11.12\n6.43\n3.84\n2.86\n2.15\n2.19\n1.87\n\n–\n–\n–\n–\n–\n–\n–\n\nTable 4.2: Mean squared errors calculated from the weighted average of the predictions from models with ρ?t−1 and ρ?t+1 when the particular interest is the intensity\nρt . Results are given for base (top panel) and augmented models (bottom panel)\nrespectively. The smallest MSE in each row is highlighted in blue colour. The (–)\nsymbol indicates no weighted average is calculated since there are no more columns\non the left or right.\ndata appropriately with respect to ρ0 . The misspecified intensity can happen for\nthe intensity of the homogeneous Poisson points as we considered in this section, or\nit is possible to obtain variable local intensities for different sub regions of clustered\npoint patterns. The latter case will be considered in the next section by taking into\naccount various regular and clustered point patterns.\n\n4.2\n\nRegular and clustered point patterns\n\nThe foundation sources to study the point patterns include Cox & Isham (1980);\nCressie (2015); Cressie & Wikle (2015); Diggle (1983); Gelfand et al. (2010); Illian\net al. (2008); Ripley (1988, 2005) and Baddeley et al. (2015). Various ways to\ngenerate different point patterns are explained and discussed in these references.\nWe are interested in using a model to generate realizations of regular and clustered\npoint patterns. The saturation process by Geyer (1999) explained in Section 1.5 has\npractical features that the different types of point patterns can be both analysed\nand simulated by the implementation of the method in a R.\n\n85\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\nThe saturation process can be performed in R using the routines in the spatsat\npackage as explained in Baddeley et al. (2015). There are two ways of choosing the\nparameters of the process, i ) the values of the parameters β, γ, r, s can be predefined,\nor ii ) the process can be fitted to a point pattern data, hence the parameters are\nestimated, then simulated realizations of the fitted point process are created using\nMetropolis-Hastings algorithm. There are also other well known processes such as\nthe Poisson cluster process, Neyman-Scott, and Bartlett-Lewis cluster processes, or\njittering the grid points, but the scheme we describe is more convenient in terms of\nthe flexibility and practicality of the control of the parameters.\nBy the modification of the parameter γ it is possible to generate realizations of\nregular and clustered point patterns. The different values of γ decides the magnitude\nof regularity and clustering. For instance, the homogeneous point pattern case is\nobtained by setting γ = 1. However, setting of γ < 1 results more regular patterns\nand γ > 1 clustered points where the more regular and clustered points are achieved\nby departure from γ = 1 increases.\nAn example of different point patterns generated from the saturation process is\nshown in Figure 4.1. From top-left to bottom-right, n points are generated for the\nvalues γ = 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 3, where n ∼ P o(200). The centre plot\nwhere γ = 1 indicates homogeneity which can be considered as the baseline and\nrealizations of regular and clustered points are shown for different values of γ < 1\nand γ > 1.\n\n4.3\n\nThe prediction of Voronoi cell area based on\nregular and clustered points\n\nArea prediction for regular and clustered points is not straightforward as in the\nhomogeneous Poisson point pattern case, but it is possible to use the models fitted\nfor homogeneous data with modified covariates of regular and clustered points as\nwe briefly mentioned. In this section, we explain the process of area prediction for\nregular and clustered points that aimed to be done by using the local intensities.\nLet the local intensities at the locations of the regular or clustered points {xi }ni=1\nbe ρi which can be estimated as ρ̂i .\nOne method to estimate the local intensity is to use the kernel smoothed intensity\nfrom the point pattern. Given a point pattern data, the method computes a fixedbandwidth kernel estimate of the intensity function of the related point process\n\n86\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\ng =0\n\ng = 0.25\n\ng = 0.5\n\ng = 0.75\n\ng =1\n\ng = 1.25\n\ng = 1.5\n\ng =2\n\ng =3\n\nFigure 4.1: Simulated points from Geyer’s saturation process (Geyer, 1999). Examples of inhibition or repulsion to homogeneity and to clustering or attraction are\nshown from top-left to bottom right with incremental magnitudes of repulsion and\nattraction where the plot with bold frame at the centre is the homogeneous case.\nThe intensity parameter β = 200, interaction radius r = 0.05, and the saturation\nthreshold s = 2 are fixed for all point patterns, and the interaction parameter γ\ntakes values 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 3 where γ < 1 indicates regularity,\nand clustering if γ > 1.\nusing an isotropic Gaussian kernel as the default option (Diggle, 1985). The edge\ncorrected intensity estimate at an arbitrary location u is\nρ̂(u) = e(u)\n\nX\n\nκ(xi − u)ωi\n\n(4.1)\n\ni\n\nwhere κ is the kernel function based on isotropic Gaussian distribution, ωi are the\nweights if assigned to the points, and e(u) is the correction term for bias at the\nedges defined as\nZ\n1\n=\nκ(v − u)dv\n(4.2)\ne(u)\nW\n\n87\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\nwhere W is the observation window. The edge corrected estimate of the local\nintensity is obtained through dividing the convolution of the Gaussian kernel by\nthe edge correction term e(u).\nAn example of the local estimate of the intensity over the region with edge correction\nis shown in Figure 4.2 using the method shown in (4.1) for the point patterns from\nFigure 4.1. The density.ppp functions in the spatstat package is used to compute\nthe kernel smoothed intensity given the point patterns.\n\nFigure 4.2: Kernel smoothed intensity of the point patterns.\nThe local intensities can be estimated at the data points rather than the entire\nregion which is particularly useful for area prediction. Given a set of points X γ =\n{xi ∈ [0, 1]2 ; i = 1, . . . , n} generated based on the value of γ, and n ∼ P o(ρ0 = 200),\nwe have a local estimate of the intensity ρ̂i at each point xi . The area prediction\nfor points is then performed as\nÂi =\n\np\nX\n\nfρ0 ,j (θij )\n\n(4.3)\n\nj=1\n\nwhere fj are the unknown smooth functions fitted in the additive model for area\nprediction, p is the number of predictors, θj are the covariates j = 1, 2, . . . , p such\n\n88\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\nas the raw cell area, perimeter, number of edges due to induced boundaries, cell\ntype and so on for p covariates in total. The model in (4.3) does not depend on the\nlocal intensity ρ̂i and may have the issues highlighted in Section 4.1, however the\nmodel can be improved as\np\nX\n?\nfρ̂i ,j (θij\n).\n(4.4)\nÂ?i =\nj=1\n\nThe model in (4.4) uses the feature of the ρ̂i to scale some of the covariates θj?\nthat depend on the ρ̂i . For instance, the scaled cell area due to boundaries is\np\nA?i = Ai ρ̂i /ρ0 , and the scaled perimeter is Pi? = Pi ρ̂i /ρ0 . These scaling factors are\ndefined based on the theoretical derivation of the expected cell area and perimeter\nfrom equation (2.1). Other covariates such as the number of cell edges do not\ndepend on the data intensity ρ0 . Although we only considered the scaling of the\ncell area and the perimeter based on different global or local point intensities, other\ncell properties such as the closest distance between the point and the boundary can\nalso be investigated to see whether it depends on the intensity.\nThe true area A0i is determined by generating the point pattern in a larger region\nΩ? = [−1, 2]2 , and studying the points inside the region Ω = [0, 1]2 . The shifting\napproach in Chapter 2 is not used because the periodic boundary conditions are not\nvery appropriate especially for clustered points. Consider clustered points at the\ncorner of the region where the cluster is clipped by the boundary. We only observe\nsome of the points that belong to the cluster. If shifting is applied, then the new\nneighbour points of the cluster may be far from the cluster. Therefore, the cell\narea A0i obtained by shifting for a sampled point in the cluster may be very large.\nInstead, we use an approach that simulates points in a larger region, Ω? = [−1, 2]2\nbut samples from the points in Ω = [0, 1]2 . The true cell area is calculated based on\nthe Voronoi tesselation of all points in Ω? = [−1, 2]2 . Therefore, irregularities such\nas clusters are not clipped, and the true area is calculated based on a continuum of\nthe cluster rather than shifting the clipped cluster to the centre.\n\n4.3.1\n\nResults for simulated data\n\nArea prediction is performed for the simulated data. In each simulation, sets of\nn points are generated for a specific value of γ, and the statistical properties of\na randomly selected point is calculated. This is done for 104 realizations for each\nvalue of γ. The data is used to predict the cell area of the randomly selected points\nusing models (4.3) and (4.4) and the results are summarized in the Table 4.3.\n\n89\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\nThe mean squared error values are given in Table 4.3 where the values are for\nMSE×106 , and their standard errors are in Table 4.4 for (SE×108 ). B and Ag\nnotations indicate the area prediction results using model (4.3) for the base and\naugmented models. On the other hand, B ? and Ag ? are the base and augmented\nmodel prediction results using the model (4.4). The base and augmented model\nresults are separated as the two main row panels (top and bottom) in Table 4.3.\nEach panel is also separated into three sub-panels which are for the global, interior\nand edge regions respectively. The interior region is defined as Ωin = [0.15, 0.85]2\nand edge region is for the points that are located within Ωed = Ω0in where Ω =\nΩin ∪ Ωed .\nThe MSE results in Table 4.3 show an overall better performance of model (4.4) in\nall irregular data situations both for base and augmented models. The MSE and\nthe standard deviation is extremely small for interior points, which is expected, but\ndifferences are more apparent for the edge points. In terms of the values of γ, the\nMSE is smallest when γ = 0 that is the most regular point pattern and highest for\nthe highly clustered points when γ = 3. The recommended model based on this\nexperiment is to use B ? (base model that use the local estimate of intensity).\nTo check whether the differences between MSE values from models (4.3) and (4.4)\nare significant, consider the MSE values with confidence intervals in Figure 4.3. The\nblack and red colours represent models (4.3) and (4.4) respectively for base models,\nand pale colours for the augmented models. Results are separated for global, interior\nand edge regions from left to right respectively. In the global case, B ? (•) always\nhave the smallest MSE. We see the same pattern for edge region which is the case\nthat is being of interest. Although all models give very small MSE values for the\ninterior region, B ? (•) is the smallest except when γ = 3. Also, the confidence\nintervals between the base and augmented models generally do not overlap which\nsuggests the base models are significantly better than the augmented models in\narea prediction. It is appropriate to keep relying on the model in (4.4) that uses\nthe estimated local intensities. However, we do not suggest a strict usage of the\nbase models since the augmented models reduce the maximum error, and the base\nmodels give the smallest global MSE. The preference between two models should\nbe based on which of these criteria is more important in a particular application.\n\n4.3.2\n\nResults for real data\n\nIn this section, the area prediction method is applied to several real data sets, all\nare available in the spatstat library in R. We selected four data sets finpines,\n\n90\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\n0\n\n0.25\n\n0.50\n\n0.75\n\n1\n\n1.25\n\n1.50\n\n2\n\n3\n\nGlobal\n\nB\nB?\nAg\nAg ?\n\n0.186\n0.163\n0.698\n0.639\n\n0.226\n0.196\n0.691\n0.629\n\n0.298\n0.243\n0.690\n0.614\n\n0.411\n0.334\n0.800\n0.717\n\n0.529\n0.429\n0.863\n0.749\n\n0.628\n0.520\n0.960\n0.840\n\n0.807\n0.631\n1.092\n0.934\n\n0.907\n0.758\n1.170\n1.059\n\n1.254\n1.173\n1.478\n1.477\n\nInterior\n\nB\nB?\nAg\nAg ?\n\n0.001\n0.000\n0.019\n0.020\n\n0.001\n0.000\n0.021\n0.017\n\n0.001\n0.001\n0.026\n0.015\n\n0.002\n0.001\n0.035\n0.015\n\n0.004\n0.003\n0.043\n0.017\n\n0.004\n0.002\n0.048\n0.019\n\n0.011\n0.006\n0.066\n0.030\n\n0.015\n0.013\n0.073\n0.050\n\n0.023\n0.038\n0.095\n0.112\n\nEdge\n\nγ\nCases\n\nB\nB?\nAg\nAg ?\n\n0.365\n0.321\n1.353\n1.239\n\n0.442\n0.385\n1.337\n1.219\n\n0.592\n0.483\n1.348\n1.210\n\n0.805\n0.656\n1.536\n1.393\n\n1.021\n0.830\n1.633\n1.437\n\n1.222\n1.016\n1.831\n1.625\n\n1.566\n1.227\n2.070\n1.798\n\n1.742\n1.458\n2.198\n2.006\n\n2.487\n2.314\n2.865\n2.849\n\nTable 4.3: Mean squared error of the predicted area using base B and augmented\nAg models. B indicates the base model, whereas B ? is a base model that uses the\nscaled covariates based on the estimated local intensities ρ̂i at the data points. The\nsame case applies for the Ag and Ag ? . Results are given in three row panels that\nare for global, interior and edge parts respectively. Columns show the MSE results\nfor each point pattern type based on the value of γ. In each column, results are\nobtained from 104 data points each of which is sampled from 104 realizations of\nindependent data sets. Results are for MSE × 106 .\n\n0\n\n0.25\n\n0.50\n\n0.75\n\n1\n\n1.25\n\n1.50\n\n2\n\n3\n\nGlobal\n\nB\nB?\nAg\nAg ?\n\n0.670\n0.532\n2.065\n1.957\n\n0.850\n0.674\n2.195\n2.000\n\n1.332\n0.903\n2.297\n2.036\n\n2.167\n1.235\n2.781\n2.401\n\n2.737\n1.653\n3.132\n2.560\n\n2.947\n2.098\n3.400\n2.873\n\n4.182\n2.741\n4.111\n3.253\n\n4.759\n3.595\n4.498\n3.922\n\n7.166\n5.940\n6.828\n5.897\n\nInterior\n\nB\nB?\nAg\nAg ?\n\n0.001\n0.001\n0.032\n0.018\n\n0.002\n0.001\n0.038\n0.018\n\n0.033\n0.031\n0.062\n0.038\n\n0.033\n0.007\n0.089\n0.036\n\n0.144\n0.127\n0.188\n0.126\n\n0.046\n0.024\n0.131\n0.054\n\n0.200\n0.156\n0.728\n0.528\n\n0.218\n0.209\n0.296\n0.562\n\n0.529\n0.581\n1.022\n0.939\n\nEdge\n\nγ\nCases\n\nB\nB?\nAg\nAg ?\n\n1.267\n0.998\n3.909\n3.661\n\n1.612\n1.270\n4.190\n3.748\n\n2.587\n1.735\n4.401\n3.880\n\n4.179\n2.340\n5.355\n4.520\n\n5.213\n3.105\n6.060\n4.773\n\n5.637\n3.983\n6.563\n5.396\n\n8.024\n5.222\n7.944\n6.100\n\n9.064\n6.827\n8.741\n7.347\n\n14.126\n11.671\n13.335\n11.459\n\nTable 4.4: Standard error of the MSE values from Table 4.3. Results are for SE×108 .\n\n91\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\nInterior\n\nEdge\n\n1.5\n\n3.0\n\n0.15\n\nGlobal\n\n0\n\n0.25\n\n0.5\n\n0.75\n\n1\n\ng\n\n1.25\n\n1.5\n\n2\n\n3\n\n2.5\n2.0\n1.0\n0.0\n\n0.0\n\n0.00\n\n0.5\n\n0.5\n\n0.05\n\n1.5\n\nMSE\n\n1.0\n\n0.10\n\nB\nB*\nAg\nAg*\n\n0\n\n0.25\n\n0.5\n\n0.75\n\n1\n\ng\n\n1.25\n\n1.5\n\n2\n\n3\n\n0\n\n0.25\n\n0.5\n\n0.75\n\n1\n\ng\n\n1.25\n\n1.5\n\n2\n\n3\n\nFigure 4.3: Confidence intervals for MSE values. The points (of all colours) show the\nMSE and the lines (of all colours) show the confidence intervals based on different\nmethods that are shown in the centre plot legend.\nlongleaf, spruces, and waka that have different spatial features. These data sets\nare created from different types of trees within specific sampling regions and are\nexamples of point pattern data. The locations of trees are marked by the height\nand the diameter of trees which make it a marked point pattern. However, we will\nonly use the tree locations as points in this chapter.\nWe additionally use a data set that contain the chemicals measured in the soil in\nBarro Colorado Island (BCI) at sampled locations. The BCI data has two-thirds\nof the locations from the equidistant grid points and one-thirds are sampled at a\nrandom isotropic direction with some distance from the regular points. Therefore,\nthis data set has a completely different nature compared to the tree data sets mentioned earlier. The BCI data is a geo-referenced data that has coordinate-based\nsampled points where chemical levels are measured. The BCI data, is collected by\na part of the Effects of soil-borne resources on the structure and dynamics of lowland tropical forests project by principal investigators: Jim Dalling, Robert John,\nKyle Harms, Robert Stallard and Joe Yavitt. The data that are publicly available\nin http://ctfs.si.edu/webatlas/datasets/bci/soilmaps/BCIsoil.html only\ncontains sampling locations and kriging estimates of the soil data. However, the\nraw soil data was obtained from Dalling et al. (2021) by personal communication.\nDescriptive information about the data sets is given in Table 4.5 and the locations\nare shown in Figure 4.4. In Table 4.5, the estimated parameter γ̂ is also given for\n\n92\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\neach data set; these fall in the range 0 ≤ γ ≤ 3 which we used in the simulations.\nThe γ̂ values indicate that spruces and BCI are the data sets that have regular\npattern, waka is almost completely homogeneous data, and finpines and longleaf\nhave clustering. Hence the presentation order of the data sets are decided based on\nthe γ̂.\nData set\n\nn\n\nγ̂\n\nΩ\n\nDescription\n\nspruces\n\n134\n\n0.28\n\n56 × 38 meter\n\nBCI\n\n300\n\n0.32\n\n1000 × 500 meter\n\nwaka\n\n504\n\n1.04\n\n100 × 100 meter\n\nfinpines\n\n126\n\n1.25\n\n10 × 10 meter\n\nlongleaf\n\n584\n\n1.60\n\n200 × 200 meter\n\nLocations of Norwegian spruces trees\nand diameters in a rectangle sampling\nregion in Saxony, Germany.\nSoil nutrient data for 13 different chemicals at the sampled locations in a rectangular sampling region in Barro Colorado Island.\nLocations and diameters of trees in\nsquare sampling region at Waka National Park, Gabon.\nLocations and diameters of pine\nsaplings in a Finnish forest.\nLocations and diameters of longleaf\npine trees in southern Georgia, USA.\n\nTable 4.5: Data set name, number of points n, estimated parameter γ̂, sampling\nregion Ω, and the description of the data sets.\nRipley’s K statistic\nThe real data sets can be diagnosed using the Ripley’s K function (Ripley, 1976,\n1977) that checks the spatial homogeneity (complete spatial randomness) in the\ndata. Let X be a set of points X = {x1 , x2 , . . . , xn } in two-dimensional region, then\nthe general form of the K statistic is defined as\nK̂(r) =\n\n1 X 1{d(xi , xj ≤ r)}\n,\nρ x 6=x ∈X\nn\ni\n\n(4.5)\n\nj\n\nwhere d(xi , xj ) is the Euclidean distance between the i-th and j-th points, 1 is\nthe indicator function that takes values 1 if the condition is true and 0 otherwise,\nwith a search radius r, ρ is the intensity of the points estimated as ρ̂ = n/|Ω|\nwhere |Ω| is the area of the region Ω within which all points are located. If the\nprocess is a homogeneous Poisson point process then K̂(r) = πr2 which indicates\na complete spatial randomness whereas departure form πr2 means clustered or\ndispersed pattern (Kiskowski et al., 2009).\n\n93\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\nThe following standardization is recommended in Besag (1977)\nq\nL̂(r) = K̂(r)/π\n\n(4.6)\n\nso the expected value of L function is r for homogeneous data. The randomness\nof the points are tested using the hypothesis Ho : L̂(r) − r = 0 (X follows a\nhomogeneous Poisson process with intensity ρ). In the violation of Ho , positive\nvalues of L̂(r) − r indicate clustering and negative value indicate dispersion. In\nChapter 4, we will consider regular and clustered points both from simulations and\nexamples from real life data. The K function and the hypothesis would be useful\nto examine the pattern of real data examples particularly.\nResults for the K function are shown in Figure 4.5 for the real data sets. The\ntop-left plot in Figure 4.5 is an example of the K function from the simulated data\nfor γ = 0.25, 1, 1.5. Plots from top-centre to the bottom-right are the K functions\nfor real data sets with the same presentation order as in Table 4.5. In Figure 4.5\n(a), which is obtained from the simulated data, the red dashed line is the expected\nK(r) for an independent simulated homogeneous Poisson points. The black lines\nare obtained using point patterns when γ ∈ {0.28, 1, 1.50}. The black line above the\nred line when γ = 1.50 is the expected K̂(r) for observed data locations, indicating\nthat the number of expected points within the search region (isotropic distance r\nfrom the data locations) is higher compared to the red line. In this case the, the line\nfor γ = 1.50 indicates clustering. On the other hand, the black solid line obtained\nfor γ = 1 almost overlap with the red line since γ = 1 indicates homogeneity of\npoints. The case when γ = 0.28, the black line is always below the red line that is\ninterpreted as the regularity.\nThe K function plots for each real data set are shown in Figure 4.5 (b – e) separately.\nThe black line in Figure 4.5 (b) which is for spruces is under the red line that\nindicates regularity. The waka data set (c) is a clear example of homogeneous\npoints since black line follows exactly the same pattern as the red line at different\nr. The finpines data in (d) is slightly clustered since the black curve is above the\nred curve. The last plot (e) indicates more clustering of locations in the longleaf\ndata. The BCI data set is not included in Figure 4.5 since it is a geo-referenced\ndata and the K function is suitable for point pattern data.\nThe advantage of using various types of data sets is to see how the area prediction\nworks for such different scenarios. Therefore, the validity and the limitations of the\nmodeling approaches that we suggested can be evaluated.\n\n94\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\nBCI ( g^ = 0.32)\n\nspruces ( g^ = 0.28)\n\nwaka ( g^ = 1.04)\n\nfinpines ( g^ = 1.25)\n\nlongleaf ( g^ = 1.60)\n\nK poi s (r )\n\nK poi s (r )\n\nK (r )\n150\n\n0.05\n\n0.10\n\n0.15\n\n0\n\n0\n\n0.00\n0.00\n\n0\n\n2\n\nr\n20\n\n(d) finpines\n\n4\n6\nr (metres)\n\n8\n\n0\n\n5\n\n10\n15\nr (metres)\n\n20\n\n25\n\n(e) longleaf\nK^ i so (r )\n\nK (r )\n6000\n\nK^ i so (r )\n\nK (r )\n10\n15\n\n(c) waka\nK^ i so (r )\n\nK (r )\n500 1000\n\n250\n\n(b) spruces\nK^ i so (r )\n\n50\n\n0.02\n\nK (r )\n0.04\n\n0.06\n\n(a) simulation\ng = 0.28\ng =1\ng = 1.50\n^ (r)\nK\npois\n\n2000\n\nFigure 4.4: Locations of the data points in the real data sets. From top-left to\nbottom-right, spruces, Barro Colorado Island, waka, finpines, and longleaf data\nare shown. The estimated parameter γ̂ = 0.28, 0.32, 1.04, 1.25, 1.60 is given for each\ndata set respectively.\n\nK poi s (r )\n\n0\n\n0\n\n5\n\n2000\n\nK poi s (r )\n\n0.0\n\n0.5\n\n1.0\n1.5\nr (metres)\n\n2.0\n\n2.5\n\n0\n\n10\n\n20\n30\nr (metres)\n\n40\n\n50\n\nFigure 4.5: Ripley’s K function plots for (a) simulated data for different values of\nγ, (b) spruces, (c) waka, (d) finpines, and (e) longleaf data sets. The red line\nis known analytically for the K-function. The black line is the expected K̂(r) from\nobserved locations.\n\n95\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\n-0.6 -0.3 0.0 0.3 0.6\n\n-0.4\n\n0.0\n\n0.4\n\n-0.2 -0.1 0.0 0.1 0.2\n\n-0.4\n\n0.0\n\n0.4\n\n-2\n\n-1\n\n0\n\n1\n\n2\n\nFigure 4.6: The adjustment pattern on the cell area using base B ? models. The\ndifference between the observed and adjusted area is calculated as Ai − Â?i where Ai\nis the calculated area due to the given rectangular boundary and Â?i is the predicted\narea. From top-left to bottom-right, the data sets follow the same order.\nArea prediction results for real data are presented in Figure 4.6 using base models,\nand in Figure 4.7 using the augmented models by illustrating how the cells are\nadjusted. It looks that the predicted area for the cells near the edges are different\nthan the observed cell area, and the predicted area for interior cells is similar to\nthe observed cell area. That means the edge cells are likely to be adjusted when\nthe models are used. The blue and red coloured cells indicate shrinkage and expansion respectively. Some very large cell areas are reduced and the small ones are\nexpanded in the prediction. As expected, interior cells are white indicating that no\nadjustments are happening to interior cells.\n\n96\n\n4.3 The prediction of Voronoi cell area based on regular and clustered\npoints\n\n-1.0 -0.5 0.0 0.5 1.0\n\n-0.8 -0.4 0.0 0.4 0.8\n\n-0.2\n\n-1\n\n0\n\n1\n\n0.0\n\n0.2\n\n-2\n\n0\n\n2\n\nFigure 4.7: The adjustment pattern on the cell area using augmented Ag ? models.\nThe difference between the observed and adjusted area is calculated as Ai − Â?i\nwhere Ai is the calculated area due to the given rectangular boundary and Â?i is the\npredicted area. From top-left to bottom-right, the data sets follow the same order.\n\n97\n\n4.4 Conclusion\n\n4.4\n\nConclusion\n\nThis chapter investigates the robustness of the area prediction by testing the performances of models on data sets with misspecified intensities. The weak performance\nof the models are improved by using the local estimate of the intensity at data locations. The local estimate of intensity is used in the models to scale the covariates\nof the cells. The improvement is achieved in different degrees of regularity, and\nclustering of points that contain the extreme examples as well. The base models\ngive the smallest overall MSE compared to the augmented models. However, if one\nwould wish to reduce the maximum error, then augmented models may be preferred.\nIt is important to decide on the best model based on the conclusions from the\nsimulated data and use the suggested model in the further studies where the Voronoi\ntessellation cell area is useful. B ? is selected as the best model to predict the area\nand it may be more appropriate for the real data sets. The real data sets are\nexamples of homogeneous, regular and clustered point patterns. It is useful to apply\nthe method on such data sets that pushes the assumptions such as having large\nnumber of points and rectangle boundary. However, the area prediction method\ngive reasonable results in the real data sets such as expanding very small edge cells\nor shrinking very large edge cells. These leads to the usage of the adjusted area as an\nalternative weight method in lifting. We will explore this application in Chapter 6,\nafter first explaining the lifting method in Chapter 5.\nConsequently, the area prediction for Voronoi cells for homogeneous points in Chapter 3 and for the regular and clustered points in the current chapter aims to treat\nthe Voronoi cells in the bounded region as if they are in an infinite plane by the\nadjustments in the cell areas. The approach we devised in Chapter 3 and 4 might be\nuseful for the methods such as the lifting scheme that uses the Voronoi cell area. In\nthe next chapters the area prediction approaches will be combined with the lifting\nscheme.\n\n98\n\nChapter 5\nLifting scheme\nIn this chapter, we explain the background of the lifting scheme, and Voronoi\ntessellation-based lifting in the two-dimensional case particularly. Lifting is an extension of wavelet methods and has grown out for the need of a generalized version\nof wavelet decomposition. The lifting scheme is an instrument that we use in the\nremainder of this thesis as an application of the ideas and approaches we developed\nin the previous chapters based on Voronoi tessellations. In this background chapter,\nwe highlight some important ideas of wavelet methods and their limitations, and\nexplain the need for a generalized version. We explain the general framework of the\nlifting scheme, discuss thresholding methods and why we use them, and illustrate\na few initial steps of Voronoi tessellation-based lifting. In the following chapters,\nwe will consider using various weight methods in lifting such as the observed cell\narea using boundaries, and predicted cell area using the methods we discussed in\nChapters 3 and 4 to reduce the boundary effects.\nIt is possible to understand the mathematical framework of lifting without any prior\nknowledge of wavelet methods, but since the wavelet theory includes the fundamental concepts that the lifting scheme is built upon, we give a short introduction to\nwavelet methods and explain how the lifting scheme aims to improve some of its\naspects in Section 5.1. Moreover, we describe the lifting scheme in two dimensions\nin Section 5.3, which we will use in the remainder of this thesis in conjunction with\nVoronoi tessellations. We describe some important thresholding methods, and outline their usage in the context of wavelet analysis and lifting in Section 5.4. We\nfinally give an example in Section 5.5 to show the steps of the algorithm and how\nthe Voronoi tessellation-based lifting scheme works in two dimensions.\n\n99\n\n5.1 Background\n\n5.1\n\nBackground\n\nThe lifting scheme is referred to as a second-generation wavelet method. The general\nform of lifting is explained in Sweldens (1998) who defined the idea behind lifting as\nan iterative transformation of the data starting with localised or fine-scale details\nand working up to broader or coarse-scale patterns. Unlike conventional wavelet\nmethods, lifting can be applied to irregularly spaced data with an arbitrary sample size which is often the case in reality. Lifting also relaxes the requirement of\nequidistant data with size n = 2J , J ∈ N of the wavelet methods. For the preliminary studies that lead to the construction of lifting scheme, earlier work of Sweldens\n(1995, 1996) can be reviewed.\nLifting also has advantages over well known methods to analyze spatial data such\nas Gaussian process regression (kriging) or model based spatial methods. Lifting\nis capable of modeling irregularities such as sharp discontinuities or spikes which\nother methods tend to do poorly on, such as over-smoothing at the boundary of the\ndiscontinuity. Pope et al. (2021) aimed to reduce such issues by partitioning the\nsampling region using Voronoi tessellations and fitting a Gaussian process in each\nsub-region (Voronoi cell) separately. The method does a good job if the data has a\nstep change, but less well in the case of repeated wiggles. However, wavelet-based\nor lifting-based approaches have been demonstrated to be effective approaches to\ndeal with these kind of situations.\n\n5.2\n\nDiscrete wavelet transform\n\nWavelet methods are commonly used in the estimation of functions corrupted by\nnoise. For an introduction to wavelets, good resources include Daubechies (1992)\nand Vidakovic (1999). The estimation of the true function involves the transformation of the noisy data into a set of coefficients, and shrinkage/thresholding procedures to remove noise followed by the inverse transform on the modified coefficients.\nThe lifting scheme serves the same purpose as an extension of the idea of multiresolution analysis and discrete wavelet transform (DWT) introduced by Mallat (1989).\nThe multiresolution analysis allows the decomposition and reconstruction of noisy\ndata.\nWavelet-based function estimation methods often assume the following model setting:\nyi = f (ti ) + \u000fi ,\n\n100\n\n5.2 Discrete wavelet transform\n\nwhere f (ti ) is the function that we are interested in at point ti which may be a time\n(in one-dimensional case) or location (two-dimensional case), \u000fi are Gaussian noise,\nassuming \u000fi ∼ N (0, σ 2 ) independently, and yi are the observed noisy data.\nConsider we have observations y(ti ), the discrete wavelet transform assumes that\n{ti = i/N : i = 0, 2, . . . , N } are discrete equispaced points in time or space. DWT\nrequires N = 2J for some positive integer J hence the full transform can be carried\nout. If the full transform is not desired, this transform can be performed for the\nfirst J0 steps which is called as the non-decimated DWT.\nThe idea of the discrete wavelet transform is to transform a vector of noisy data y\ninto a vector of coefficients d using the low pass filter H = {hk } and high pass filter\nG = {gk } where hk and gk are the coefficients of filters. We first define the scaling\ncoefficients as cJ,i = y(ti )> at level J and perform the discrete wavelet transform at\nlevels j = J − 1, . . . , 0 and by calculating\n\ncj,i =\n\nX\n\nhn−2i cj+1,n\n\n(5.1)\n\ngn−2i cj+1,n\n\n(5.2)\n\nn\n\ndj,i =\n\nX\nn\n\nthat gives a collection of wavelet coefficients dj,i and an individual coefficient c0,0\nat the end of the full transform. This transform is an orthogonal transform of the\nobserved data yi of length N into the wavelet domain. The yi can be reconstructed\nby applying the inverse transform.\nSince the discrete wavelet transform is a linear transformation of the noisy data, it\ncan be expressed as\nd = Wy,\nwhere W is an orthogonal matrix and multiplying the noisy data y by W gives the\ncollection of coefficients d.\nThe lifting scheme can also be explained in the same way which we will show\nin Section 5.3 by describing the calculation of the transform matrix in the lifting\nscheme context. Jansen & Oonincx (2005) explains the usage of filter banks in\nthe lifting scheme. Further details of the wavelet transform are given in Mallat\n(1989), and Nason (2008) but are beyond the scope of our core mechanism lifting\none coefficient at a time which will be discussed later.\n\n101\n\n5.3 Lifting in two dimensions\n\n5.3\n\nLifting in two dimensions\n\nIn this section, we explain and use the lifting one coefficient at a time (LOOCAT)\nmethod in two dimensions based on the description in Jansen et al. (2009). Another\nrelated study based on LOOCAT by Jansen et al. (2001) used the lifting scheme as\na smoothing method for irregularly spaced data which was one of the preliminary\nstudies after the lifting scheme is introduced in Sweldens (1998). We adopt the\nlifting one coefficient at a time technique introduced in (Jansen et al., 2009), where\ndata points are lifted and coefficients are calculated sequentially. The general form\nof lifting consists of three major steps: split, predict and update, and the lifting one\ncoefficient at a time technique differ from earlier versions of lifting in the way that\nit splits the data. The standard wavelet and lifting methods use dyadic splitting\nbased on the odd and even indices of the data such as in Claypoole et al. (1998)\nwhich is why the n = 2J condition is required. LOOCAT relies on a method to\ndecide on the order of the calculations of the coefficients one by one.\nThere are other approaches such as the adaptive lifting in Nunes et al. (2006)\nwho used the local features of the data in the prediction step when calculating the\ncoefficients. The calculation of the coefficients includes different regression methods\nand ways of defining neighbours, and they choose the configuration that gives the\nsmallest absolute value of the coefficient. Non-decimated lifting, introduced in\nKnight & Nason (2009), considers paths or trajectories that are n! possible lifting\norders rather than relying on one. However, they sub-sample a smaller number of\npaths from n! trajectories and obtain a set of coefficients at each point rather than\none coefficient. The lifting scheme is also used in Heaton & Silverman (2008) in the\ncontext of imputation.\nNow let us explain the lifting scheme based on Voronoi tessellations proposed by\nJansen et al. (2009). Recall the settings where x is a set of data locations x =\n{x1 , x2 , . . . , xn } which are n irregularly spaced points in a two-dimensional space,\nthat is {xi }ni=1 ∈ R2 . At each point xi , we observe noisy data yi . Now we assume\nthe model\nyi = f (xi ) + \u000fi\n\n(5.3)\n\nwhere fi are the values of an underlying true function which we are interested in\nbut only the data yi are available, and \u000fi ∼ N (0, σ 2 ) are independent Gaussian\nnoise. LOOCAT aims to transform the vector of noisy data values into a set of\ncoefficients by calculating each coefficient at a time. The operation of transforming\nan individual data point into a coefficient is referred to as lifting that observation.\n\n102\n\n5.3 Lifting in two dimensions\n\nThe lifting method has two important aspects: the order of lifting the observations\nand the neighbourhood structure of the points. Therefore, the decision of the lifting\norder and the determination of the neighbourhood structure is crucial. This is the\nfirst part where Voronoi tessellation is used in the lifting scheme. More importantly,\nusing the Voronoi tessellation cell areas attributed to points {xi }ni=1 , we perform\nthe split, predict and update stages in lifting that are explained in the next section\nin detail.\n\n5.3.1\n\nSteps of the lifting transform\n\nThe steps of the lifting scheme are explained in this section. These steps are the\nintermediate calculations in the lifting transform that eventually maps the noisy\ndata into a set of coefficients in the lifting domain. Since the transform is linear\nand can be expressed as a non-singular matrix, it is easily invertible. That means\nthe noisy values can be recovered exactly from the coefficients. The inversion can\nalso be accomplished by following the lifting algorithm in reverse.\nThe lifting transform is an iterative process, and we repeatedly perform the steps\nuntil we have a small number of non-lifted points. We start the (forward) transform\nby selecting the first data location to be lifted; this selection process is called the\nsplitting step. Then we predict the observed value at the selected point from its\nneighbours, which is the prediction step. Finally the values of the neighbours are\nupdated in the update step, and the selected point is lifted.\nLet r be the current stage of the lifting transform. We first set r = n and increment\nby −1 at each step until r = l + 1, where l is the number of points to keep (not\nto lift). The general form of the Voronoi tessellation-based lifting scheme has the\nfollowing steps:\n1. At stage r of the lifting transform, we identify the next point to be lifted,\nwhich is the point with the smallest Voronoi cell area. This ensures we lift\nthe point with the finest level of detail in the data. The point with the\nsmallest cell area is likely to have nearby neighbours that tend to have similar\ncharacteristics with the neighbours unless there is a large local feature in the\ndata. Hence the noise at the selected point can be well detected by the values\nof its neighbours. Also, the coefficient that we obtained for that point has the\ninformation that is representative only for a small region. At stage r = n we\n\n103\n\n5.3 Lifting in two dimensions\n\nselect the smallest cell by\nir = argmin Ir,i\n\n(5.4)\n\ni∈{1,...,n}\n\nwhere ir is the index number of the selected point at stage r, and Ir,i is the\narea of the Voronoi cell associated with point xi at stage r. Then, let Jr\ndenote the set of indices of neighbours of the selected point xir .\n\nx2\nx4\nx3 x1\nx5\n\nFigure 5.1: An illustration of the neighbourhood structure of a selected point x1\ngiven that ir = 1, and its neighbours x2 , x3 , x4 , x5 such that Jr = {2, 3, 4, 5}.\nThis first step is illustrated in Figure 5.1. The selected point xir = x1 at stage\nr is shown as (•) and its neighbours Jr = 2, 3, 4, 5 with ( ) points. The index\nnumbers of the points in Figure 5.1 are arbitrarily chosen to illustrate the\nneighbourhood more clearly. In this step, the red cell has the smallest area\nand we predict the value at xir using its neighbours Jr at the next step.\n2. Now, we predict the value yir of the selected point xir by ŷir = a> yJr where\nyJr is a vector of values of the neighbours Jr , and a is the vector of prediction\nweights obtained from a regression procedure over the neighbours Jr which\nwill be discussed later in detail in Section 5.3.2. We then calculate the detail\ncoefficient dir , which is the difference between the observed and predicted\nvalue,\ndir = yir − ŷir .\n\n(5.5)\n\n3. We update the values of the neighbours of the removed point, using the detail\ncoefficient dir by setting\nyJ?r = yJr + dir b,\n\n104\n\n(5.6)\n\n5.3 Lifting in two dimensions\n\nwhere the elements of vector b are calculated by\nIr,ir Ir−1,j\n.\nbj = P 2\nIr−1,k\n\n(5.7)\n\nk∈Jr\n\nHere Ir−1,j is the cell area of the neighbours after the point xir is lifted, so the\narea of Ir,ir will be shared by its neighbours. We define Ir−1,j = Ir,j + aj Ir,ir\nfor all j ∈ Jr , where r − 1 indicates the next stage of lifting transform. The\naj are the values of the vector of weights a in Step 2.\n4. Finally, we remove xir from the entire data set and return to the first step,\nrecalculating the Voronoi tessellation of points. From the remaining data, we\nchoose the next point that has the smallest cell area and follow the predict\nand update steps accordingly until we are left with l non-lifted points.\n\nx2\n\nx2\nx4\n\nx4\n\nx3 x1\n\nx3\nx5\n\nx5\n\nFigure 5.2: An illustration of the neighbourhood structure of a selected point x1\n(left), and the change in the cells of the neighbours x2 , x3 , x4 , x5 after x1 is removed\n(right).\nWe illustrate the appearance of the cells of the neighbours x2 , x3 , x4 , x5 after x1 is\nremoved from the data set in Figure 5.2 (right). We show the cell edges of x1 in gray,\nand how its area is shared by its neighbours after x1 is removed. The neighbouring\ncells are expanded in the new tessellation in the absence of x1 . The expansion of\nneighbouring cells is denoted as Ir−1,j = Ir,j + aj Ir,ir where the first term indicates\nthe original cell area, and the second term is the part gained from Ir,ir which will\nbe clarified in Section 5.3.2. Algorithm 1 shows the pseudo code of the forward\ntransform.\n\n105\n\n5.3 Lifting in two dimensions\n\nAlgorithm 1: Lifting transform\nInput: Points x = xi , function values y = f (xi ) + \u000fi for i ∈ {1, ..., n} and\n\u000fi ∼ N (0, σ 2 ) is the Gaussian noise.\nDecide l, the number of points to keep\nLet the stages of the transform be r = {n, n − 1, ..., l + 1}\nfor r = n to l + 1 do\nPartition the space into Voronoi cells Vi\nCalculate cell area Ir,i for cells Vi\nSplitting step: Find the cell with smallest area\nChoose ir = argmini∈{1,...,n} Ir,i\nDetermine the set of neighbours Jr\nPrediction step: Calculate the detail coefficient\nIr\ndir = yir − a> yJr where aj = Iiir,j , for all j ∈ Jr\nUpdate step: Update the function values of neighbours\nIr,ir Ir−1,j\nand Ir−1,j = Ir,j + aj Irir\nyJ?r = yJr + dir b where bj = P\nI2\nk∈Jr\n\nr−1,k\n\nRemove xir\nOutput: Detail coefficients d = {dir , dir−1 , ..., dil+1 }, lifting order s =\n{ir , ir−1 , ..., il+1 }, remaining points xi and function values fi for all\ni∈\n/ s, and the transform matrix L.\n\n5.3.2\n\nMethods of prediction\n\nFor the LOOCAT algorithm based on Voronoi polygons in two-dimensional space,\nJansen et al. (2009) discussed two prediction schemes, natural neighbour interpolation and local least squares prediction. As mentioned at Step 2 in Section 5.3.1,\nfor the selected point xir at stage r, we aim to predict yir by a weighted average of\nthe values of its neighbours yJr specifying the prediction weights a at each stage.\nWhen the point xir is removed, the Voronoi tessellation of the remaining points at\nthe next stage r−1 can be recomputed. However, Voronoi cell Vir will disappear and\nits area is shared by its neighbours Jr . Let us carry on the explanation assuming\nthe selected point is x1 and its neighbours are x2 , x3 , x4 , x5 as previously mentioned.\nIn a finite region Ω, let Vir ,j be the part of Vir which joins to neighbour j ∈ Jr . We\nadopt the natural neighbourhood interpolation explained in Jansen et al. (2009)\nthat works by setting\naj =\n\n|Vir ,j |\n|Vir |\n\nwhere |.| denotes the area of the Voronoi cell. Note that\n0 < aj ≤ 1 for all j ∈ Jr.\n\n(5.8)\nP\n\naj = 1 by definition and\n\nIn the examples from Figure 5.1 and 5.2, let V1 = V1,2 ∪V1,3 ∪V1,4 ∪V1,5 be the Voronoi\ncell of x1 , and let V1,j , j ∈ 2, . . . , 5 be the divided parts of V1 , and V2 , . . . , V5 be the\n\n106\n\n5.3 Lifting in two dimensions\n\ncells of the neighbours. The division of the cell V1 by the neighbours is illustrated\nin Figure 5.3.\n\nV2\nV2\n\nV1,2\n\nV4\n\nV1\n\nV4\nV1,4\n\nV1,3\n\nV3\nV5\n\nV3\n\nV1,5\n\nV5\n\nFigure 5.3: An illustration of the calculation of weights based on partitioned cell of\nthe removed point.\nIn this case, the prediction weights are calculated as\n>\n\na =\n\n\u0012\n\n|V1,2 | |V1,3 | |V1,4 | |V1,5 |\n,\n,\n,\n|V1 | |V1 | |V1 | |V1 |\n\n\u0013\n.\n\n(5.9)\n\nUsing the weights aj from the natural neighbour interpolation, the detail coefficient\nfor x1 is calculated as\nd1 = y1 − (a2 y2 + a3 y3 + a4 y4 + a5 y5 ).\nNatural neighbour interpolation can also be expanded to d = 1 or d ≥ 3 dimensional\ncases. Computational intensity is the only disadvantage of this method.\nAnother prediction method called local least squares is a computationally simpler\napproach. In stage r, a least squares plane is fitted to the selected site ir and its\nneighbours Jr , however, this method is not interpolating. Limitations arise when\na point is very close to one of its neighbours; more distant neighbours will still\nhave a high influence. Hence, as a more stable method, natural neighbourhood\ninterpolation is recommended when calculating the detail coefficients in Jansen\net al. (2009).\n\n5.3.3\n\nDerivation of transform matrix\n\nThe lifting transform can be represented by a transform matrix which is independent\nof the observations or the function values and only depends on the data locations.\n\n107\n\n5.3 Lifting in two dimensions\n\nTherefore, once the transform matrix is obtained, it can be reused for different\nobserved values at the same locations. In this section, the construction of the\ntransform matrix will be explained. Recall that we have data locations xi ∈ R2\nfor i = 1, . . . , n, and observe yi at each location. While performing the lifting\ntransform as explained in Section 5.3.1, a transform matrix L can be constructed\nsimultaneously. Then the split, predict, and update steps of the lifting transform\ncan be achieved through pre-multiplication of the data vector y by the transform\nmatrix L to obtain the vector of detail coefficients d:\n\n\nd1\ny1\nw11 w12 w13 . . . w1n\nd2\nw12 w22 w23 . . . w2n     y2\n\n\n(5.10)\n..   =   ..\n.  ,\n..\n..\n.\n.\n..\n..\n.\n.\n..\n.\n.\ndn\nyn\nwn1 wn2 wn3 . . . wnn\n| {z }\n{z\n} | {z }\n|\ny\n\nL\n\nd\n\nwhere the elements wij of L depend on the cell areas and data locations. This is the\ngeneral form of L that allows us to calculate detail coefficients as d = Ly for any\nobserved data vector y. The transform matrix L can also be used for the inverse\ntransform to reconstruct the observed data y = L−1 d. The inverse transform is\nmore useful to invert the adjusted or thresholded detail coefficients which will be\ndiscussed in Section 5.4.\nWe start the construction of the transform matrix by initializing the transform\nmatrix as being an identity matrix L = In×n and hence y = Ly which returns the\nsame vector of observed data y as\ndn\n\nz }| {\nz\n\ny1\n1\ny2\n0\n\n\n..   =   ..\n.\n.\nyn\n0\n\nLn\n\n0\n1\n..\n.\n0\n\n}|\n0 ...\n0 ...\n.. . .\n.\n.\n\nyn\n\n{ z }| {\n0\ny1\ny2\n0\n\n..     ..   .\n.    .\n0 ... 1\nyn\n\n(5.11)\n\nAssume that the lifted point is x1 at the first stage r = n − 1 of the lifting transform\nand x2 , x3 , and x4 are its neighbours. Then the transform matrix Ln−1 at stage\n\n108\n\n5.3 Lifting in two dimensions\n\nr = n − 1 takes the form\nd?(n−1)\n\ny(n)\n\nL?(n−1)\n\nz }| {\n}|\nz\n\nd1\n1 −a2 −a3 −a4\ny2\n0 1\n0\n0\n\n\ny3\n0 0\n1\n0\n\n\ny4   =  0 0\n0\n1\n\n\n..\n..\n..\n..\n..\n.\n.\n.\n.\n.\nyn\n0 0\n0\n0\n\n...\n...\n...\n...\n..\n.\n...\n\n{ z }| {\n0\ny1\ny2\n0\n\n\n0\ny3\n\n\n0\ny4\n..     ..\n.    .\nyn\n1\n\n(5.12)\n\nwhere the vector d?(n−1) only includes the detail coefficient d1 which is calculated\nfrom the first row of L?(n−1) and vector y(n) as d1 = y1 − (a2 y2 + a3 y3 + a4 y4 ) which\nis the first value of d?(n−1) and the remaining values in d?(n−1) are the original\nobserved values y2 , . . . , yn .\nTo update the values of neighbours y2 , y3 , y4 , we make the following calculation\nd(n−1)\n\nL(n−1)\n\nd?(n−1)\n\nz\nz }| {\n\n\nd1\n1\ny ?\nb2\n2\n\ny ?\nb3\n3\n\ny ?   =  b4\n4\n\n..\n..\n.\n.\nyn\n0\n\n}|\n0 0\n0 0\n1 0\n0 1\n.. ..\n. .\n\n{ z }| {\n0\nd1\n\n\n0    y2\n\n\n0\ny3\n\n0\ny4\n..     ..\n.    .\nyn\n1\n\n0\n1\n0\n0\n..\n.\n\n...\n...\n...\n...\n..\n.\n\n0 0 0 ...\n\n(5.13)\n\nhence the vector d(n−1) includes the first detail coefficient d1 and the updated values\ny2? , y3? , y4? and remaining values y5 , . . . , yn . The aj and bj are the coefficients obtained\nfrom the calculation explained in Section 5.3.1. The point x1 is lifted and can no\nlonger be a lifted point or a neighbour in the further stages.\nNow we move on to the next step r = n − 2 of the transform. Let us assume the\nnext point to be lifted is x2 and its neighbours are x3 , x4 , and x5 at stage r = n − 2.\nThis stage is similarly performed by calculating d?(n−2) as\nd?(n−2)\n\nL?(n−2)\n\nz }| { z\n\nd1\n1\nd2    0\n\ny3    0\n\ny4    0\n=\ny5    0\n\n..     ..\n.    .\nyn\n0\n\n}|\n0\n0\n−a4 −a5\n0\n0\n1\n0\n0\n1\n..\n..\n.\n.\n0\n0\n\n0 0\n1 −a3\n0 1\n0 0\n0 0\n..\n..\n.\n.\n0 0\n\n109\n\nd(n−1)\n\n...\n...\n...\n...\n...\n..\n.\n...\n\nz }| {\n{\n0\nd1\ny ?\n0\n2\n?\n0\ny3\n?\n\n0\ny4\n\n0\ny5\n..     ..\n.    .\n1 yn\n\n5.3 Lifting in two dimensions\n\nwhere d(n−1) include the information from the previous stage r = n − 1 but we\noverwrite it and do not need to keep in the memory. Then the updated values of\nthe neighbours y3? , y4? , y5? are calculated as\nd(n−2)\n\nz }| { z\n\nd1\n1\nd2    0\n\ny ?    0\n3\ny ?    0\n4  =\ny ?    0\n5\n..     ..\n.    .\nyn\n0\n\n0\n1\nb3\nb4\nb5\n..\n.\n\nL(n−2)\n\nd?(n−2)\n\n}|\n0 0\n0 0\n0 0\n1 0\n0 1\n.. ..\n. .\n\n{ z }| {\n0\nd1\n\n\n0   d2\n\ny3\n0\n\n\n0\ny4\n\n\n0\ny5\n..     ..\n.    .\nyn\n1\n\n0\n0\n1\n0\n0\n..\n.\n\n...\n...\n...\n...\n...\n..\n.\n\n0 0 0 0 ...\n\nNow we have the vector d?(n−2) that contain the detail coefficients d1 , d2 and updated\nvalues y3? , y4? , y5? . The transform continues by the selection of the next point to lift\nand so on until we have l points left. The idea is the same as in the first two\nsteps and the full transform is performed by the following matrix multiplications.\nCollapsing the first two stages r = n − 1, n − 2 into a more compact from, we can\nshow how the transform matrix L is constructed as\nd(n−1) = L(n−1) L?(n−1) y\n| {z }\n(n−2)\n\nd\n\n(n−2)\n\n=L\n\nd?(n−1)\n?(n−2) (n−1)\n\nL\n|\n\nd(n−3) = L(n−3) L\n|\n\nL\n\n{z\n\nL?(n−1) y\n}\n\nd?(n−2)\n?(n−3) (n−2) ?(n−2)\n\nL\n\nL\n\n{z\n\nL(n−1) L?(n−1) y\n}\n\nd?(n−3)\n\n..\n.\nd(l) = L(l) L?(l) . . . L(n−1) L?(n−1) y.\n\n(5.14)\n\nWe obtain the final vector of detail and scaling coefficients (updated values) d(l)\non the left hand side of (5.14), and the final form of the transform matrix L in\nequation (5.10) is calculated as\nL = L(l) L?(l) L(l+1) L?(l+1) . . . L(n−2) L?(n−2) L(n−1) L?(n−1) .\n\n(5.15)\n\nAs mentioned previously, L only depends on the locations xi . One would prefer\nthe matrix calculation d = Ly to calculate the detail coefficients instead of running\nthe full lifting transform each time. This is useful in the case of multiple vectors\nof observed data yi , yii , . . . observed at the same data locations xi . This choice\nreduces the computational cost since the multiplication of the matrix with a vector\n\n110\n\n5.3 Lifting in two dimensions\n\nwill be faster than running the steps of the lifting transform each time. However,\nnote that we have to run the full transform once to obtain the matrix L.\n\n5.3.4\n\nImplementation of 2D lifting in R\n\nIn this section, we explain the implementation of two-dimensional lifting in R programming language, (R Core Team, 2021). Available libraries to perform lifting in\nR do not have the option of Voronoi tessellation-based lifting in two-dimensional\ncases. Current packages allow adaptive lifting, using the adlift package (Nunes\n& Knight, 2018), and nondecimated lifting transform using nlt package (Knight &\nNunes, 2018), however, they are only useful for lifting in one-dimensional cases and\ntheir mathematical framework is not suitable for our two-dimensional case. Therefore, we wrote our own function lift2D to implement lifting on two-dimensional\ndata. The lift2D function has various options to choose the type of the boundary,\nand to assign any vector of weights including the Voronoi cell area-based weights.\nThe routines to perform the two-dimensional lifting in lift2D function requires\nthe key packages deldir and tripack introduced by Turner (2021) and Gebhardt\net al. (2020) respectively to compute the Voronoi tessellation of points and to extract\ninformation such as the identification of neighbourhood structure, calculation of cell\narea, etc. Another package rgeos by Bivand & Rundel (2020) is used to intersect the\npolygons with the boundaries during the intermediate steps of the lifting transform.\nThe lift2D function has the following input structure:\nlift2D(x, y, f, nleft, stage, keepnbrs, Lmat, rw, method, ... )\nwhere vectors x and y are the coordinates of the data locations, and f is the vector\nof observed values at the locations. The number of points to leave (not to lift) l\nis defined as nleft. The stage option is to save the data in the selected stages\nof lifting such as stage=c(90, 70, 50, 25, 12) that saves the coordinates of the\nremaining points and the updated values of f at the specified stage. This is useful\nto check and illustrate the different stages of the algorithm. If keepnbrs = TRUE,\nthen the indices of the neighbours of the lifted point at each stage is recorded.\nAn important option Lmat specifies whether to calculate and output the transform\nmatrix L. The rw is the window of observed locations such as the rectangular\nboundary. If known, it can be specified, otherwise convex hull of points may be\nused. Finally the method option is used to specify the weights to be used in the\ntransform. The weights we use in this thesis are based on the Voronoi cell area\nwhich are discussed in Section 6.2 but any vector of weights can be specified.\n\n111\n\n5.4 Shrinkage in lifting\n\nWe designed the lift2D function to perform the Voronoi tessellation-based lifting\ngiven a set of irregularly spaced data in two dimensions when the boundaries are\ntaken into account. Cell area-based weights can be specified in multiple ways such as\nthe cell area calculated directly from the Voronoi tessellation. However, the method\nwe introduced in Chapter 3 allows us to reduce the boundary affect by adjusting\nthe cell area. The adjusted cell area can also be used in the lifting framework as an\nalternative weight method; we shall investigate this in Chapter 6 and 7. Therefore,\nwe can investigate the performances of different weight methods in lifting. The\nmethod in the function option allows us to specify these different weights. However,\nit is important to note that the use of different weights will create different transform\nmatrices L since the weights are used in the steps of the lifting algorithm to calculate\nthe coefficients aj and bj and to determine the lifting order.\n\n5.4\n\nShrinkage in lifting\n\nWavelet methods and lifting are often used in situations where one would like to\nanalyze a data corrupted by noise and estimate the underlying true patterns in the\ndata. The purpose of thresholding is to identify the coefficients that represent only\nnoise, and hence follow a N (0, σ 2 ) distribution. Thresholding schemes assume that\nthe small empirical coefficients are due to the small variations in the data (noise),\nhence we assume their true values to be zero. The large coefficients are kept or\nadjusted depending on the thresholding technique since they are considered to be\ndue to activity in the true function f , and are referred to as the signal.\nWe have discussed the lifting scheme that transforms data that contain iid Gaussian\nnoise yi = f (xi ) + \u000fi into a vector of coefficients d in the lifting domain. The\nresulting transform gives a vector d which includes a collection of detail and scaling\ncoefficients. Usually, d is a sparse representation of y in the wavelet or lifting\ndomain. However, we are interested in the estimation of fˆi which is achievable using\nthe shrinkage techniques. This is done by the inverse transform of the thresholded\ncoefficients to have an estimate fˆ of the function f .\nDonoho & Johnstone (1994) and Donoho et al. (1995) suggested thresholding or\nshrinking the coefficients in d. Thresholding methods aim to identify and modify\nthe coefficients that represent noise, and preserve the coefficients that represent the\nactual activity in the underlying pattern. The coefficients that are smaller than a\nthreshold are assumed to be due to the noise in the data and these coefficients are\nshrunk to zero. Larger coefficients are either kept unchanged or also adjusted but\n\n112\n\n5.4 Shrinkage in lifting\n\nnot shrunk to zero depending on the thresholding technique. The estimation of fˆi\nis then followed by the inversion of the modified coefficients.\nThe shrinkage techniques in Donoho & Johnstone (1994) and Donoho et al. (1995)\nare known as the wavelet shrinkage, but it can be easily extended to the lifting\ncontext. Let us consider the setting in (5.3) such that we have the noisy data\nyi = f (xi ) + \u000fi observed at the data locations xi ∈ R2 for i = 1, ..., n, where the\nunderlying true function values f (xi ) are corrupted by the Gaussian noise term\niid\n\u000fi ∼ N (0, σ 2 ).\nUsing the transform matrix L, we are able to calculate the detail coefficients from\nthe noisy observations yi . Let us show how the noise component is translated into\nthe lifting domain. Let us expand the notation d = Ly as\ndi = Lyi = L(f (xi ) + \u000fi ) = L(f (xi )) + L(\u000fi ) = d∗i + εi\nwhere εi is the lifting transform of the noise component and ε ∼ N (0, σε2 ) where\nε = L\u000f ∼ N (L0, L> σ 2 L) = N (0, σ 2 L> L).\nHence the variance of the noise component is denoted σε2 = σ 2 L> L and it is σε2 = σ 2 I\nif the lifting transform is orthogonal L> L = I where I is the identity matrix. The\nwavelet transform is orthogonal but it is not always the case for the lifting.\nThresholding methods are applied on di to have an estimate d̂i = t(di ) where t stands\nfor the thresholding scheme being used. Next, the small coefficients are assumed\nto be zero and coarser coefficients are kept unchanged that is the stage where the\nnoise is suppressed. The threshold δ is estimated from the data itself which will be\ndiscussed later. Three thresholding rules are widely used in the wavelets and lifting\nliterature; hard, soft and empirical Bayes thresholding.\n\n5.4.1\n\nHard and soft thresholding\n\nThe hard and soft thresholding methods are introduced in Donoho & Johnstone\n(1994). In the hard thresholding method, a ‘kill or keep’ strategy is adopted. If the\nabsolute value of a detail coefficient is larger than the threshold, it is not changed,\notherwise, it is set to zero. A detail coefficient d is thresholded based on the threshold δ as\n\n(\nd if |d| ≥ δ\ndˆ = tH,δ (d) =\n.\n0 if |d| < δ\n\n113\n\n(5.16)\n\n5.4 Shrinkage in lifting\n\nOn the other hand, the soft thresholding applies an adjustment on the coefficients\nlarger than the threshold by reducing them by δ,\n(\n(|d| − δ)sgn(d) if |d| ≥ δ\ndˆ = tS,δ (d) =\n.\n0\nif |d| < δ\n\n(5.17)\n\nOne popular choice for δ is the universal threshold defined as\nδ=\n\np\n2 log(N − l)ζ 2\n\n(5.18)\n\nin Donoho & Johnstone (1994) where (N − l) is the number of coefficients. The\nvariance of the noise ζ is usually estimated from the median absolute deviation of\nfinest-scale detail coefficients from zero.\n\n5.4.2\n\nEmpirical Bayesian thresholding\n\nEmpirical Bayesian threshold (Johnstone & Silverman, 2004, 2005a) has great adaptivity features and has been widely used in the lifting scheme. We assume the model\nof observations Zi = h(ti ) + εi where the noise is ε ∼ N (0, σ 2 ) independently. Consider we have a parameter θ and an observation Z ∼ N (θ, 1) where the parameter θ\nis the lifting coefficient of h hence the variance can be scaled if σ 2 6= 1 to have unit\nvariance. In the context of this approach, sparsity is modeled through a suitable\nprior distribution of independent θi as\nfprior (θ) = (1 − ω)δ0 (θ) + ωγ(θ)\nwhere ω is the mixing weight and γ is a symmetric, uni-modal density. One way to\nestimate thresholded coefficient is the posterior median θ̂(z; ω) which is a monotonic\nfunction of z and there exist a function t(ω) > 0 such that θ̂(z; ω) = 0 if and\nonly if |z| ≤ t(ω). Hence for each observation Zi = zi , the posterior distribution,\nfpost (θi |Zi = zi ) can be calculated.\nThe mixing weight w, or the threshold t(ω) can be specified by letting φ(z) be the\nstandard normal distribution and defining g = γ ? φ where ? denotes convolution.\nThe marginal density of Z is\nZ ∼ (1 − ω)φ(z) + ωg(z).\n\n(5.19)\n\nThe maximum likelihood estimator of ω̂ of ω can be obtained by maximizing the\n\n114\n\n5.4 Shrinkage in lifting\n\nmarginal log-likelihood\nl(ω) =\n\nn\nX\n\nlog{(1 − ω)φ(z) + ωg(zi )}.\n\n(5.20)\n\ni=1\n\nIn order to prevent the empirical Bayesian threshold being greater than the universal\n√\nthreshold, we set a restriction t(ω) ≤ 2 log n which assures the removal of all purenoise coefficients. The posterior distribution of θ|X = x can be expressed as\nfpost (θ|X = x) = (1 − ωpost )δ0 (θ) + ωpost f1 (θ|x),\n\n(5.21)\n\nwhere ωpost (x) = P (θ 6= 0|X = x) and f1 (θ|X = x) = f (θ|X = x, θ 6= 0). Let\ng(x)\nβ(x) = φ(x)\n− 1, then the posterior probability is defined as\n1 + β(x)\n.\nω −1 + β(x)\n\nωpost (x) =\n\nWe define the posterior median θ̂(x; w) by considering\nZ ∞\nF̃1 (θ|x) =\n\nf1 (u|x)du.\nθ\n\nThus, if x ≥ 0\n(\nθ̂(x; w) = 0\n\u0001\nF̃1 θ(x; w)|x = 1/2ωpost\n\nif ωpost (x)F̃1 (0|x) ≤ 1/2\notherwise.\n\n(5.22)\n\nIf x < 0, then θ(x; w) = −θ(−x; w) by the anti-symmetry property.\nThere is an implementation of empirical Bayes thresholding in the EBayesThresh\npackage in R (Johnstone & Silverman, 2005b). The EBayesThresh function thresholds each coefficient using an empirical Bayesian procedure instead of fixing the\nthreshold.\nThese thresholding methods are widely used in the lifting literature. However,\nthere are various other methods available such as block thresholding in Cai (1999,\n2002) and Hall et al. (1999) that considers thresholding the coefficients in groups\ninstead of individual adjustments, NeighBlock and NeighCoeff (Cai & Silverman,\n2001) that is built upon block thresholding and takes neighbouring coefficients into\naccount, SureShrink, based on Stein’s Unbiased Risk Estimator (SURE) introduced\nin Donoho & Johnstone (1995) based on (Stein, 1981) that chooses the threshold\nby minimizing the SURE, cross validation based thresholding to find the optimal\n\n115\n\n5.5 Example\n\nthreshold parameter in Nason (1996) and Jansen & Bultheel (1999). Antoniadis\net al. (2001) gave an extensive comparative study considering many alternative\nmethods in addition to the ones listed above.\n\n5.5\n\nExample\n\nIn this section, an example is given to show the calculations of the coefficients in\nthe first few steps of the lifting transform, and to perform the thresholding and the\nestimation of the underlying function. In this example, we first generate n = 100\nuniform random points inside a unit square sampling region Ω = [0, 1]2 . Locations\nof data points x = {x1 , x2 , . . . , x100 } are shown in Figure 5.4. If the points were\nconsidered in an infinite plane where no points were outside the unit square, some of\nthe edge cells would have infinite areas, and some would have finite but extremely\nlarge areas. This would affect the steps of the lifting and misguide us on the\ncalculations. Since the cell areas are used in the lifting scheme, use of boundaries\nis important. Here we show two simple types of boundaries. The unit square is\nshown as the black square, and the convex hull of points is shown as the large red\npolygon. These options are considered as the boundaries and the cell areas due\nto these imposed boundaries are calculated. In this example we only use the unit\nsquare boundary as an illustration but we will consider various options later in\nChapters 6 and 7.\nAt each point xi , we observe some function value f = {f1 , f2 , ..., f100 } by taking\nfi = f (xi ) based on a two-dimensional function called Doppler. The formula of the\nDoppler test function is given in (B.1) and it is explained with other test functions\nin Section 6.1 in detail. We artificially add iid Gaussian noise \u000f ∼ N (0, 0.2) to the\ntest function, and obtain the noisy observations yi = f (xi ) + \u000fi . The vector of noisy\nfunction values are y = {0.741, 0.019, . . . , −0.765, 1.133}. The locations and the\nnoisy function values at the data locations are shown in Figure 5.5. The R package\nggvoronoi by Garrett et al. (2021) is used to create the Voronoi tessellation plots\nwith coloured cells with the colour scheme throughout the thesis.\nThe index numbers i = 1, 2, . . . , 100 help us to keep track of the points we lift and\nidentify their neighbours. We shall start the algorithm by identifying the cell with\nthe smallest area.\n1. As the first step r = 100, the smallest area calculated at the site i100 = 34\nand its area is I100,34 = 0.00195. The set of neighbours of x34 is found as\n\n116\n\n5.5 Example\n\nFigure 5.4: Voronoi tessellation of 100 uniform random points generated in a unit\nsquare. The black square is the unit square boundary and the red polygon is the\nconvex hull of points. Gray dashed lines show the shapes of the polygons if no\nboundary was imposed.\n\ny\n1.0\n0.5\n0.0\n-0.5\n-1.0\n\nFigure 5.5: Voronoi tessellation of 100 uniform random points generated in a unit\nsquare. Cells are coloured based on the noisy function value at the locations.\nJ100 = {x8 , x28 , x99 }. Also, the observed noisy value calculated for x34 is\ny34 = −1.04, and yJ100 = {−0.869, −1.215, −0.765} for its neighbours.\n2. In order to calculate the detail coefficient d34 for the point x34 using equation (5.5), we need to calculate the vector of weights a first. After removing\nthe point to be lifted at this stage, its area is shared by the neighbours and\n\n117\n\n5.5 Example\n\neach piece of the shared area allows us to calculate the weights taking the\nratio over the entire area. Figure 5.6 shows an illustration of this process. We\nsee a zoomed version of the Voronoi tessellation of the point x34 along with\nits neighbours x8 , x28 and x99 . After lifting point x34 , the Voronoi cells of the\nneighbours take the form of the solid lines. Dashed gray lines are the former\nlines of x34 . Note that all other cells will remain the same since the lifting\nonly affect the neighbouring cells.\n\nx99\n\nx99\n\nx34\n\nx28\n\nx28\n\nx8\n\nx8\n\nFigure 5.6: Zoomed in plot of Voronoi tessellation of the lifted point x34 and its\nneighbours x8 , x28 and x99 (left), and the partition of the V34 by the neighbours\n(right).\nBy equation (5.8), the weights are calculated as\na8 =\n\n0.00154\n= 0.788,\n0.00195\n\na28 =\n\n0.00003\n= 0.002,\n0.00195\n\na99 =\n\n0.0004\n= 0.210,\n0.00195\n\nand using (5.5), the detail coefficient is\nd34 = 1 − (0.788, 0.002, 0.210)> (−0.869, −1.215, −0.765) = −0.193.\n3. Note that to update the function values of the neighbours, we need to calculate\nthe weights b first. By equation (5.7),\nb8 =\n\n0.00195 × 0.0648\n= 0.129,\n0.000098\nb99 =\n\nb28 =\n\n0.00195 × 0.0637\n= 0.127\n0.000098\n\n0.00195 × 0.0039\n= 0.078,\n0.000098\n\n118\n\n5.5 Example\n\nhence we update the function values of neighbours using (5.6)\nyJ?100 = (−0.869, −1.215, −0.765) + (−0.193)(0.129, 0.127, 0.078)\n= (−0.894, −1.239, −0.780).\n4. Finally, x34 is removed at stage r = 100 and the same procedure is repeated\nat the next stage r = 99.\nIn the stage r = 99, i99 = 49, so we lift x49 whose function value is y49 = 0.804\nand area is I99,49 = 0.00275. Now, J99 = {x15 , x30 , x67 , x80 , x83 } are the neighbours\nof x49 . The detail coefficient at x49 is found to be d49 = −0.048. Also, updated\nfunction values for x15 , x30 , x67 , x80 , x83 are calculated as\n?\nyJ99\n= (0.852, 0.907, 0.834, 1.007, 0.910),\n\nrespectively. For the next stage i98 = 7, so the site x7 will be lifted and the process\nwill be repeated until r = l + 1.\nNow we shall perform the full lifting transform using the lift2D function we created.\nBased on the same data locations and function values, the lifting transform is\nperformed setting l = 12. In Figure 5.7, the progression of the forward transform of\nlifting is illustrated with snapshots of the updated function values of the remaining\npoints in the intermediate steps. The top-left plots is the Voronoi tessellation of\nall points where the cells are coloured based on the observed noisy function values.\nThe remaining plots show the updated function values of the non-lifted points at\ndifferent stages of lifting r = 80, 60, 50, 40, 30, 20, 13.\nThe plots except the top-left one, are colored based on the updated function value\nwhere the mutual color scheme is given on the right end of the figure. The points\nappear on the plots are the points that remain in the data set. The function has\nactivity near the bottom-left corner and is smooth otherwise. The initial data at\nthe top-left plot is smoothed over the stages of the lifting. The Voronoi cells of the\nremoved points join the areas of their neighbours hence the cell areas increase as\nthe number of points decrease. Also, the updated function values of the removed\npoints include averaging over the neighbours hence we obtain the smoothest pattern\nat the final stage.\nWe now show the detail coefficients di and estimated function values fˆi in Figure 5.8\nat the data points. The detail coefficients are calculated for each removed point xi\nand the cells are colored based on the value of the coefficient. The color scheme is\n\n119\n\n5.5 Example\n\n1.0\n0.5\n0.0\n-0.5\n-1.0\n\nFigure 5.7: Progression of the lifting transform. Top-left plot is the Voronoi tessellation of all points where the cells are coloured based on the noisy function values.\nRemaining plots show the updated function values for the non-lifted points at different stages of the lifting transform.\n\n^f\n\nd\n1.0\n\n1.0\n\n0.5\n\n0.5\n\n0.0\n\n0.0\n\n-0.5\n\n-0.5\n\n-1.0\n\n-1.0\n\nFigure 5.8: Detail coefficients calculated at each lifting stage (left), and the estimated function values for the points (right). Cell areas are coloured based on the\ndetail coefficient or the estimated function value. Gray polygons on the left plot\nindicate the non-lifted points for which the detail coefficients are not calculated\ngiven at the right side of both plots. Gray polygons signify points that are not lifted\nhence no detail coefficients are calculated. The negative coefficients are colored in\nred and the positive ones are in blue. The coefficients very close to zero are colored\nin white. We observe the majority of the detail coefficients that are close to zero\nare located in the less active parts of the function. Large detail coefficients (both\npositive and negative) occur for the points where the function has high activity. The\ninverse transform is performed on the thresholded detail coefficients to estimate the\nunderlying function fˆi at points xi . We used the empirical Bayes threshold in this\n\n120\n\n5.5 Example\n\nexample. The estimated function values do not contain the variations caused by\nthe Gaussian noise as in the top-left plot in Figure 5.7 since the noise is separated\nfrom the yi by thresholding the detail coefficients di . Hence, the inverse transform\nof the thresholded coefficients gives an estimate of the underlying true function that\nis denoised, and the active parts in the true function are not over-smoothed which\nis one of the main advantages of using lifting as a smoothing technique..\n\n121\n\nChapter 6\nLifting results for homogeneous\ndata\nWe report the lifting results for homogeneous simulated data in this chapter. The\nlifting scheme explained in Chapter 5 is used for function estimation using simulated data comprising two-dimensional tests functions with artificial noise. The\ntest functions used in this thesis are explained in Section 6.1. One of the important\naspects of the lifting scheme is a set of weights which are used in the calculation\nof the lifting coefficients and to decide on the lifting order. The Voronoi tessellation cell area-based weight methods we propose are described in Section 6.2. The\nsimulated data is generated based on the design in Chapter 2 that considers points\nfrom the homogeneous Poisson process with ρ = 200 within a unit square. However, we generate independent replications of data sets each of which contain a set\nof test function values at a set of Poisson points. The design of the simulation is\nexplained in Section 6.3. Finally, function estimation results are presented for each\ntest function in Section 6.4 and conclusions are in Section 6.5.\n\n6.1\n\nTest functions\n\nThe function estimation performance of the lifting scheme is evaluated using twodimensional test functions: Doppler, Heavisine, Blocks, Bumps and Maartenfunc\nthat are shown in Figure 6.1. The test functions in (a)-(d) were first introduced\nby Donoho & Johnstone (1994) in one dimensional form, and Maartenfunc in (e) is\ndesigned to be used in lifting by Jansen et al. (2009). Two-dimensional analogues\nof these test functions are used in Nason et al. (2004). The formulae and R implementation of the test functions are provided in Appendix B.1. The true function\n\n122\n\n6.2 Weight methods\n\nvalues f (x) is obtained from the formulae of the functions given in Appendix B.1\nand the noise is added artificially. Therefore, the estimated function values fˆi are\ncompared to the true function values fi .\n(a)\n\n(b)\n\n(c)\n\n(d)\n\n(e)\n\nFigure 6.1: Test functions: (a) Doppler, (b) Heavisine, (c) Blocks, (d) Bumps, (e)\nMaartenfunc.\nThe test functions have different spatial characteristics. The Doppler in Figure 6.1\n(a) is spherically symmetric function around the origin and has higher frequency\noscillation closer to the origin and lower frequency activity otherwise. The Heavisine\n(b) has spherically symmetric with regular sinusoidal waves, and has a sharp spike\naround the centre at coordinates (0.55, 0.5). The Blocks function (c) generates\nblocks that form a smiling face where the block heights take different integer values.\nThe Bumps function (d) has three spikes where each spike has different heights\nand width. Lastly, the Maartenfunc (e) is a piecewise function that has a planar\ndiscontinuity at the intersection of the two planes.\n\n6.2\n\nWeight methods\n\nThe usage of the Voronoi cell area-based weights in the steps of lifting scheme is\nexplained in Section 5.3.1. The basic idea is to use the cell area to decide on the lifting order as shown in (5.4), and in the calculations of the predict and update stages\nin (5.5) and (5.6) respectively. Also, the neighbourhood structure is determined\nusing the Voronoi tessellation of data points as shown in Figure 5.1. Hence, the\nweights play a key role in the function estimation since they affect the calculations\nof the lifting coefficients that are then transformed to the estimated function values.\nGiven that the statistical properties of cell area differ for the cells near the boundaries as demonstrated in Chapter 2 regardless of the boundary types we used, we\nexpect to see differences in the estimated function values if we were to use different\nmethods to calculate cell area. In this chapter, we will use various available options\nto calculate the cell area that correspond to observed weights, and the prediction of\ncell area as we devised in Chapter 3 which we call adjusted weights.\n\n123\n\n6.3 Design of the simulation\n\nTwo main groups of weight methods are considered. The observed weights contain\nthe cell area calculated using unit square and convex hull boundaries. On the other\nhand, adjusted weights includes the predicted cell area using base and augmented\nmodels, and doubled edge cell area. The observed weights are the standard ones\nused in the literature. However, using the adjusted weights, particularly the prediction of cell area, is the novel approach we propose. We introduced this method\nin Chapter 3 for homogeneous data and expanded it in Chapter 4 for the regular\nand clustered data cases. Now we combine this method with the lifting scheme in\nthe context of the weights. The full list of weight methods we consider and their\nexplanations is:\ni. convex: Cell area using the convex hull boundary.\nii. unit: Cell area using the unit square boundary.\niii. double: Edge cell area using the unit square boundary is doubled, and interior\ncells area is kept the same.\niv. base: The ensemble prediction of cell area is calculated using the base models\nfrom Chapter 3.\nv. augm.: The ensemble prediction of cell area is calculated using the augmented\nmodels.\nThese five weight methods are used separately in the lifting transform in the calculation of detail coefficients which are thresholded and inverted later for function\nestimation. We aim to investigate whether the usage of different weight methods\nmatters in function estimation, highlight which methods are accurate and robust\nfor different test functions, and look at local performance such as in the places\nwhere discontinuities happen, edges, corners, etc. Whilst the weight methods from\nobserved cell area are more rigid methods, adjusted weights, especially the base\nand augmented model predictions are novel approaches that also aim to advance\nthe performances of the other methods.\n\n6.3\n\nDesign of the simulation\n\nThe simulation follows these steps:\ni) Generate a set of n homogeneous Poisson points where n ∼ P o(200) in the unit\nsquare sampling region Ω = [0, 1]2 .\n\n124\n\n6.4 Results for simulated homogeneous data\n\nii) At each point xi for i = 1, . . . , n, calculate yi = f (xi ) + \u000fi using the selected\ntest function. The \u000fi are iid Gaussian noise assuming \u000f ∼ N (0, σ 2 ) and the\nvariance is determined from σ = σt /z where σt is the standard deviation of the\ntrue function values f (xi ) and z is the root signal to noise ratio to define the\nmagnitude of the noise, which is taken as z = 3 in this experiment.\niii) Perform the forward lifting transform using the selected weight method to\ndetermine the vector of detail coefficients d = Lf ? .\niv) Shrink the detail coefficients using the hard, soft, and empirical Bayes thresholding methods to obtain dˆi = t(di ).\nv) Perform the inverse transform ŷ = L−1 d̂ on the vector of thresholded coefficients d̂ to obtain the vector of function estimates ŷ (denoised values).\nvi) Record the locations xi and estimates ŷi for all points, and repeat the process\nfor r = 250 independent realizations.\n\n6.4\n\nResults for simulated homogeneous data\n\nThe simulation study generates 250 data sets and each data set has {nj }250\nj=1 ∼\n4\nP o(200) locations. In total, there are expected to be ρ × 250 ≈ 5 × 10 points\ngenerated in the unit square. Data sets are independent from each other. However,\nwe do not change the 250 data sets for different configurations such as the test functions, weight methods, and the thresholding rule. Therefore, function estimation is\nmade using the same data locations for different techniques that makes the function\nestimation results comparable based on the weight methods.\nThe way of presentation and discussion of the results is important. We combine\nthe results from all data sets. The accuracy of the function estimation may be\ndiscussed for the unit square globally. However, we are actually interested in the\nlocal details as well. Therefore, different parts such as the interior and edge regions,\nand diagonal, vertical and horizontal transects, especially near the boundaries, may\nbe used. We focus on the edges particularly because different weight methods\nusually have different values for edge cells and minor or no differences for interior\ncells. To define the transects and for the summary results, the entire unit square\nis divided into equal-sized square bins as shown in Figure 6.2. The zoomed-in plot\nof the bins shows how the points are scattered into each bin. Checking the data\nreveals that there are no bins missing a data point.\n\n125\n\n0.00\n\n0.02\n\ny\n\n0.04\n\n0.06\n\n6.4 Results for simulated homogeneous data\n\n0.00\n\n0.02\n\n0.04\n\n0.06\n\n0.08\n\n0.10\n\nx\n\nFigure 6.2: Zoomed in bottom left corner of the unit square divided into a 50 × 50\ngrid of square bins, showing how the points fall into the first few.\nIn the remainder of this chapter, the function estimation using five different weight\nmethods is discussed for each test function separately in the global, local, and\ntransect-based parts of the region. The parts where the differences between weight\nmethods occur are highlighted and the advantages of using particular weight methods are discussed.\n\n6.4.1\n\nDoppler\n\nStarting with the Doppler test function, a pairwise comparison of different weight\nmethods is the first step. This can be checked by looking at the global mean squared\nerror (MSE) between the estimates and the true function values based on different\nmethods. The formula to calculate the global MSE for a data set is\nnj\n\n1 X\n(fi − ŷi )2\nnj i=1\n\n(6.1)\n\nwhere nj is the size of the j−th data set and fi and ŷi are the true and estimated\nfunction values using the data locations in the j−th data set respectively. Even\nthough global inference is important, it may not be very informative since the\nlocal details are hidden behind the global inference. What is more interesting and\nvaluable is to check the MSE at the transects and at the locations where the test\nfunction show high activity, spikes, and discontinuities.\nThe same data sets are used for function estimation when we alter the weight\nmethod. Hence the function estimation is made for the same data locations but\nusing different approaches. This allows us to see the function estimation at the same\nlocations using different configurations of weight methods. Therefore, it is possible\n\n126\n\n6.4 Results for simulated homogeneous data\n\nto have a pairwise comparisons of the weight methods for each test function. We\nrely on the mean squared error when comparing the methods, which can be global\nMSE, or MSE at different parts such as the edge, or transects. If the global MSE\nis not being used, we use a version of equation in (6.1) by sampling the locations\nin the defined area or transect which we are interested in.\nWe first look at whether there are significant differences between the weight methods\nin terms of function estimation. If we look at Figure 6.2 again, there are numerous\ndata locations in each bin. The pairwise comparison of the weight methods is going\nto be made for the data locations in each bin separately and the results are going\nto be transformed to plots and the tables.\nWe are interested in the comparison at local details in addition to the standard\nglobal results. The MSE results using different methods and transects are given\nin Figure 6.3. Some vertical and horizontal transects v1 , v2 , h1 , h2 , and diagonal\ntransect are selected over the sampling region, and shown in the top-left image on\noriginal Doppler test function; different transects will be used for other test functions. These transects are mainly selected in the regions that we are interested in.\nThe transects are denoted as the v1 , v2 , h1 and h2 in Figure 6.3. The v1 , v2 , . . . , vmax ,\nand h1 , h2 , . . . , hmax are always the vertical and horizontal edge transects and there\nare no other transects between these transects and the boundary for the other test\nfunctions we are going to consider.\nThe MSE is calculated based on the estimated function values for the data locations\nin each bin that the transects pass over. There are 50 bins in each transect, and\nthe MSE is calculated for the data locations in each bin separately. Then the 50\nMSE values are shown in the line plots in Figure 6.3 for each transect separately.\nThe line colours signify the weight method which are labeled in the bottom right\nplot. In each line plot from top-centre to the bottom-right, we aim to check the\ndifferences between the MSE values obtained from each weight method based on\ndifferent transects.\nThere are parts where the results for different methods have obvious differences\nand similarities. The differences mainly exist near the edges at v1 and h1 where\nthe oscillation of the function has higher activity. At v1 and h1 , the green line\n(augmented method) generally has the smallest MSE and the black line (convex\nhull boundary) has the highest. The pattern is similar for both transects since the\nDoppler has symmetric properties. On the other hand, the MSE is very small at the\nv2 and h2 transects where the function show lower activity. The performances of the\ndifferent weight methods are both similar and satisfactory at v2 and h2 . The bins\n\n127\n\n0.15\nMSE\n0.00\n\n0.05\n\nMSE\n0.05\n0.00\n\n0.4\n\n0.15\n\n0.2\n\n0.8\n\n1\n\n0.2\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.10\n\nDiag.\n\nMSE\n\nConvex\nUnit\n\n0.05\n\nDouble\nBase\nAugm.\n\n0.00\n\n0.05\n\nMSE\n\n0.10\n\nh2\n\n0.00\n\n0.00\n\n0.05\n\nMSE\n\n0.10\n\nh1\n\n0.6\n\n0.15\n\nh1\n\nv2\n\n0.15\n\nv1\n\nv2\n\n0.10\n\nv1\n\n0.10\n\nh2\n\n0.15\n\n6.4 Results for simulated homogeneous data\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFigure 6.3: The original Doppler test function (top left). The lines show the mean\nsquared error calculated at transect bins using different weight weight methods.\nTransects are shown with dashed lines on the test function.\nlocated at the diagonal transect mostly take place far from boundaries hence the\nedge effect is minimal. Here, the methods show similar patterns and some variation\nat different parts which is due to the small sample size in individual bins.\nThe numerical results of the MSE in different spatial regions and transects are shown\nin Table 6.1. The results are separated into MSE calculated globally, interior and\nouter region, and the transects. The entire sampling region is Ω = [0, 1]2 , and the\ninterior region is defined as Ωin = [0.15, 0.85]2 and the outer region (edge) is the\nΩed = Ω0in . The smallest MSE in each row is coloured in blue, and multiple values\nare highlighted if they are equal or very close. The augmented method outperforms\nthe others in most cases especially close to the boundaries and where there is high\nactivity in the function. The results in the table validates the conclusions from\nFigure 6.3.\n\n6.4.2\n\nHeavisine\n\nResults for the Heavisine test function are presented and discussed in this section.\nWe check the accuracy of the estimations based on the MSE in the line plots and\nsummarize the numerical values in the table that gives a better understanding of\nthe best and worst methods.\n\n128\n\n6.4 Results for simulated homogeneous data\n\nΩ\nΩin\nΩed\nv1\nv2\nh1\nh2\nD\n\nConvex\n\nUnit\n\nDouble\n\nBase\n\nAugm.\n\n0.046\n0.039\n0.054\n0.111\n0.017\n0.100\n0.016\n0.070\n\n0.046\n0.041\n0.051\n0.102\n0.014\n0.089\n0.014\n0.069\n\n0.047\n0.044\n0.050\n0.093\n0.013\n0.080\n0.014\n0.070\n\n0.045\n0.041\n0.050\n0.094\n0.013\n0.088\n0.013\n0.067\n\n0.044\n0.040\n0.049\n0.087\n0.013\n0.083\n0.013\n0.062\n\nTable 6.1: Results for the Doppler test function. Table shows the global Ω, interior\nΩin and edge Ωed MSE, and MSE at vertical v1 , v2 , horizontal h1 , h2 , and diagonal\nD transects. The smallest MSE is highlighted in blue.\nThe MSE values calculated along the transects are shown in Figure 6.4 using the\ntransects v1 , v2 , h1 , h2 , and the diagonal transect. Since the v1 , h1 and the v2 , h2\nare the transects where the function values are symmetric, the MSE calculated for\nthese pairs of transects are very similar. The line plots show that the green line\n(augmented method) is having smaller MSE in general and the black line (convex\nhull method) is the highest hence we can conclude the overall performance of the\naugmented methods is satisfactory and the worst method is the convex hull boundary as in the Doppler case. In Table 6.2, the augmented model has smallest MSE\nglobally and at the edges, and at edge transects v1 , h1 .\n\nΩ\nΩin\nΩed\nv1\nv2\nh1\nh2\nD\n\nConvex\n\nUnit\n\nDouble\n\nBase\n\nAugm.\n\n0.334\n0.300\n0.367\n0.438\n0.639\n0.437\n0.628\n0.362\n\n0.304\n0.307\n0.301\n0.365\n0.433\n0.379\n0.434\n0.282\n\n0.309\n0.336\n0.282\n0.317\n0.345\n0.332\n0.323\n0.308\n\n0.297\n0.303\n0.291\n0.336\n0.399\n0.342\n0.390\n0.285\n\n0.293\n0.307\n0.279\n0.307\n0.351\n0.306\n0.332\n0.294\n\nTable 6.2: Results for the Heavisine test function. Table shows the global Ω, interior\nΩin and edge Ωed MSE, and MSE at vertical v1 , v2 , horizontal h1 , h2 , and diagonal\nD transects. The smallest MSE is highlighted in blue.\n\n129\n\n1.0\n\n1.0\n\nh2\n\n6.4 Results for simulated homogeneous data\n\nv2\n\n0.8\nMSE\n0.4\n0.6\n0.2\n0.0\n0.2\n\n0.4\n\n1.0\n\n0.8\n\n1\n\nh2\n\n0.8\n\n0.8\n\nh1\n\n0.6\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.8\n\n1\n\nDiag.\nConvex\n\n0.8\n\n1.0\n\nv1\n\nv2\n\n1.0\n\nh1\n\n0.0\n\n0.2\n\nMSE\n0.4\n0.6\n\n0.8\n\nv1\n\nUnit\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nMSE\n0.4\n0.6\n\nBase\nAugm.\n\n0.0\n\n0.2\n\nMSE\n0.4\n0.6\n0.2\n0.0\n\n0.0\n\n0.2\n\nMSE\n0.4\n0.6\n\nDouble\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\nFigure 6.4: The original Heavisine test function (top left). The lines show the mean\nsquared error calculated at transect bins using different weight weight methods.\nTransects are shown with dashed lines on the test function.\n\n130\n\n6.4 Results for simulated homogeneous data\n\n6.4.3\n\nBlocks\n\nThe Blocks test function has different characteristics compared to the previous test\nfunctions with its block spikes in different shapes and heights. The spatial pattern of\nthe MSE in Figure 6.5 is not very clear to visually distinguish, and a generalization\nof the overall quality of the best method is difficult. The MSE values have spikes\nwhere the function has sharp changes. Otherwise, the MSE for all methods are\nvery close to zero at the parts where the function is flat. The numerical values are\nshown in Table 6.3 where the augmented method has the smallest MSE at most of\ntransects. The base and augmented methods perform similarly in some transects,\n\n1\n\n1.0\n0.5\n0.0\n0.2\n\n0.4\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nh3\n\nConvex\nUnit\n\nMSE\n\nMSE\n\n1.0\n\nh2\n\n0.5\n\nMSE\n\n1.0\n\nh1\n\n0.6\n\n1.5\n\n0.8\n\n0.5\n\n0.5\n\nMSE\n\n1.0\n\n0.6\n\n1.0\n\n0.4\n\n1.5\n\n0.2\n\nv4\n\nMSE\n\n1.0\n0.0\n\n0.5\n\nMSE\n\nv4\n\nv3\n\nDouble\nBase\n\n0.5\n\nv3\n\nv2\n\n1.5\n\nv1\n\nv2\n\n1.5\n\nh1\n\n0.0\n\n0.5\n\nh2\n\nMSE\n\n1.0\n\nv1\n\n1.5\n\n1.5\n\n1.5\n\nh3\n\nand it is important to highlight their comparability for the edge MSE.\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.0\n\n0.0\n\n0.0\n\n0.0\n\nAugm.\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nFigure 6.5: The original Blocks test function (top left). The lines show the mean\nsquared error calculated at transect bins using different weight weight methods.\nTransects are shown with dashed lines on the test function.\n\n131\n\n6.4 Results for simulated homogeneous data\n\nΩ\nΩin\nΩed\nv1\nv2\nv3\nv4\nh1\nh2\nh3\n\nConvex\n\nUnit\n\nDouble\n\nBase\n\nAugm.\n\n0.531\n0.759\n0.312\n0.545\n0.800\n0.445\n0.320\n0.257\n0.488\n0.133\n\n0.513\n0.764\n0.271\n0.435\n0.801\n0.438\n0.260\n0.190\n0.482\n0.116\n\n0.538\n0.818\n0.270\n0.351\n0.858\n0.465\n0.249\n0.183\n0.501\n0.120\n\n0.511\n0.767\n0.265\n0.403\n0.800\n0.434\n0.255\n0.188\n0.492\n0.111\n\n0.518\n0.780\n0.266\n0.380\n0.787\n0.399\n0.263\n0.179\n0.479\n0.113\n\nTable 6.3: Results for the Blocks test function. Table shows the global Ω, interior\nΩin and edge Ωed MSE, and MSE at vertical v1 , . . . , v4 , and horizontal h1 , h2 , h3\ntransects. The smallest MSE is highlighted in blue.\n\n132\n\n6.4 Results for simulated homogeneous data\n\n6.4.4\n\nBumps\n\nThe results for the Bumps test function show that the noticeable differences between\nthe weight methods occur near the boundaries. The transects v1 , v2 , v3 , v4 , v5 , h1 ,\nand h2 are selected to see the patterns of MSE for different weight methods in\nFigure 6.6. The are edge transects, and transects where the function has spikes.\nWe also selected v3 where the function is flat as a control case where all methods\nare expected to work equally well.\nFigure 6.6 shows the noticeable differences between weight methods on v1 , v2 , v4 , v5\nand the non flat parts of h1 and h2 that are all transects close to the edges. In the\nvertical transects v1 and v5 , the green line is the closest one to zero in most bins\nhence the augmented method outperforms the other methods. The usage of unit\nsquare and the convex hull methods give the worst estimates on these transects. The\ndifferences are less obvious for v2 and v4 which are relatively close to the boundary,\nhowever, the green line of the augmented method seems to have the smallest MSE\nvalues which is confirmed in Table 6.4. The augmented method also performs well\nat the horizontal transects h1 and h2 where the differences are detected when the\nfunction is not flat. The high MSE values are calculated in regions where the\nfunction has spikes, which is a general issue in all test functions.\nTable 6.4 clarifies the good performance of the augmented method in all cases. Even\nthough the augmented method MSE is not the smallest in some cases such as the Ωin\nand v3 , it is very close to the smallest value found from another weight method. The\nresults for the Bumps test function clearly suggest the usage of augmented method.\nThe exceptional scenarios are for the interior region and v3 transect which the\nfunction is flat. The MSE values for these two cases are very close between weight\nmethods but the augmented method has the best performance in all important\ncases.\n\n6.4.5\n\nMaartenfunc\n\nThe last test function used in the simulations is the Maartenfunc. The differences\nbetween the weight weight methods are not very obvious from the MSE line plots\nin Figure 6.7. The most visible differences between weight methods are in v3 which\nis an edge transect and the function is close to be linear. The edge cell doubling\nmethod seem to give the smallest MSE values along the transect and the MSE\nbetween the best and worst method is larger near the corners as the convex hull\n\n133\n\nv3\n\n1.0\n\nConvex\n\nMSE\n\nUnit\nDouble\nBase\n\n0.5\n\nMSE\n\n1.0\n\nv2\n\n0.5\n\n0.5\n\nMSE\n\n1.0\n\nv1\n\n1.5\n\n1.5\n\n1.5\n\nh2\n\n6.4 Results for simulated homogeneous data\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.6\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.6\n\n0.8\n\n1\n\n0.6\n\n0.8\n\n1\n\nh2\n\nMSE\n0.0\n\n0.2\n\n0.4\n\n0.5\n\nMSE\n\n1.0\n\nh1\n\n0.5\n\nMSE\n\n1.0\n\nv5\n\n0.5\n0.2\n\n0.4\n\n1.5\n\n0.8\n\n0.0\n\n0.0\n\n0.5\n\nMSE\n\n1.0\n\nv4\n\n0.6\n\n1.0\n\n0.4\n\n0.0\n\n0.0\n0.2\n\n1.5\n\n1.5\n\nv4 v5\n\n0.0\n\nv3\n\nv1 v2\n\n1.5\n\nh1\n\n0.0\n\nAugm.\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.2\n\n0.4\n\nFigure 6.6: The original Bumps test function (top left). The lines show the mean\nsquared error calculated at transect bins using different weight weight methods.\nTransects are shown with dashed lines on the test function.\n\nΩ\nΩin\nΩed\nv1\nv2\nv3\nv4\nv5\nh1\nh2\n\nConvex\n\nUnit\n\nDouble\n\nBase\n\nAugm.\n\n0.427\n0.295\n0.553\n0.969\n0.589\n0.099\n0.701\n1.265\n0.607\n0.302\n\n0.376\n0.299\n0.449\n0.734\n0.468\n0.098\n0.589\n0.974\n0.495\n0.213\n\n0.377\n0.328\n0.424\n0.645\n0.499\n0.100\n0.588\n0.801\n0.429\n0.192\n\n0.358\n0.293\n0.420\n0.592\n0.485\n0.093\n0.586\n0.837\n0.429\n0.179\n\n0.349\n0.295\n0.402\n0.548\n0.468\n0.100\n0.576\n0.721\n0.396\n0.177\n\nTable 6.4: Results for the Bumps test function. Table shows the global Ω, interior Ωin and edge Ωed MSE, and MSE at vertical v1 , . . . , v5 , and horizontal h1 , h2\ntransects. The smallest MSE is highlighted in blue.\nmethod is the worst. The differences between weight methods are more apparent\nat the other edge transects v1 , h1 , h3 for the parts [0, 0.2] and [0.8, 1].\nThe MSE values in Table 6.5 are very close to each other especially when the double,\nbase, and augmented methods are used. They have compatible performances in the\nedge region and it is appropriate to use the adjusted weights. However, since the\ndoubling is a rigid process compared to the prediction of cell area using base and\naugmented models, the usage of augmented method would be more appropriate\nconsidering its overall performance in the other test functions.\n\n134\n\n0.8\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.20\n0.10\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\nh3\n\n0.20\n\nConvex\nUnit\n\nMSE\n\n0.20\n\nDouble\nBase\n\n0.10\n\nMSE\n\nAugm.\n\n0.00\n\n0.10\n0.2\n\n0.00\n0.2\n\nh2\n\nv3\n\nMSE\n\n1\n\n0.00\n\n0.00\n\n0.10\n\nMSE\n\n0.20\n\n0.6\n\n0.30\n\n0.4\n\n0.30\n\n0.2\n\nh1\n\n0.30\n\n0.30\nMSE\n0.10\n0.00\n\n0.00\n\n0.10\n\nMSE\n\nh2\nh1\n\nv3\n\nv2\n\n0.30\n\nv1\n\nv2\n\n0.20\n\nv1\n\n0.20\n\nh3\n\n0.30\n\n6.5 Conclusions\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n0.2\n\n0.4\n\n0.6\n\nFigure 6.7: The original Maartenfunc test function (top left). The lines show\nthe mean squared error calculated at transect bins using different weight weight\nmethods. Transects are shown with dashed lines on the test function.\n\nΩ\nΩin\nΩed\nv1\nv2\nv3\nh1\nh2\nh3\n\nConvex\n\nUnit\n\nDouble\n\nBase\n\nAugm.\n\n0.055\n0.040\n0.070\n0.057\n0.053\n0.206\n0.106\n0.046\n0.054\n\n0.049\n0.040\n0.058\n0.047\n0.050\n0.162\n0.079\n0.042\n0.046\n\n0.045\n0.038\n0.053\n0.044\n0.050\n0.116\n0.077\n0.043\n0.049\n\n0.047\n0.039\n0.055\n0.041\n0.049\n0.146\n0.077\n0.043\n0.044\n\n0.047\n0.039\n0.054\n0.040\n0.052\n0.134\n0.074\n0.044\n0.043\n\nTable 6.5: Results for the Maartenfunc test function. Table shows the global Ω, interior Ωin and edge Ωed MSE, and MSE at vertical v1 , v3 , v3 , and horizontal h1 , h2 , h3\ntransects. The smallest MSE is highlighted in blue.\n\n6.5\n\nConclusions\n\nThis chapter presented and discussed the function estimation results using the lifting\nscheme for homogeneous data, giving emphasis on what happens when different\nweight methods are used in the lifting. The simulation setting considers various\nimportant configurations such as the usage of test functions that have different\nspatial characteristics, and weight methods to evaluate the performances of different\napproaches in function estimation. The MSE values attained at different parts and\ntransects of the region highlight the differences between the usage of different weight\nmethods.\n\n135\n\n6.5 Conclusions\n\nThe results in this chapter highlight two important aspects of function estimation\nusing lifting. First, the significant differences between weight methods are demonstrated, then the best method to estimate the function is suggested. More importantly, we focused on local details such as the parts that are close to the boundaries,\nand functions having discontinuities, spikes, etc. Throughout the discussion of the\nresults for each test function, the augmented method gave a better performance\ncompared to the other weight methods. It achieved more accurate function estimation especially at the edge transects in which we aimed to improve the accuracy of\nthe function estimation obtained from standard observed weights.\nEarlier lifting research used traditional weight methods and emphasized the issues\nthat may occur for the data locations near the boundaries. Our work in this thesis\nprimarily suggests ways to eliminate, or at least reduce the boundary effects in\nfunction estimation. Simulation results show that our proposed weight method,\nwhich uses the predicted cell area from augmented models, is the favourable option.\nTherefore, a general use of the augmented weight method is suggested in lifting for\nhomogeneous data.\n\n136\n\nChapter 7\nLifting results for regular,\nclustered and real data examples\n7.1\n\nLifting for regular and clustered data\n\nIn this chapter, the lifting study is extended to the case of regular and clustered\ndata cases. An introduction to the regular and clustered points was given in Section 4.2, where we relied on the saturation process introduced by Geyer (1999) to\ncreate regular and clustered points with different types of irregularity such as clustering and inhibition. We rely on the same point process to simulate regular and\nclustered points in this chapter. The lifting scheme is a multiscale method used to\nanalyze irregularly spaced data, and it is important to see its capability in dealing\nwith different types of irregularity. In this chapter, we not only investigate the\nperformance of the lifting scheme in function estimation for regular and clustered\npoints, we also consider the extreme cases of highly regular and clustered point\npatterns and check how the lifting scheme performs.\nThis chapter presents and discusses the lifting results for regular and clustered\ndata from simulations, and real data examples. The design of the simulation is\nsimilar to Section 6.3, however, there are various point pattern cases rather than a\nsingle homogeneous Poisson point process case. The point patterns are determined\nbased on different values of the parameter γ of the process. Essentially, the same\nparameter values γ = 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 3 are used to generate the\ndata sets as in Figure 4.1 in Section 4.2. For each value of γ, a set of n points,\nwhere n ∼ P o(200), are generated and function estimation is conducted for the test\nfunctions from Section 6.1. This process is carried out for 250 replicates of each\n\n137\n\n7.2 Results for simulated data\n\npoint pattern altering γ. Therefore we have 250 × 9 data sets each of which has a\nsize {nj }250\nj=1 . Function estimation results attained from lifting scheme with various\nweight methods are compared to kriging estimates.\nThe main purpose of this chapter is to investigate how the regularity and clustering in the data affects the function estimation. Also, a comprehensive analysis is\nperformed to see the effects of different weight methods. In addition to the five\nweight methods used in Chapter 6, we introduce another version of the base and\naugmented model prediction methods. Area prediction for regular and clustered\npoints is done using the scaled covariates based on the estimated local intensities\nas explained in Chapter 4 that draw attention to the better performance of the B ?\nand Ag ? models over B and Ag models in the sense of area prediction. The weight\nmethods with ? superscript are the versions of B and Ag that are designed for\nregular and clustered data cases, and we use both versions to have a comparison in\nthis chapter. Application of the lifting for the regular and clustered points does not\nhave methodological differences to the homogeneous case, so the same lifting steps\nfor forward transform, thresholding the detail coefficients, and the inverse transform\nare followed.\nThis chapter also considers the application of lifting to the real data sets; spruces,\nBarro Colorado Island (BCI), waka, finpines, and longleaf which are explained\nin Section 4.3.2. We examine how lifting works for real data locations and measurements at the locations that show examples of regular, clustered and homogeneous\npatterns. These data sets are especially chosen for the purpose of having examples\nof homogeneous and regular and clustered real data examples. The lifting results\nfor simulated and real data sets are presented and discussed in Sections 7.2 and 7.4\nrespectively. We also compared results for function estimation using lifting and\nkriging in Section 7.3, using the weight method that gave the best results in lifting\nfor simulated regular and clustered data.\n\n7.2\n\nResults for simulated data\n\nThe lifting results for the regular and clustered data cases from all the test functions\nare presented in Figures 7.1 - 7.5. The number of cases we investigate is enormous,\nhence discussion of the numerical results is not very practical. Tables C.1 - C.5 in\nAppendix C show the MSE values for the function estimation for different configurations of point patterns, weight methods, and test functions at different parts and\n\n138\n\n7.2 Results for simulated data\n\ntransects of the sampling region. Due to the complex structure of the tables, it is\nmore effective to inspect the plots.\n\n7.2.1\n\nDoppler\n\nWe start presenting lifting results from the simulations based on the Doppler test\nfunction. We selected the same vertical, horizontal and diagonal transects as in the\nprevious chapter to compare the weight methods at different point pattern cases.\nThe coloured points in the plots in Figure 7.1 show the MSE value corresponding to\nthe different weight methods, and the values i = 1, 2, . . . , 9 in the x−axis correspond\nto the index value of parameter {γi }9i=1 = 0, 0.25, 0.5, 0.75, 1, 1.25, 1.5, 2, 3. In each\nplot, the results are given for global, interior and edge of the region, and at different\ntransects of the region. Note that the range of the y−axis is different for each plot.\nThe MSE values shown with points in different shapes and colour are the overall\nMSE for the associated transect, global, interior, or edge points. Numerical results\nare presented in Tables C.1 - C.5 in Appendix C.\nThe top-left plot in Figure 7.1 shows the global MSE for the Doppler test function.\nWhilst the points are very close for the regular point pattern and even overlap,\nthe differences are more apparent for the homogeneous and clustered points as γ\nincreases to 3. For the interior points, the MSE is smaller than the global MSE\nvalues. It is more interesting to analyze the edge points and the points located\non the edge transects as the differences between the methods are clearer and have\nsome pattern. For the ease of interpretation, convex hull, unit square and doubling\nare shown in red, base and augmented methods in blue, and ? models in black.\nThe weight methods show differences in terms of the MSE values for edge points.\nThe smallest MSE is achieved with the solid black triangle for most of the regular\nand clustered point patterns which is the augmented weight method Ag ? . The\nsolid circles of the B ? method are either very similar or better in occasional cases\nthat shows the robustness of the usage of local intensity methods in the Doppler\nexample. The weight methods using the observed cell area generally give the worst\nestimation results.\nThe edge transects v1 and h1 are the transects where the function has spherically\nsymmetric properties, hence conclusions can be made jointly. A satisfactory performance of the Ag ? method exists in v1 and h1 for the regular and clustered points.\nIt is important to note that the smallest MSE is found using the blue triangle at\nγ5 = 1 which indicates the homogeneity of the points. It is also the same for the\n\n139\n\n6\n\n7\n\n8\n\n9\n\n0.00\n\n0.09\n\n0.2\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n0.15\nMSE\n0.05\n0.00\n\n0.12\n7\n\n8\n\n0.8\n\n6\n\n7\n\n8\n\n9\n\n0.15\n\n0.11\n\n0.10\n\n0.09\n\n0.10\n6\n\nMSE\n\n0.07\n0.06\n\n9\n\nh2\n\nConvex\nUnit\nDouble\nBase\nAugm\nBase*\nAugm*\n\n0.6\n\n5\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nDiagonal\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n0.050\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nFigure 7.1: Lifting MSE results for Doppler at different parts and the transects.\nx−axis shows the index of γi for i = 1, ..., 9, and y−axis is the MSE which varies\nfor each plot.\nplot titled with ‘edge’. This means the Ag method that is created for the homogeneous data has better performance than methods that uses the local intensities. It\nis pertinent because the B and Ag models are trained to predict the cell area for\nhomogeneous points but the B ? and Ag ? models scale the covariates based on the\nestimated local intensity which are designed to be used for regular and clustered\ndata cases.\nRevisiting the ‘global, edge, interior’ plots, the pattern of the MSE values for adjusted weights from γ7 to γ9 deviate from each other. The solid points remain\nthe smallest, however the non-solid points increase in MSE and perform almost as\nbadly as the convex hull method. Although the performances of the weight methods\nshown with solid and open points were similar for regular point patterns, they are\nextremely different for the highly clustered points. Considering the regular point\npatterns, where the cells would have similar sizes, and the estimated local intensity\n\n140\n\n0.\n\n0.08\n\n0.10\nMSE\n4\n\n0.010\n\n0.07\n3\n\n3\n\n0.012\n\n0.08\n\n0.4\n\n0.06\n2\n\n2\n\n0.018\n\n0.11\n\nh1\n\n0.10\n\n0.018\n0.016\n0.014\n0.012\n0.010\n1\n\n1\n\n1\n\n0.00\n\n5\n\n0.8\n\nh2\n\n0.080\n\n4\n\n0.6\n\nv1\n\n0.070\n\n3\n\n0.4\n\n0.060\n\n2\n\n0.040\n\nMSE\n1\n\n0.05\n\n9\n\n0.00\n\n8\n\n0.020\n\n7\n\n0.016\n\n6\n\nv2\n\n0.014\n\n5\n\n0.05\n\n0.035\n4\n\n0.12\n\n3\n\n0.045\n\n0.10\n\n0.050\n\nh1\n\n0.045\n0.040\n\n0.045\n0.040\n0.035\n\n2\n\n0.020\n\n1\n\n0.2\n\nEdge\n\n0.15\n\nInterior\n\n0.15\n\nGlobal\n\nv2\n0.055\n\n0.050\n\n0.050\n\nv1\n\n0.05\n\nh1\n\n0.00\n\n0.05\n\nMSE\n\n0.10\n\nv1\n\n0.10\n\nh2\n\n0.15\n\n7.2 Results for simulated data\n\n1\n\n0.\n\n7.2 Results for simulated data\n\nρ̂i at the points is not too different than the global point intensity ρ0 as can be\nseen in Figure 4.2. Therefore, in these cases the scaling procedure of the B ? and\nAg ? models will have minimal changes on the covariates since the scaling factor is\nρ̂i /ρ0 ≈ 1. Hence the (B, Ag), and (B ? , Ag ? ) methods obtain similar function estimation results for the regular point patterns. However, the usage of estimated local\nintensity ρ̂i become more important for the clustered points because the estimated\nlocal intensity ρ̂i can have a larger deviation from the global intensity ρ0 . Therefore, the new methods, especially Ag ? , has better performance for the clustered\ndata cases than the unscaled methods (B, Ag).\n\n7.2.2\n\nHeavisine\n\nIn this section, lifting results for the Heavisine function are presented. Figure 7.2\nshows the MSE values for different weight methods. The function estimates at the\nedge region give smaller MSE using the proposed weight methods B, Ag, B ? , and\nAg ? compared to the weight methods from observed cell area. The solid and open\ntriangles and circles give very similar MSE for regular points that indicates the\nsimilarity of the B, Ag, B ? , and Ag ? methods. However, the solid points of B ? and\nAg ? methods persistently perform better for highly clustered points, and the open\npoints of B and Ag start giving higher MSE after γ7 . For the interior points, the\nweight methods generally show compatible results except the highly regular and\nclustered cases at γ = 1 and γ = 9.\nThe results at the separate edge transects give similar conclusions. The augmented\nmethod Ag ? gives the smallest MSE for all point pattern types at transects v1 , v2 , h1\nand h2 . The MSE values for all methods decrease from γ1 to γ9 that points out the\nHeavisine function can be better estimated using the clustered points, and highly\nclustered point pattern types give the smallest MSE. The function has sinusoidal\nwaves with a sharp spike around the centre. The neighbourhood structure has a\nmajor impact on the prediction of the function values in the prediction step of\nlifting. The function value at a data location is predicted as the weighted average\nof the function values of its neighbours. If the points have a regular pattern, then\nthe interesting features of the test function may not be captured properly by the\nmore distant neighbours, especially in functions like Heavisine. The function value\nat a selected point may be very different from its neighbours.\nTo make the example more specific, consider a point x1 located on the spike, and\nit has several neighbours say x2 , x3 , x4 and x5 with a reasonable distance due to\na regular point pattern. If the point x1 on the spike is selected to be lifted, then\n\n141\n\n1.0\n\n1.0\n\nh2\n\n7.2 Results for simulated data\n\n8\n\n9\n\n2\n\n3\n\n4\n\n5\n\n0.8\nMSE\n0.4\n0.6\n0.2\n0.0\n0.6\n\n6\n\n7\n\n8\n\nh2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1.0\nMSE\n0.4\n0.6\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\nDiagonal\n\n0.2\n\n0.4\n\n8\n\n9\n\nConvex\nUnit\nDouble\nBase\nAugm\nBase*\nAugm*\n\n0.6\n\n0.8\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0.2\n\n0.2\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nFigure 7.2: Lifting MSE results for Heavisine at different parts and the transects.\nx−axis shows the index of γi for i = 1, ..., 9, and y−axis is the MSE which varies\nfor each plot.\nits function value is predicted based on the values of its neighbours. Since its\nneighbours’ function values range between the highest and lowest values of the\nsinusoidal waves, hence the predicted function value fˆ(x1 ) for x1 will be affected\nby the discrepant values of the neighbours. However, if the points are clustered\nwhich means the neighbours are likely to be closer (at least most of them), then the\nneighbours have more relevant information about the function value to be predicted\nat x1 . This is likely to happen for the functions that short range irregularities like\nHeavisine.\n\n7.2.3\n\nBlocks\n\nThe Blocks test function has different types of sharp changes than those seen in\nthe Doppler and Heavisine functions. The results are shown in Figure 7.3. The\nglobal results show little difference between methods, but differences between weight\n\n142\n\n0.\n\n0.8\n\n0.5\n0.4\n1\n\n0.3\n\n0.3\n0.2\n\n0.2\n\n0.3\n\n0.3\n\n9\n\n0.4\n\n0.4\n\n0.2\n\n0.2\n\n1.0\n\n1\n\n1\n\n0.2\n\n7\n\n0.6\n\nh1\n\n6\n\n0.8\n\nh2\n\n0.5\n\n5\n\n0.0\n\n0.6\n0.5\n0.4\n\nMSE\n0.4\n0.6\n\n0.30\n0.20\n\n4\n\n0.6\n\nv1\n\n0.4\n\n3\n\n0.8\n\n0.40\n\n0.8\nMSE\n0.4\n0.6\n2\n\n0.5\n\n0.7\n\n0.8\n\n1\n\n0.4\n\n0.3\n\n9\n\n0.0\n\n8\n\n0.8\n\n7\n\n0.7\n\n6\n\n0.6\n\n5\n\nv2\n\n0.5\n\n4\n\nh1\n\n0.2\n\n3\n\n0.50\n\n1.0\n\n0.45\n0.40\n0.35\n0.30\n0.25\n2\n\n0.6\n\n1\n\n0.2\n\nEdge\n\nInterior\n\n0.20\n\n0.20\n\n0.25\n\n0.30\n\n0.35\n\n0.40\n\n0.45\n\nGlobal\n\nv2\n\n0.2\n\n0.50\n\n0.50\n\nv1\n\n0.0\n\nh1\n\n0.0\n\n0.2\n\nMSE\n0.4\n0.6\n\n0.8\n\nv1\n\n1\n\n0.\n\n7.2 Results for simulated data\n\nmethods are noticeable at the edge region and separate transects. The global MSE\nfor all weight methods have a decreasing trend from γ1 to γ9 as observed for the\nHeavisine. The Blocks and Heavisine test functions have characteristics in common\nin terms of the spikes in Heavisine and the discontinuities in Blocks. However, the\ndiscontinuities of the Blocks function are rectangular prisms in different shapes and\n\n1.5\n\n1.5\n\nh3\n\nheights.\n\nv4\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n4\n\n5\n\n1.0\nMSE\n0.5\n0.0\n1.0\n0.9\n\n6\n\n7\n\n8\n\n9\n\nh1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n2\n\n3\n\n4\n\n6\n\n7\n\n8\n\n9\n\n1\n\n0.2\n\n0.45\n0.40\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0.2\n\n0.\n\n1.5\n1.0\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\nh3\n\n0.4\n\n7\n\n8\n\n9\n\nConvex\nUnit\nDouble\nBase\nAugm\nBase*\n0.8\nAugm*\n\n0.6\n\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nFigure 7.3: Lifting MSE results for Blocks at different parts and the transects.\nx−axis shows the index of γi for i = 1, ..., 9, and y−axis is the MSE which varies\nfor each plot.\nThe number of transects we selected is higher for Blocks since this function also\nhas interesting features at both the edge and intermediate transects. In the edge\nplot (top-centre) in Figure 7.3, the weight methods show similar results except for\nthe convex hull. Even though the smallest MSE is achieved by the Ag ? method\nfor only γ2 , γ4 , γ5 , and γ9 , it is very close to the best method in the other cases.\nThe function has similar features at edge transects v1 and h1 . They overlap at\n\n143\n\n0.\n\nMSE\n\n0.6\n\n0.7\n5\n\nh2\n\n0.8\n\n0.15\n1\n\n1\n\n0.50\n\n0.6\n\n0.25\n\n0.4\n\n0.20\n\n0.25\n0.15\n\n0.20\n\n0.40\n0.35\n0.30\n\n2\n\n3\n\n0.2\n\n0.5\n\n2\n\n0.2\n\n1\n\nMSE\n\n0.40\n0.30\n1\n\n0.20\n\n9\n\n0.15\n\n8\n\nh1\n\n0.8\n\n1.0\n\n0.30\n0.28\n0.26\n\n7\n\n1\n\n0.10\n\n6\n\n0.30\n\n0.45\n\nv3\n\n5\n\n0.8\n\n0.05\n\n4\n\n0.5\n\n3\n\n0.50\n\n0.32\n2\n\n0.6\nv2\n\n1.5\n\n1.5\n1\n\n0.4\n\n0.0\n\n9\n\n0.60\n\n8\n\n0.55\n\n7\n\nv4\n\n0.35\n\n6\n\n0.2\nv1\n\n0.30\n\n5\n\n0.0\n\n4\n\n0.35\n\n3\n\n0.5\n\n0.65\n\n0.70\n\nMSE\n\n0.75\n\n1.0\n\n0.80\n\n0.85\n\n0.55\n0.50\n0.45\n\n2\n\n0.50\n\n1\n\nv4\n\nEdge\n\n0.34\n\nInterior\n\n0.90\n\nGlobal\n\nv3\n\nv2\n\n0.60\n\n0.95\n\n0.60\n\nv1\n\n0.0\n\nh1\n\n0.0\n\n0.5\n\nh2\n\nMSE\n\n1.0\n\nv1\n\n7.2 Results for simulated data\n\nthe bottom-left corner, and there is a sharp discontinuity on v1 compared to the\nmoderate discontinuity on h1 .\nThe smallest MSE at v1 ranges within the approximate interval of (0.33, 0.4), but\nfor h1 , it is (0.17, 0.2). The two flat parts of the function in v1 form a step that\nis higher compared to h1 hence the MSE is larger at v1 . The Ag ? method (shown\nwith solid triangles) has good performance in both transects. The point shown with\n(×) symbol interestingly gives the smallest MSE for γ4 − γ8 at h1 . The other two\nedge transects, v4 and h3 , also have similar features. A large part of both transects\ncontain the same constant value of the function. A small part of h3 at the top left\ncorner of the function has slightly higher value than the remaining part. For v4 ,\nthe bottom right part of the function has higher values, hence the h3 give smaller\nMSE.\nIf the intermediate transects are checked, for instance, the v2 , v3 , and h2 , the MSE\nvalues are the highest for v2 that passes over three blocks of the function and it is\nhard to suggest a specific weight method due to the inconsistent patterns. However,\nthe MSE has a decreasing trend from regular to clustered points which validates\nthe previous conclusions regarding the better function estimations using clustered\npoints for the functions that have discontinuities and spikes.\n\n7.2.4\n\nBumps\n\nThe Bumps test function has three spikes at different parts of the region. However,\nthe spikes are due to the finite exponential increases in the function value rather\nthan rectangular prisms as in Blocks. The function is constant on the rest of the\nregion. The global MSE in Figure 7.4 shows a decreasing trend from regular to\nclustered points as it was in the Heavisine and the Blocks. These three functions\nmay be considered to belong to the same family based on having sharp increases\nsuch as discontinuities and spikes.\nThe edge region MSE shows that the smallest values are achieved using the Ag ?\nmethod in most γi cases, and the Ag method has a comparable performance for\nregular and homogeneous points. But the performance of Ag becomes worse for\nclustered points. The edge transects v1 , v5 , h1 and h2 give the same conclusions\nin terms of the best weight methods. The solid and non-solid points mostly have\nsimilar MSE values except for the highly clustered cases. For general use, Ag ? weight\nmethod would be the preferred method since it has a consistent performance for all\n\n144\n\n1.5\n\n1.5\n\nh2\n\n7.2 Results for simulated data\n\nv3\n\n5\n\n6\n\n7\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n1.0\nMSE\n0.5\n0.0\n8\n\n9\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n0.2\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nConvex h2\n\n0.4\n\nUnit\nDouble\nBase\nAugm\nBase*\n0.6\nAugm*\n\n1\n\n3\n\n0.8\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n0.2\n0.1\n\n0.4\n0.3\n\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n2\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nFigure 7.4: Lifting MSE results for Bumps at different parts and the transects.\nx−axis shows the index of γi for i = 1, ..., 9, and y−axis is the MSE which varies\nfor each plot.\npoint patterns. The intermediate transect v3 do not contain any variability of the\nfunction, hence the MSE is very similar and close to zero for all weight methods.\n\n7.2.5\n\n0.\n\n1.0\n1\n\n0.5\n\n0.8\n0.6\n3\n\n0.2\n\nMSE\n\n0.45\n0.40\n0.35\n7\n\n0.3\n\n1.0\n\n0.8\n\n6\n\n0.6\n\n0.6\n\n5\n\nh1\n\n0.7\n\n0.0\n\n0.4\n\n0.4\n2\n\n0.\n\n0.50\n2\n\n0.5\n\n0.08\n0.06\n1\n\n0.2\n\nv2\n\n0.55\n\n1.5\nMSE\n\n0.6\n0.4\n1\n\nv5\n\n0.7\n\n0.12\n\n1.0\n\n0.8\n9\n\nv4\n\n0.2\n\n0.10\n\n0.60\n\n1.2\n1.0\n\n0.55\n0.35\n8\n\n1\n\n0.5\n\n4\n\n0.8\n\n0.0\n\n3\n\n0.40\n\nMSE\n2\n\n1.2\n\n0.14\n\n1\n\n0.6\n\nv5\n\n0.5\n\n9\n\n0.4\n\n0.4\n\n8\n\n0.5\n\n7\n\nv1\n\n0.0\n\n6\n\nv4\n\n0.8\n\n5\n\nv3\n\nEdge\n\n0.6\n\n4\n\n0.5\n\n0.26\n3\n\n0.8\n\n2\n\n0.2\n\n0.45\n\n1.0\n\n0.30\n0.28\n\n0.40\n0.35\n0.30\n1\n\nv4 v5\n\n0.50\n\n0.32\n\n0.45\n\n0.60\n\nInterior\n\n0.34\n\nGlobal\n\n1.5\n\n0.50\n\nv1 v2\n\n1.5\n\nh1\n\n0.0\n\n0.5\n\nMSE\n\n1.0\n\nv1\n\nMaartenfunc\n\nMaartenfunc is a completely different type of test function than the previously\ndiscussed ones. It has two separate intersecting planes where a discontinuity exists\nat the intersection line. The global MSE results for Maartenfunc in Figure 7.5 show\nan increasing trend from regular to clustered points. The regular point patterns\ngive better function estimation in this case, since the function value of a selected\npoint and its neighbours would be similar except near the discontinuity.\nThe smallest global and edge MSE values are achieved by the Ag ? method (with\nsolid triangle) for all cases and this exists in most cases of the edge transect plots\n\n145\n\n7.3 Comparison of lifting estimates with kriging\n\nv1 , v3 , h1 , and h3 , showing the Ag ? method to be more favourable. The minimum\nMSE in v3 is always higher than 0.10 for all γ values, but it is always smaller than\n0.05 in v1 and h3 . Maartenfunc has its highest values at v3 and the lifting has a\nweakness on estimating the maximum values of the function, hence the MSE at the\ntransects where the function has local or global maximums are higher. Higher MSE\nwas also observed in Chapter 6 when the transect bins coincided with the local or\n\n0.14\n\nh1\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nConvex h2\nUnit\nDouble\nBase\nAugm\nBase*\n0.4\n0.6\nAugm*\n\n0.05\n0.04\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n2\n\n3\n\n4\n\n0.30\nMSE\n0.10\n0.00\n\nh2\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\n1\n\n0.2\n\n0.4\n\n0.6\n\n0.8\n\n1\n\n1\n\n2\n\n3\n\n4\n\n5\n\n6\n\n7\n\n8\n\n9\n\nComparison of lifting estimates with kriging\n\nIn this section, we compare the lifting method using the suggested weight method\nAg ? to kriging or Gaussian process regression which is a standard spatial interpola-\n\n146\n\n0.\n\nh3\n\nFigure 7.5: Lifting MSE results for Maartenfunc at different parts and the transects.\nx−axis shows the index of γi for i = 1, ..., 9, and y−axis is the MSE which varies\nfor each plot.\n\n7.3\n\n0.2\n\nMSE\n\n0.03\n6\n\n0.\n\n0.20\n\n0.05\n0.04\n\n5\n\n0.2\n\n0.30\n\nv2\n\n0.06\n\n0.30\n0.20\n\nMSE\n1\n\n0.8\n\n0.03\n1\n\n0.07\n\n0.08\n0.07\n0.06\n0.05\n0.04\n0.03\n\n1\n\n0.2\n\n0.06\n2\n\n0.08\n9\n\n0.08\n\n0.14\n0.12\n0.10\n1\n\n0.07\n0.03\n8\n\n0.10\n\n0.16\n\n7\n\n1\n\n0.00\n\n6\n\n0.10\n\n5\n\n0.8\n\n0.00\n\n4\n\n0.07\n\n3\n\n0.04\n\nMSE\n2\n\n0.12\n\n0.18\n\n0.20\n\nv3\n\n1\n\nv1\n\n0.06\n\n9\n\n0.6\n\n0.05\n\n8\n\n0.4\n\n0.04\n\n7\n\n0.2\n\n0.03\n\n6\n\n0.07\n\n5\n\n0.06\n\n4\n\n0.10\n\n3\n\n0.00\n\n2\n\nh1\n\n0.05\n\n0.20\n\n0.05\n0.04\n1\n\nEdge\n\n0.06\n\n0.06\n\nInterior\n\n0.03\n\n0.03\n\n0.04\n\n0.05\n\n0.06\n\nGlobal\n\nv3\n\nv2\n\n0.30\n\n0.07\n\n0.07\n\nv1\n\n0.10\n\nh1\n\n0.00\n\n0.10\n\nMSE\n\nh2\n\n0.20\n\nv1\n\n0.20\n\nh3\n\n0.30\n\nglobal maximums of the function. In this chapter, the information based on the\ntransect bins are collapsed for the ease of interpretation.\n\n7.3 Comparison of lifting estimates with kriging\n\ntion method introduced in Cressie (2015). Given the observations {z(x1 ), . . . , z(xn )}\nat locations {x1 , . . . , xn }, kriging aims to give a linear prediction of the value Z(x0 )\nat a location x0 that is the Best Linear Unbiased Estimator (BLUE) (Christensen,\n1991). We use the ordinary kriging method which assumes spatial stationarity\nin Z(x) = μ + \u000f(x) where the unknown constant μ is mean of Z(x). The linear\nprediction at a location x0 is made as,\nẐ(x0 ) =\n\nn\nX\n\nλi Z(xi ) + \u000f(x0 ),\n\n(7.1)\n\ni=1\n\nwhere λi is the kriging weight and estimated by minimizing the prediction variance\nas\n{λ̂i }ni=1 = argmin E[\u000f(x0 )2 ]\n= Var[Ẑ(x0 ) − Z(x0 )]\n= E[{Ẑ(x0 ) − Z(x0 )}2 ] − {E[Ẑ(x0 ) − Z(x0 )]}2 .\n\n(7.2)\n\nDenote X = (x1 , . . . , xn )> , Σ = Cov(X) and c = Cov(X, x0 ) and c ∈ Rn , the\nlinear prediction of Z(x0 ) minimizing (7.2) is\nẐ(x0 ) = c> Σ−1 X.\n\n(7.3)\n\nWe used the functions in the R package gstat developed by Gräler et al. (2016);\nPebesma (2004) to perform kriging. Although kriging is mainly used to estimate\nthe value of a random variable over a continuous region using unknown locations\nsuch as the grid points, our study aims to attain estimations at the actual data\nlocations by detecting and separating the measurement errors. In this case, the\nkriging estimates can be compared to the lifting estimates. We used a ‘nugget\neffect’ to determine the short scale random variability in the data. The value of the\nnugget term can be obtained from the variogram as the intercept of the variogram\nfunction at a lag distance of almost zero. A large nugget effect value indicates high\nshort-range variability in the random variable and would lead to smooth kriging\nestimates.\nThe same replicates of regular and clustered point patterns and the same noisy function values are used to perform the kriging as explained in Section 7.1. The kriging\nestimates are obtained for the data locations for 250 replicates, using the Doppler,\nHeavisine, Blocks, Bumps and Maartenfunc for various regular, homogeneous, and\nclustered point patterns. In Table 7.1 we compared the function estimation results\n\n147\n\n7.3 Comparison of lifting estimates with kriging\n\nusing Ag ? which was the most favourable weight method in lifting, and kriging results in terms of MSE values with its standard errors. We only selected γ1 , γ5 , γ9\npoint pattern cases rather than using all alternatives, and summarized the results\nbased on the global, interior and edge MSE. For each case, the method with the\nsmallest MSE is highlighted in blue colour. The presented values are for MSE×100,\nand se × 100.\nIn the Doppler test function, Ag ? method performed better than kriging for all\ncases. The variability for the Ag ? is similar for interior and edge regions however\nthe variability at the edges is always higher than interior in kriging. Although the\nDoppler test function generally has smooth features which is suitable for kriging\nestimation, there is a highly oscillated part located at the corner which cause very\nlarge MSE for kriging.\nWe make opposite conclusions for Heavisine, for which kriging performs better in\nall cases with less variation in the estimated function values. The periodic waves\nof Heavisine can be better estimated by kriging, and both Ag ? and kriging have\nhigher MSE for interior region where the spike occurs.\nThe Blocks test function includes various discontinuities over the surface which is\ndifficult for kriging to capture. Kriging over-smooths the blocks and hence causes\nvery high MSE and standard error. It is important to note that the MSE is higher\nfor the interior part where the blocks mostly take place and the edges are flat with\nminor discontinuities.\nKriging gives better performance in the Bumps test function for homogeneous and\nclustered points, and only for the interior of the regular points. The MSE at the\nedges where the bumps are located is higher than the interior. Since the bumps are\nnot sharp discontinuities, the kriging method still works better than Ag ? .\nFinally, in the Maartenfunc, both methods perform similarly but kriging MSE and\nstandard errors are slightly smaller especially for the clustered point patterns. Kriging would not find it difficult to estimate the flat parts of the piecewise linear functions, but to understand how well it estimates the parts with discontinuity, we may\ncheck the results for the transect h1 which is both an edge transect and contain the\nsharpest discontinuity. Since we are interested in the part of the h1 where the discontinuity happens, we cut the transect and take only the part when x = [0.2, 0.4].\nThe MSE results are as follows; Kriging: (R : 17.52, H : 16.63, C : 15.14), and\nAg ? : (R : 14.13, H : 11.69, C : 13.42) for regular, homogeneous and clustered points\nrespectively. It is clear that the Ag ? method performs much better than kriging\n\n148\n\n7.4 Real data application of lifting\n\nwhere the discontinuity exists, and this is valid for the other transect sub-parts as\nwell.\nIf entire transect, or the global, interior and edge parts are considered, the dominance of the good estimation results at the smooth parts of the function in that\ntransect may overshadow the identification of the local performance where the discontinuity occur. Hence checking finer details where the discontinuity happen is\nmore accurate as we uncovered. To sum up, the lifting scheme with Ag ? method has\nsignificantly better performance for the test functions or sub-regions with the discontinuities, which kriging over-smooths. For some cases in Bumps and in Maartenfunc, the Ag ? method compares favourably with kriging, and kriging perform better\nfor the functions with smooth features.\nRegular\nMethod\n\nAg ?\n\nDP\n\nGlobal\nInterior\nEdge\n\nHV\n\nHomogeneous\n\nClustered\n\nKriging\n\nAg ?\n\nKriging\n\nAg ?\n\nKriging\n\n4.41 ± 0.02\n4.03 ± 0.03\n4.75 ± 0.03\n\n8.95 ± 0.04\n6.80 ± 0.04\n10.83 ± 0.07\n\n4.33 ± 0.02\n3.99 ± 0.03\n4.65 ± 0.03\n\n8.73 ± 0.04\n7.17 ± 0.05\n10.2 ± 0.06\n\n4.20 ± 0.02\n3.75 ± 0.02\n4.65 ± 0.03\n\n8.82 ± 0.04\n7.26 ± 0.05\n10.42 ± 0.07\n\nGlobal\nInterior\nEdge\n\n44.19 ± 0.20\n45.74 ± 0.30\n42.84 ± 0.26\n\n34.14 ± 0.15\n37.30 ± 0.24\n31.38 ± 0.19\n\n29.17 ± 0.13\n30.37 ± 0.19\n28.02 ± 0.17\n\n17.19 ± 0.08\n19.15 ± 0.12\n15.30 ± 0.10\n\n22.77 ± 0.10\n23.74 ± 0.15\n21.79 ± 0.14\n\n14.73 ± 0.07\n16.68 ± 0.10\n12.72 ± 0.08\n\nBL\n\nGlobal\nInterior\nEdge\n\n55.03 ± 0.25\n86.91 ± 0.57\n27.15 ± 0.17\n\n200.68 ± 0.90\n361.71 ± 2.37\n59.81 ± 0.37\n\n52.27 ± 0.23\n79.06 ± 0.50\n26.60 ± 0.17\n\n158.26 ± 0.71\n271.79 ± 1.73\n49.42 ± 0.31\n\n50.30 ± 0.22\n74.14 ± 0.47\n26.25 ± 0.17\n\n156.28 ± 0.70\n259.02 ± 1.62\n50.42 ± 0.32\n\nBM\n\nGlobal\nInterior\nEdge\n\n36.26 ± 0.16\n31.12 ± 0.20\n40.75 ± 0.25\n\n42.32 ± 0.19\n22.79 ± 0.15\n59.40 ± 0.36\n\n34.61 ± 0.15\n30.09 ± 0.19\n38.93 ± 0.24\n\n25.53 ± 0.11\n16.06 ± 0.10\n34.61 ± 0.22\n\n33.12 ± 0.15\n28.23 ± 0.18\n38.06 ± 0.24\n\n21.92 ± 0.10\n14.22 ± 0.09\n29.86 ± 0.19\n\nMR\n\nGlobal\nInterior\nEdge\n\n4.22 ± 0.02\n3.49 ± 0.02\n4.86 ± 0.03\n\n3.02 ± 0.01\n3.02 ± 0.02\n3.02 ± 0.02\n\n4.66 ± 0.02\n3.88 ± 0.02\n5.42 ± 0.03\n\n3.11 ± 0.01\n3.03 ± 0.02\n3.19 ± 0.02\n\n4.73 ± 0.02\n4.00 ± 0.03\n5.47 ± 0.03\n\n3.31 ± 0.01\n3.25 ± 0.02\n3.37 ± 0.02\n\nTable 7.1: Mean squared errors with standard errors (both ×100) for lifting estimates using Ag ? method and kriging. Only the results for γ1 = 0: regular, γ5 = 1:\nhomogeneous, and γ9 = 3: highly clustered points are shown. Row panels show\nthe global, interior and edge MSE for Doppler, Heavisine, Blocks, Bumps, and\nMaartenfunc respectively.\n\n7.4\n\nReal data application of lifting\n\nIn this section, we present lifting estimation results for the real data sets spruces,\nBarro Colorado Island (BCI), waka, finpines, and longleaf described in Section 4.3.2. These data sets are particularly selected since the estimated γ̂ for the\ndata sets fall in the interval [0, 3] we used in the simulations, and the real data\nsets also have examples of regular, clustered and homogeneous points. Also, the\nnumber of data locations in these data sets has a large range. The sizes of each\n\n149\n\n7.4 Real data application of lifting\n\ndata, estimated parameter γ̂, the sampling region Ω, and the data set descriptions\nare given in Table 4.5.\nIn the area prediction, only the locations of the points were necessary to tessellate\nthe points in the sampling region and to calculate the cell properties. However,\nlifting requires both the data locations xi for i = 1, . . . , n and the observations yi at\nthe locations yi . The spruces, waka, finpines, and longleaf data sets from the\nspatstat package contain locations of the trees (xi ) and tree diameter observations\n(yi ). The Barro Colorado Island data set is based on the soil nutrient measurements\n(yi ) at the sampled locations (xi ) in a region. There are several chemicals measured\nbut we only used the Aluminum level in the soil.\nThese real data examples have different structures; the BCI data set is a georeferenced data that we measure chemical levels at sampled locations and these\ntype of data are ideally analyzed using kriging methods. The remaining data sets\nare marked point patterns where the marks are the tree diameter and height. We\ninitially used the locations obtained from these data sets in Chapter 4 for the prediction of Voronoi cell area. We also use these data sets for the application of the\nlifting scheme since it is our intention to see how the lifting method works in different data structures. Therefore, in this chapter, we are more interested in the\nillustration of lifting method for real data sets that have homogeneous, clustered,\nregular, and sampled points with different sizes and boundary windows, and measurements rather than solving a real life problem. Hence we aim to inspect the if\nlifting method has limitations in certain cases.\nWe concluded in Section 7.2 that the weight method Ag ? has the best overall performance for the test functions over varying point patterns in terms of the accuracy\nof the lifting estimates. Therefore, it is suggested as the best method and we apply\nit to the real data since its validity has been demonstrated based on the different\nconfigurations in the simulation study. We performed the lifting scheme for the\nreal data sets based on the tree diameters at tree locations, and the Aluminum levels at the sampled locations. The forward transform, thresholding and the inverse\ntransform procedures give the lifting estimations at the locations.\nThe observed values and the results of the estimated values are visualized in Figure 7.6 for all real data sets together. Rows correspond to spruces, BCI, waka,\nfinpines, and longleaf data sets respectively. In each row, the left plot is the\nobserved measurements, centre is the lifting estimates, or the denoised values, and\nthe normal quantile-quantile plot of the residuals are on the right. Voronoi cells\nare coloured based on the observed or estimated values. In this thesis, we do not\n\n150\n\n-0.05\n\nSample Quantiles\n0.00\n0.05\n\nspruces\n\n0.10\n\n7.4 Real data application of lifting\n\n0.15 0.20 0.25 0.30 0.35\n\n0\n\n500 1000 1500\n\n0\n\n500 1000 1500\n\n0\n\n25 50 75 100 125\n\n0\n\n25 50 75 100 125\n\n-2\n\n-1\n0\n1\nTheoretical Quantiles\n\n-2\n\n-1\n0\n1\nTheoretical Quantiles\n\n2\n\n-600\n\nBCI\n\nSample Quantiles\n-200\n0\n200\n\n400\n\n600\n\n0.15 0.20 0.25 0.30 0.35\n\n2\n\n3\n\n-40\n\nwaka\n\nSample Quantiles\n-20\n0\n20\n\n40\n\n60\n\n-3\n\n-2\n\n-1\n0\n1\n2\nTheoretical Quantiles\n\n3\n\n-4\n\n-2\n\nfinpines\n\nSample Quantiles\n0\n2\n\n4\n\n-3\n\n-2\n2\n\n4\n\n6\n\n20\n\n40\n\n60\n\n0\n\n2\n\n4\n\n6\n\n20\n\n40\n\n60\n\n-1\n0\n1\nTheoretical Quantiles\n\n2\n\n-30\n\n-20\n\nlongleaf\n\nSample Quantiles\n-10\n0\n10\n20\n\n30\n\n0\n\n-3\n0\n\n0\n\n-2\n\n-1\n0\n1\n2\nTheoretical Quantiles\n\n3\n\nFigure 7.6: From top row panel to bottom, the lifting results are presented for\nspruces, BCI, waka, finpines, and longleaf data sets respectively. At each row\npanel, original tree diameter (left), lifting estimations (centre), the normal q-q plot\nof the residuals (right) are shown.\n\n151\n\n7.4 Real data application of lifting\n\nstudy the imputation feature of the lifting at unobserved locations such as the grid\npoints. It is one of the challenging aspect of the lifting as discussed in Heaton &\nSilverman (2008) and Peck (2010). The estimated surface is created by disjoint\npolygons which are piecewise linear sub-surfaces.\nThe presentation order of the results is determined based on their estimated γ̂\nparameter values from Table 4.5. The spruces data set has the most regular\npattern in the first row panel. The original tree diameter measurements have some\nirregularity with observed high diameter near the top and left boundary lines and\nsmall diameters at intermittent locations. The lifting estimates at the centre plot\ncreates a quite smooth pattern. The estimated high values are located near the\ntop and left boundary lines where the high diameter was observed at the relevant\nlocations. The locations at the centre of the region are mainly similar and take\napproximately the mid value of the scale.\nThere is over-smoothing cases at some locations, for instance, two points at the\ntop right corner originally have very small and high diameters that are coloured\nin green and dark blue, however, lifting estimated the very similar values for these\npoints. Hence over-smoothing might be a concern here. The normal q-q plot of\nthe residuals on the right show that the majority of the points are located on the\nreference line, but minor violations indicate a slightly right skewed distribution.\nThe Barro Colorado Island data in the second row panel has different nature due\nto the mixture of regular and irregular points. The sampling region is rectangular,\nhence the BCI data set is an example of lifting in a non-square region. The measurements (yi ) at the locations (xi ) is the soil Aluminum level. The smoothing in\nthe lifting estimates at the centre is reasonable and not over-smoothed. We also\nsee some of the discontinuities are preserved from the observed values. The soil\nAluminum estimation is the lowest at the centre of the region and top right corner,\nand high values are observed at the centre-left and top-centre-right. If the overall surfaces for measured and estimated values are compared, the estimated values\nseem to be an appropriately denoised version. The residuals for this data has a\nslight left-skewed shape.\nA homogeneous point pattern, waka, is given the third row panel. It is actually\ndifficult to identify any pattern from the observed tree diameters on the left. One\nextreme observation is located around the top left corner for which the estimated\nvalue is smaller. The overall pattern of the lifting estimates do not show anomaly\nand occasional patterns are identified. Although irregular discontinuities occur near\nthe boundary, the lifting estimations are smooth. The centre plot gives a sensible\n\n152\n\n7.5 Conclusions\n\nunderlying pattern using lifting. The right skew on the residuals is heavier than\nthat seen for spruces.\nAlthough the tree locations in finpines data set constitute some clustering, the\nclusters are not isolated from each other. The pattern of the tree locations generally\nlooks homogeneous but contains clusters at slightly right of centre at the bottom,\nand the top-right corner. The original tree diameter measurements look irregular\nand it is hard to identify if any pattern exists. However, the lifting estimations\non the centre plot show estimated values that clarifies the underlying pattern by\nsmoothing the random variations and also preserving discontinuities. We do not see\nany anomalous boundary effect in the estimated values. The lifting estimates are\nthe highest at the bottom right corner due to the high observed values, however,\nsome of the cells with light yellow colour became darker due to the smoothing.\nIn the original tree diameter values, there are examples of two trees very close to each\nother, one with large and the other with very small diameter. It is possible for two\nadjacent trees to have small diameters, but it is not likely to have both trees to have\nlarge diameter. The lifting estimates for such adjacent trees are indistinguishable\nalthough they are very different in the reality. Other high diameter values are\nestimated at the top-left corner and top-centre, and there are three parts where the\nestimated diameter is small two of which are near the clusters, and one is closer to\nthe top-centre. At the other locations, the lifting estimates are similar and close to\nthe median value.\nThe last row panel in Figure 7.6 shows the results for the longleaf data which also\nshow some degree of clustering. At the highly clustered locations, trees have smaller\ndiameter coloured in yellow and the most high values are observed at the bottomright and bottom-centre. The lifting estimates clarify the underlying pattern that is\nsmooth but the parts where the tree diameter is small and high are still noticeable.\nThe residuals have a skewed distribution as observed in the previous data sets.\n\n7.5\n\nConclusions\n\nThis chapter extends the application of lifting for regular and clustered data scenarios, and real data sets. As concluded in Chapter 6 which suggested the usage of\nthe adjusted weights rather than the observed weights, and Ag ? in particular. Since\ncell properties such as the area depend upon global point intensity ρ for homogeneous points, and local intensity ρi for regular and clustered points, we estimated\nthe {ρ̂}ni=1 and scaled the covariates with respect to ρ̂i and highlighted the area\n\n153\n\n7.5 Conclusions\n\nprediction with ? superscript. The base and augmented model prediction of area\nusing ρ̂i is denoted as B ? and Ag ? which are used as the new weight methods in\nthis chapter hence the total number of weight methods we consider is increased to\nseven.\nThe performances of the weight methods are investigated for the test functions\nusing regular and clustered point patterns with different degrees of regularity and\nclustering. The process we used to generate regular and clustered points is the\nsaturation process by Geyer (1999). The lifting estimates are examined globally,\ninterior, and edge regions, and at different transects. The Ag ? method gave the\nsmallest MSE for most cases, especially near the boundaries. The usage of the\nstandard weight methods that are the observed cell area using the boundaries do\nnot give satisfactory results. In fact, we demonstrated that the convex hull boundary\nhas a poor performance. The area prediction model framework we proposed can be\nused in the situations where the spatial data has a boundary that is either known\nor unknown as we explained in Section 3.5.1 and 3.5.2.\nThe adjusted weight methods B, Ag, B ? and Ag ? generally have compatible performances, except for highly clustered points. When points are highly clustered, the\nrange for {ρ̂i }ni=1 is expected to be higher than the regular points and hence the\ncell area is highly affected by the local intensities. We concluded in Chapter 4 that\nthe area prediction is not robust for regular and clustered points and it is better to\nuse the estimated local intensity (ρ̂i ) to scale the covariates which gave better predictions. Similarly, in lifting, the use of B ? and Ag ? methods gives better function\nestimations compared to B and Ag. For the regular points, the lifting is robust\nwithin the adjusted weight methods.\nThe area prediction results in Chapter 3 and 4 showed that the base model gave\nan overall smaller MSE and the augmented method gave larger MSE but reduced\nthe maximum error. It is surprising that the usage of augmented model in the\nlifting generally outperformed the base model but the lifting estimations in some\ncases were very similar. It is our conclusion that the reduced maximum error on\nthe area prediction yield better lifting estimates since the area is predicted more\naccurately, or the predicted area that has an extreme error gives unstable lifting\nestimates since the area is used as the weight. Hence, although it is difficult to\nsuggest one of the base or augmented methods strictly, we recommend that if one\nwould wish to reduce the maximum error, then augmented models may be used for\nboth area prediction and lifting scheme.\n\n154\n\n7.5 Conclusions\n\nThere are cases where the lifting using the Ag ? weight method performs better\nagainst kriging for the test functions, or the parts of the test functions. The better\nperformance of the lifting is usually near the edges, and on the sub-regions where\ndiscontinuities occur. We performed lifting using the Ag ? method for real data sets\nwhich are examples of regular, homogeneous, and clustered point patterns. It is\nuseful to apply the method on such data sets that forces the previously considered\nsettings of the lifting such as the highly clustered and regular point patterns, having large number of points, and rectangle boundaries. However, the lifting scheme\nworks well for all real data sets and the results are satisfactory. The only downside\nof the algorithm is to become computationally expensive for large n since matrix calculations are involved which would also happen in many other spatial data analysis\nmethod.\n\n155\n\nChapter 8\nDiscussion\nThis thesis investigated the statistical properties of Voronoi cells in bounded regions,\nproposed ways that consider the data in a finite region as if it is in the infinite\nplane, and implemented this method into the lifting scheme framework which is\na denoising method for spatial data. We started the thesis with the investigation\nof the statistical properties of Voronoi cells in the bounded regions using various\nboundary types in Chapter 2. This part of the study was based on the homogeneous\nPoisson points with a specific point intensity and discovered the effects of imposed\nboundaries on the cell properties such as cell area, perimeter, and number of cell\nedges. The distributions of the cell properties differed for the cells that are close to\nthe boundary compared to the cells in the infinite plane. Also, we found that the\nboundary type matters. The study was also carried out for various unit intensities\nof points and we found that the differences in the cell properties near the boundaries\nremained.\nOur initial study in Chapter 2 raised an important concern in the analysis of spatial data that usually come within a finite region and depend on a neighbourhood\nstructure. In the case of bounded or finite regions, the neighbourhood structure is\ndisrupted by the boundary. Therefore a data point located at or near the edge or\nthe corner of the region may only have neighbours occasionally. This may cause\nissues if the neighbourhood structure is used in the analysis of the spatial data such\nas the lifting scheme we used.\nIn the lifting scheme, a weighted sum of the values of the neighbours is calculated.\nFor a data point located at the edge or corner of the region, there might be only a\nfew neighbours which the weighted sum is calculated from, and no neighbours on the\nother side. Boundaries act as a cutoff point and hence no further observations are\n\n156\n\navailable. In the context of Voronoi tessellations, boundaries restrict the Voronoi\ncells that affect the cell area. Since the lifting scheme uses both the neighbourhood\nstructure and the cell area that are determined by the Voronoi cells, our findings in\nChapter 2 have importance on understanding the boundary effects on the function\nestimation.\nWe devised a process in Chapter 3 that treats the areas of Voronoi cells in the\nbounded region as if they are in an infinite plane by area adjustment based on a\nregression-based method. We extended this method for regular and clustered data\ncase in Chapter 4 and combined it with the lifting scheme later in Chapter 6 and 7\nand compared the performances of the proposed and standard methods. However,\nthe approaches we proposed in Chapter 3 and 4 would have a general potential use\nin the analysis of spatial data.\nOne application would be the case where data are represented as a marked point\nprocess, with the marks being related to the area surrounding each point. For\ninstance, consider ecological or forestry data that contain plant or tree locations\nand the territories that the plants occupy. Voronoi cells can be considered as the\nterritories that the plants occupy, and we would adjust the cells (territories) near\nthe edges using the method we created. This would be useful when the areas of\nVoronoi cells are considered as the mark process that is associated with a point\nprocess, hence the correction of the cell areas would aim to reduce the bias near\nthe edges. Such issues related to dependencies between the marks and locations are\nmentioned in Schlather et al. (2004).\nAn exploratory analysis is performed in Section 3.6 for the classification of the cells\nthat are likely to be affected by a boundary. We used the simplest approach to\nclassify the boundary effected cells which gave promising results. However, the\nclassification of boundary effected cells can be a separate extensive study where\nmore sophisticated binary classification methods such as decision trees or random\nforests.\nThe lifting scheme in two dimensions based on Voronoi tessellations is the mechanism we used throughout the thesis based on the specifications in Jansen et al.\n(2009). The lifting scheme requires data locations and observations, and has vital\nconfigurations which depend on cell area such as the decision of the lifting order,\nand the weights that are used in the calculations. Voronoi tessellations assist to\nhandle these facets of the method. However, one should be careful about the boundary effects on the Voronoi cells. The standard way of performing the Voronoi-based\nlifting is to use the cell areas as the weights. If the data is given within a finite\n\n157\n\nregion, the actual sampling region Ω or the convex hull can be taken as the boundary as standard options to calculate the cell areas. However, Ω may not always be\navailable or we would like to use a more sophisticated way that deals with this issue.\nHere the area adjustment method introduced in Chapter 3 becomes a functional\ntool to assign new weights to the cells rather than using observed cell area.\nThe steps of the lifting scheme are described in Chapters 5 which we explained the\nrole of the weights. The method in Chapter 3 and 4 works well in conjunction with\nthe lifting scheme and gives promising results, as presented in Chapter 6 and 7.\nWhen such a method is suggested, it is important to validate its performances for\nvarious scenarios so we considered many different point patterns, test functions, and\nweight methods, and compared the results with methods such as kriging which our\nsuggested method compares favourably to many cases. However, the settings of the\nconfigurations can be expanded and other situations may be taken into account. For\ninstance, we considered homogeneous Poisson points, clustered points, and regular\npoints, but there are many other point processes for which the lifting scheme and cell\narea adjustment method can be tested. Also, we focused on two essential boundary\ntypes: convex hull and unit square, and have not used other types of imposed or real\nboundaries to avoid moving beyond the scope of the thesis, but this is a potential\navenue for future work.\nWe understood from this thesis that the geometrical properties of Voronoi cells\nchange when the boundaries are imposed. This consequence should be contemplated\nin the usage of methods that rely on Voronoi tessellations. We used the cell area in\nthe lifting scheme but there are many areas where the properties of Voronoi cells\nare used such as astronomy, geology, agriculture, physics, and wireless networks.\nIt is important to consider the impact of the boundaries on the cell perimeter and\nthe number of cell edges when these cell properties are used in a study solely or in\nconjunction with other methods. Furthermore, the study of Voronoi tessellations\nin two-dimensional bounded regions can be expanded to three-dimensional case.\nAlthough the three-dimensional Voronoi tessellation is investigated in Kumar et al.\n(1992); Lazar et al. (2013); Muche (1996); Tanemura (2003), the focus was not on\nthe properties of the polyhedrons due to imposed boundaries. The lifting scheme in\nthree dimensions using the Voronoi polyhedrons would be another important future\nstudy.\nThe lifting scheme we used aims to estimate underlying true patterns separated\nfrom noise by the inverse transform of the thresholded detail coefficients. These\n\n158\n\nestimations are made for the data locations itself. However, another interesting objective would be to estimate the value at an unobserved location. The lifting scheme\nis a recently developed method and most of its aspects are still being developed.\nHeaton & Silverman (2008) and Peck (2010) introduced lifting-based imputation\nmethods which our weight method approaches might be combined with, and the\nperformances of different weight methods could be tested. Finally, we have checked\nthe q-q plots of the lifting estimates for real data cases and the results show that\nthe residuals do not obey a particular parametric distribution. Hence, the future\nwork may also consider the residual analysis of the lifting estimates.\n\n159\n\nAppendix A\nExtra plots and tables\n\nFigure A.1: Gamma, Weibull and log-normal distributions are fitted for the standardized cell area in the infinite plane.\n\n160\n\nTraining Data\n\nAugmented Training Data\n\nunit.area:type\nunit.area\ndist.edge2\ndist.edge:type\nunit.per\nchull.area:on.chull\nchull.per\nchull.area:dist.edge2\nunit.area:dist.edge\nchull.area\nunit.vert\nunit.edge\nchull.edge\nchull.vert\nm\ndist.edge\n\nunit.area:type\nunit.area\nm\ndist.edge2\ndist.edge:type\nchull.per\nchull.area:on.chull\nunit.per\nchull.edge\nunit.edge\nunit.area:dist.edge\nchull.area:dist.edge2\nchull.area\nunit.vert\nchull.vert\ndist.edge\n0\n\n20 40 60 80\nTimes selected\n\n100\n\n0\n\n20 40 60 80\nTimes selected\n\n100\n\nFigure A.2: Selected variables in the unit square boundary models when the related\nvariables are removed. Results are given for the base models (left) and augmented\nmodels (right).\n\n161\n\nAppendix B\nTest functions and R Codes\nB.1\n\nTest functions\n\nLet us specify the theoretical definitions of the test functions.\n\nDoppler\n\u0012\nf (x, y) = sin\n\n1\n2\nx + y2\n\n\u0013\n,\n\n0 < x, y ≤ 1\n\n(B.1)\n\nHeavisine\n1 z−μ 2\n1\nf1 (z; μ, σ 2 ) = √ e− 2 ( σ )\nσ 2π\np\nf2 (x, y) = sin(a x2 + y 2 )\n\nf3 (x, y) = f1 (x; μ1 , σ 2 )f1 (y; μ2 , σ 2 )\nf (x, y) = f2 + f3\n\n0 ≤ x, y ≤ 1\n\n(a = 20, σ = 0.01, p = 0.005, μ1 = 0.55, μ2 = 0.50)\n\n162\n\n(B.2)\n\nB.1 Test functions\n\nBlocks\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nf (x, y) = 5\n\n\n6\n\n\n7\n\n\n8\n\n\n0\n\nif 0 ≤ x < 0.1\nif 0 ≤ y < 0.2\nif 0.3 < x < 0.4, 0.7 < y < 0.8\nif 0.7 < x < 0.8, 0.7 < y < 0.8\nif 0.5 < x < 0.6, 0.4 < y < 0.6\nif 0.3 < x < 0.8, 0.2 < y < 0.3\nif 0.2 < x < 0.3, 0.3 < y < 0.4\nif 0.8 < x < 0.9, 0.3 < y < 0.4\notherwise\n\nfor all 0 ≤ x, y ≤ 1\n\n(B.3)\n\nBumps\n\u001a\n\u001b\n|z − μ|\n1\nexp −\nf1 (z; μ, b) =\n2b\nb\n3\n\u0010\nX\np \u0011\np \u0011 \u0010\ny\nx\nf (x, y) =\nf1 x; μj , bj f1 y; μj , bj\n\n0 ≤ x, y ≤ 1\n\n(B.4)\n\nj=1\n\n(μx = (0.1, 0.8, 0.9), μy = (0.4, 0.7, 0.1), b = (0.01, 0.02, 0.015))\n\nMaartenfunc\n(\n2x + y\nf (x, y) =\n5x − y\n\nif 3x − y < 1\nif 3x − y ≥ 1\n\n163\n\nfor all 0 ≤ x, y ≤ 1\n\n(B.5)\n\nB.1 Test functions\n\n## Doppler test function\ndopplerfunc <- function (x,y) {\nr = sqrt(x^2 + y^2)\nf = sin(1/(r^2))\nf }\n## Blocks test function\nblockfunc <- function(x,y){\nf <- rep(0, length(x))\nsv <- x < 0.1; f[sv] <- f[sv] + 1\nsv <- y < 0.2; f[sv] <- f[sv] + 2\nsv <- (x>0.3)&(x < 0.4)&(y<0.8)&(y>0.7); f[sv] <- f[sv] + 3\nsv <- (x>0.7)&(x < 0.8)&(y<0.8)&(y>0.7); f[sv] <- f[sv] + 4\nsv <- (x>0.5)&(x < 0.6)&(y<0.6)&(y>0.4); f[sv] <- f[sv] + 5\nsv <- (x>0.3)&(x < 0.8)&(y<0.3)&(y>0.2); f[sv] <- f[sv] + 6\nsv <- (x>0.2)&(x < 0.3)&(y<0.4)&(y>0.3); f[sv] <- f[sv] + 7\nsv <- (x>0.8)&(x < 0.9)&(y<0.4)&(y>0.3); f[sv] <- f[sv] + 8\nf }\n## Heavisine test function\nheavisinefunc = function(x,y, pp=0.005, sd=0.01, freq=20){\nr = sqrt(x^2 + y^2)\nf1 = sin(freq*r)\nf2 = pp*dnorm(x,0.55,sd=sd)*dnorm(y,0.5, sd=sd)\nf1+f2\n}\n## Bumps test function\nbumpsfunc <- function (x,y) {\nxc = c(0.1, 0.8, 0.9); yc = c(0.4,0.7, 0.1); vc = c(0.01,0.02, 0.015)\nnc = length(xc)\nans = rep(0,length(y))\nfor(i in 1:nc) {ans = ans + doubexp(x, mean=xc[i], rate=sqrt(vc[i]))*\ndoubexp(y, mean=yc[i], rate=sqrt(vc[i]))}\nans}\ndoubexp = function(x, mean=0, rate=1){exp(-abs(x-mean)/rate)/(2*rate)}\n## Maartenfunc\nmaartenfunc <- function(x,y){\nfun = numeric()\nfor (i in 1:length(x)) {\nif((3*x[i] - y[i]) < 1) {fun[i] = 2*x[i] + y[i]}\nif((3*x[i] - y[i]) >= 1){fun[i] = 5*x[i] - y[i]}\n}\nfun\n}\n\n164\n\nB.2 Example code for statistical properties of Poisson Voronoi cells\n\nB.2\n\nExample code for statistical properties of Poisson Voronoi cells\n\nrequire(deldir)\nrequire(tripack)\nrequire(rgeos)\n## function to calculate distance from a point to a line\npt.ln = function(x0, y0, x1, y1, x2, y2){\ndistance = abs((y2-y1)*x0 - (x2-x1)*y0 + x2*y1 - y2*x1)/sqrt((y2-y1)^2 + (x2-x1)^2)\ndistance\n}\n## Generate points\nset.seed(22)\nrho = 200\nn = rpois(1, rho)\nx = runif(n, 0, 1);y = runif(n, 0, 1)\n## Voronoi tessellation\ntes = deldir(x,y, rw = c(0,1,0,1))\n## Convex hull of the points\nchull1 = convex.hull(tri.mesh(x, y))\npoly1 = Polygon(cbind(chull1$x,chull1$y))\np1 = SpatialPolygons(list(Polygons(list(poly1), \"p1\")))\n## Define variables\nunit.area = unit.per = unit.edge = chull.area = chull.per = chull.edge = numeric()\ndist.edge = dist.edge2 = dist.cent = unit.vert = chull.vert = numeric()\non.chull = type = logical()\nfor (k in 1:n) {# Loop to calculate cell properties for n points\n## Cell vertex coordinates\nss = rbind(as.matrix(tes$dirsgs[(tes$dirsgs[,5] == k)|(tes$dirsgs[,6] == k),]))\nv = matrix(unlist(ss[, 1:4]), ncol = 4)\nbp1 = ss[, 7]\nbp2 = ss[, 8]\nv1 = cbind(v[, 1:2, drop = FALSE], 0 + bp1)\nv2 = cbind(v[, 3:4, drop = FALSE], 0 + bp2)\nvv = rbind(v1,v2)\n\n165\n\nB.2 Example code for statistical properties of Poisson Voronoi cells\n\nangle = atan2(vv[, 2] - y[k], vv[, 1] - x[k])\nangle.0 = sort(unique(angle))\nvert = vv[match(angle.0, angle), ]\nvv1 = vert[, 1]\nvv2 = vert[, 2]\nbp = as.logical(vert[, 3])\nrw = tes$rw\ni.crnr = get.cnrind(x, y, rw)\nii = i.crnr %in% k\nx.crnrs = rw[c(1, 2, 2, 1)]\ny.crnrs = rw[c(3, 3, 4, 4)]\nvert.x = c(vv1, x.crnrs[ii])\nvert.y = c(vv2, y.crnrs[ii])\n## Cell vertices ordered\nvert.coord = cbind(vert.x,vert.y)\nf.bp = c(bp, rep(TRUE, sum(ii)))\nf.sort = atan2(vert.coord[, 2] - y[k], vert.coord[, 1] - x[k])\nf.sort.0 = sort(f.sort)\nf.vert = vert.coord[match(f.sort.0, f.sort), ]\n## Cell edge segments\nlgth = numeric()\nsgm = dim(f.vert)[1]\nfor (kk in 1:sgm) {\nlgth[kk] = if(kk+1 <= sgm){\nsqrt((f.vert[kk,][1]-f.vert[kk+1,][1])^2 +\n(f.vert[kk,][2]-f.vert[kk+1,][2])^2)\n}\nelse{sqrt((f.vert[1, ][1]-f.vert[kk,][1])^2 +\n(f.vert[1,][2]-f.vert[kk,][2])^2) }\n}\n# -----------------------------------------------unit.per[k] = sum(lgth) # <-- unit perimeter\n# -----------------------------------------------# -----------------------------------------------unit.edge[k] = sgm # <-- unit edges\n# -----------------------------------------------# ------------------------------------------------\n\n166\n\nB.2 Example code for statistical properties of Poisson Voronoi cells\n\ntype[k] = sum(f.bp) # <-- cell type\n# -----------------------------------------------# Cell vertices as SP class\nchull2 = convex.hull(tri.mesh(vert.coord[,1], vert.coord[,2]))\npoly2 = Polygon(cbind(chull2$x, chull2$y))\np2 = SpatialPolygons(list(Polygons(list(poly2), \"p2\")))\n# Intersect the cell with convex hull\nres = gIntersection(p1, p2)\n# ----------------------------------------------------------------------------chull.area[k] = unlist(sapply(slot(res, \"polygons\"), function(p) sapply(\nslot(p, \"Polygons\"), slot, \"area\")))\n# ----------------------------------------------------------------------------# vertices of the intersection\npts = matrix(unlist(sapply(slot(res, \"polygons\"), function(p) sapply(\nslot(p, \"Polygons\"), slot, \"coords\"))), ncol=2)\n# chull line segments\nchull.line = numeric()\nchull.lgth = dim(pts)[1]\nfor (jj in 1:(chull.lgth-1)) {\nchull.line[jj] =\nsqrt((pts[jj,][1]-pts[jj+1,][1])^2 +\n(pts[jj,][2]-pts[jj+1,][2])^2)\n}\n# -----------------------------------------------------------------chull.per[k] = sum(chull.line)\n# <-- Convex hull perimeter\n# -----------------------------------------------------------------# ----------------------------------------------------------------chull.edge[k] = (chull.lgth - 1) # <-- Convex hull edges\n# ----------------------------------------------------------------# ----------------------------------------------------------------------------dist.edge[k] = min(abs(x[k]-1), abs(x[k]-0), # <-- distance from the point to\nabs(y[k]-1), abs(y[k]-0)) #\nunit boundary\n# ----------------------------------------------------------------------------# ----------------------------------------------------------------------------on.chull[k] = sum(on.convex.hull(tri.mesh(x,y),x[k], y[k])) # <-- on convex hull\n\n167\n\nB.2 Example code for statistical properties of Poisson Voronoi cells\n\n# ----------------------------------------------------------------------------## unit vertex dist\nrwin = matrix(c(0,0,0,1,\n0,1,1,1,\n1,1,1,0,\n1,0,0,0), 4,4)\nd = dd = numeric()\nfor (zz in 1:(nrow(vert.coord))) {\nfor (ll in 1:4) {\nd[ll] = pt.ln(vert.coord[zz, 1], vert.coord[zz, 2], rwin[1,ll],rwin[2,ll],\nrwin[3,ll],rwin[4,ll])}\ndd[zz] = min(d)}\n# --------------------------------------------------------------------------unit.vert[k] = min(dd) # <-- min distance from the vertices to the\nto the unit square boundary\n# --------------------------------------------------------------------------## chull vertex distance\ncwin = slot(poly1, ’coords’)\ncver = pts\nd = dd = numeric()\nfor (zz in 1:(nrow(cver))) {\nfor (ll in 1:(nrow(cwin) -1)) {\nd[ll] = pt.ln(cver[zz, 1], cver[zz, 2], cwin[ll,1],cwin[ll,2],\ncwin[ll+1,1],cwin[ll+1,2])}\ndd[zz] = min(d)}\n# --------------------------------------------------------------------------chull.vert[k] = min(na.omit(dd)) # <-- min distance from the vertices\nto the chull boundary\n# --------------------------------------------------------------------------## min distance from chull boundary\ndc = numeric()\nfor (ll in 1:(dim(cwin)[1] -1)) {\ndc[ll] = pt.ln(x[k], y[k], cwin[ll,1],cwin[ll,2],\ncwin[ll+1,1],cwin[ll+1,2])}\n# ----------------------------------------------------------------------------dist.edge2[k] = min(dc) # <-- distance from the point to chull boundary\n# ----------------------------------------------------------------------------# -----------------------------------------------------------------------------\n\n168\n\nB.2 Example code for statistical properties of Poisson Voronoi cells\n\ndist.cent[k] = (0.5-x[k])^2 + (0.5-y[k])^2 # <-- distace from the centre\n# ----------------------------------------------------------------------------}# end of loop\n# ----------------------------------------------------------------------------unit.area = tes$summary$dir.area # <-- unit area\n# ----------------------------------------------------------------------------sim.df = data.frame(x, y, unit.area, unit.per, unit.edge, chull.area,\nchull.per, chull.edge, dist.edge, dist.edge2, dist.cent,\nunit.vert, chull.vert,on.chull, type)\n\n169\n\nAppendix C\nTables of MSE values for regular\nand clustered data\n\n170\n\nγ=0\n\nγ = 0.25\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nG\nI\nE\n\n0.045\n0.038\n0.052\n\n0.044\n0.039\n0.048\n\n0.044\n0.040\n0.048\n\n0.045\n0.040\n0.049\n\n0.044\n0.040\n0.048\n\n0.045\n0.040\n0.050\n\n0.044\n0.040\n0.047\n\n0.045\n0.039\n0.051\n\n0.045\n0.041\n0.048\n\n0.046\n0.042\n0.049\n\n0.045\n0.040\n0.049\n\n0.045\n0.041\n0.048\n\n0.045\n0.041\n0.049\n\n0.045\n0.041\n0.048\n\nv1\nv2\nh1\nh2\nD\n\n0.100\n0.015\n0.105\n0.014\n0.068\n\n0.090\n0.014\n0.093\n0.013\n0.068\n\n0.087\n0.012\n0.090\n0.012\n0.069\n\n0.087\n0.013\n0.089\n0.012\n0.066\n\n0.087\n0.012\n0.086\n0.011\n0.067\n\n0.091\n0.013\n0.091\n0.012\n0.069\n\n0.084\n0.012\n0.082\n0.011\n0.065\n\n0.103\n0.015\n0.106\n0.014\n0.072\n\n0.092\n0.014\n0.090\n0.013\n0.066\n\n0.089\n0.013\n0.088\n0.012\n0.068\n\n0.090\n0.012\n0.093\n0.012\n0.067\n\n0.090\n0.011\n0.088\n0.012\n0.065\n\n0.089\n0.012\n0.091\n0.013\n0.068\n\n0.087\n0.011\n0.086\n0.012\n0.064\n\nG\nI\nE\n\n0.468\n0.425\n0.506\n\n0.456\n0.438\n0.472\n\n0.455\n0.447\n0.461\n\n0.451\n0.451\n0.450\n\n0.444\n0.459\n0.431\n\n0.449\n0.451\n0.446\n\n0.442\n0.457\n0.428\n\n0.451\n0.408\n0.489\n\n0.430\n0.417\n0.442\n\n0.432\n0.428\n0.435\n\n0.424\n0.424\n0.424\n\n0.422\n0.435\n0.410\n\n0.419\n0.421\n0.418\n\n0.420\n0.439\n0.403\n\nv1\nv2\nh1\nh2\nD\n\n0.551\n0.818\n0.544\n0.813\n0.489\n\n0.517\n0.678\n0.512\n0.680\n0.459\n\n0.508\n0.588\n0.503\n0.593\n0.456\n\n0.494\n0.584\n0.501\n0.580\n0.445\n\n0.480\n0.520\n0.487\n0.539\n0.444\n\n0.501\n0.573\n0.503\n0.590\n0.466\n\n0.480\n0.515\n0.475\n0.534\n0.431\n\n0.554\n0.783\n0.528\n0.790\n0.484\n\n0.488\n0.630\n0.483\n0.621\n0.419\n\n0.476\n0.545\n0.474\n0.538\n0.412\n\n0.481\n0.560\n0.458\n0.560\n0.415\n\n0.460\n0.500\n0.454\n0.501\n0.411\n\n0.468\n0.532\n0.452\n0.542\n0.418\n\n0.463\n0.488\n0.443\n0.492\n0.396\n\nG\nI\nE\n\n0.553\n0.839\n0.303\n\n0.520\n0.800\n0.276\n\n0.521\n0.807\n0.271\n\n0.545\n0.852\n0.277\n\n0.555\n0.876\n0.275\n\n0.560\n0.873\n0.286\n\n0.550\n0.869\n0.272\n\n0.545\n0.812\n0.312\n\n0.528\n0.812\n0.281\n\n0.529\n0.819\n0.276\n\n0.533\n0.818\n0.285\n\n0.536\n0.837\n0.274\n\n0.533\n0.827\n0.276\n\n0.524\n0.820\n0.267\n\nv1\nv2\nv3\nv4\nh1\nh2\nh3\n\n0.505\n0.875\n0.447\n0.273\n0.246\n0.518\n0.132\n\n0.422\n0.802\n0.423\n0.242\n0.207\n0.516\n0.115\n\n0.374\n0.789\n0.428\n0.229\n0.194\n0.512\n0.109\n\n0.400\n0.923\n0.432\n0.249\n0.203\n0.525\n0.115\n\n0.362\n0.915\n0.436\n0.253\n0.182\n0.531\n0.109\n\n0.417\n0.917\n0.458\n0.252\n0.205\n0.516\n0.119\n\n0.360\n0.904\n0.443\n0.250\n0.173\n0.531\n0.119\n\n0.532\n0.824\n0.417\n0.281\n0.272\n0.544\n0.129\n\n0.448\n0.832\n0.407\n0.252\n0.208\n0.587\n0.110\n\n0.387\n0.827\n0.413\n0.244\n0.198\n0.583\n0.105\n\n0.425\n0.836\n0.408\n0.261\n0.212\n0.546\n0.109\n\n0.363\n0.806\n0.411\n0.265\n0.179\n0.566\n0.104\n\n0.403\n0.809\n0.415\n0.233\n0.204\n0.580\n0.107\n\n0.338\n0.821\n0.416\n0.255\n0.172\n0.532\n0.103\n\nG\nI\nE\n\n0.443\n0.306\n0.562\n\n0.396\n0.294\n0.485\n\n0.385\n0.296\n0.464\n\n0.380\n0.305\n0.445\n\n0.360\n0.306\n0.408\n\n0.376\n0.304\n0.438\n\n0.363\n0.311\n0.408\n\n0.419\n0.292\n0.531\n\n0.399\n0.294\n0.490\n\n0.388\n0.299\n0.464\n\n0.366\n0.295\n0.427\n\n0.356\n0.310\n0.396\n\n0.360\n0.294\n0.417\n\n0.358\n0.313\n0.397\n\nv1\nv2\nv3\nv4\nv5\nh1\nh2\n\n0.840\n0.519\n0.096\n0.649\n1.208\n0.674\n0.297\n\n0.706\n0.453\n0.093\n0.559\n1.033\n0.566\n0.209\n\n0.599\n0.488\n0.090\n0.624\n0.819\n0.483\n0.167\n\n0.566\n0.462\n0.092\n0.563\n0.862\n0.469\n0.202\n\n0.507\n0.474\n0.092\n0.562\n0.711\n0.399\n0.156\n\n0.579\n0.454\n0.089\n0.569\n0.825\n0.458\n0.180\n\n0.485\n0.467\n0.088\n0.552\n0.723\n0.399\n0.155\n\n0.852\n0.494\n0.102\n0.652\n1.158\n0.580\n0.277\n\n0.717\n0.438\n0.103\n0.587\n1.050\n0.548\n0.222\n\n0.597\n0.467\n0.100\n0.641\n0.828\n0.476\n0.174\n\n0.548\n0.428\n0.092\n0.582\n0.802\n0.426\n0.173\n\n0.496\n0.409\n0.102\n0.577\n0.649\n0.387\n0.150\n\n0.554\n0.395\n0.094\n0.569\n0.766\n0.409\n0.169\n\n0.461\n0.423\n0.098\n0.610\n0.672\n0.381\n0.154\n\nG\nI\nE\n\n0.053\n0.038\n0.066\n\n0.048\n0.037\n0.057\n\n0.048\n0.039\n0.055\n\n0.045\n0.037\n0.052\n\n0.043\n0.035\n0.049\n\n0.045\n0.036\n0.052\n\n0.042\n0.035\n0.049\n\n0.054\n0.040\n0.066\n\n0.048\n0.037\n0.057\n\n0.047\n0.039\n0.054\n\n0.046\n0.038\n0.054\n\n0.044\n0.037\n0.050\n\n0.046\n0.036\n0.054\n\n0.044\n0.036\n0.051\n\nv1\nv2\nv3\nh1\nh2\nh3\n\n0.051\n0.050\n0.185\n0.101\n0.047\n0.052\n\n0.045\n0.046\n0.160\n0.085\n0.041\n0.042\n\n0.041\n0.046\n0.133\n0.078\n0.042\n0.042\n\n0.038\n0.047\n0.131\n0.074\n0.041\n0.043\n\n0.034\n0.045\n0.112\n0.070\n0.039\n0.039\n\n0.041\n0.045\n0.126\n0.074\n0.040\n0.043\n\n0.035\n0.045\n0.108\n0.070\n0.037\n0.039\n\n0.057\n0.050\n0.181\n0.100\n0.044\n0.053\n\n0.044\n0.046\n0.154\n0.084\n0.044\n0.044\n\n0.039\n0.047\n0.125\n0.076\n0.045\n0.043\n\n0.039\n0.046\n0.130\n0.077\n0.039\n0.043\n\n0.039\n0.045\n0.106\n0.072\n0.039\n0.040\n\n0.041\n0.048\n0.127\n0.076\n0.042\n0.044\n\n0.038\n0.048\n0.108\n0.070\n0.041\n0.043\n\nTable C.1: Mean squared error for the lifting estimations for regular and clustered\npoints when γ = 0, 0.25. The rows denote the different spatial parts of the region,\nand the columns give the results for different weight methods. If the parameter γ <\n1 it indicates the incremental magnitudes of inhibition or repulsion, and clustering\nif γ > 1. Results are given for Dopper, Heavisine, Blocks, Bumps and Maartenfunc\ntest functions from top to bottom panel respectively. MSE calculated globally is\ndenoted as (G), interior (I), edge (E) of the region, and the vertical (v), horizontal\n(h), and diagonal (D) transects.\n171\n\nγ = 0.5\n\nγ = 0.75\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nG\nI\nE\n\n0.046\n0.039\n0.053\n\n0.044\n0.039\n0.049\n\n0.045\n0.040\n0.049\n\n0.045\n0.042\n0.049\n\n0.045\n0.041\n0.048\n\n0.044\n0.040\n0.047\n\n0.045\n0.041\n0.048\n\n0.045\n0.038\n0.052\n\n0.044\n0.039\n0.048\n\n0.045\n0.040\n0.049\n\n0.044\n0.039\n0.047\n\n0.042\n0.039\n0.045\n\n0.042\n0.039\n0.045\n\n0.043\n0.039\n0.046\n\nv1\nv2\nh1\nh2\nD\n\n0.106\n0.015\n0.106\n0.014\n0.064\n\n0.094\n0.014\n0.097\n0.013\n0.066\n\n0.092\n0.013\n0.096\n0.012\n0.067\n\n0.090\n0.013\n0.087\n0.013\n0.062\n\n0.087\n0.011\n0.086\n0.011\n0.059\n\n0.089\n0.012\n0.089\n0.012\n0.056\n\n0.087\n0.011\n0.085\n0.012\n0.061\n\n0.105\n0.015\n0.111\n0.016\n0.064\n\n0.088\n0.014\n0.098\n0.013\n0.058\n\n0.086\n0.013\n0.097\n0.012\n0.060\n\n0.086\n0.012\n0.089\n0.013\n0.057\n\n0.084\n0.012\n0.081\n0.012\n0.061\n\n0.086\n0.012\n0.085\n0.012\n0.058\n\n0.086\n0.011\n0.080\n0.012\n0.057\n\nG\nI\nE\n\n0.426\n0.391\n0.456\n\n0.399\n0.392\n0.405\n\n0.401\n0.404\n0.399\n\n0.386\n0.392\n0.379\n\n0.381\n0.390\n0.372\n\n0.384\n0.390\n0.379\n\n0.375\n0.389\n0.362\n\n0.387\n0.354\n0.418\n\n0.350\n0.347\n0.353\n\n0.352\n0.357\n0.348\n\n0.345\n0.349\n0.341\n\n0.336\n0.353\n0.321\n\n0.347\n0.350\n0.344\n\n0.338\n0.353\n0.323\n\nv1\nv2\nh1\nh2\nD\n\n0.514\n0.730\n0.540\n0.750\n0.451\n\n0.462\n0.590\n0.459\n0.600\n0.388\n\n0.451\n0.510\n0.451\n0.522\n0.390\n\n0.431\n0.512\n0.435\n0.523\n0.367\n\n0.404\n0.459\n0.412\n0.460\n0.361\n\n0.418\n0.496\n0.422\n0.525\n0.366\n\n0.403\n0.459\n0.415\n0.438\n0.357\n\n0.462\n0.728\n0.492\n0.710\n0.407\n\n0.414\n0.520\n0.426\n0.521\n0.345\n\n0.408\n0.454\n0.416\n0.457\n0.347\n\n0.369\n0.483\n0.387\n0.467\n0.364\n\n0.345\n0.410\n0.368\n0.400\n0.350\n\n0.371\n0.484\n0.384\n0.470\n0.353\n\n0.334\n0.406\n0.363\n0.401\n0.330\n\nG\nI\nE\n\n0.549\n0.815\n0.308\n\n0.537\n0.819\n0.283\n\n0.539\n0.827\n0.279\n\n0.533\n0.820\n0.275\n\n0.527\n0.813\n0.270\n\n0.542\n0.835\n0.277\n\n0.539\n0.832\n0.276\n\n0.548\n0.798\n0.315\n\n0.521\n0.783\n0.277\n\n0.522\n0.794\n0.269\n\n0.528\n0.803\n0.272\n\n0.538\n0.826\n0.269\n\n0.524\n0.798\n0.269\n\n0.538\n0.826\n0.270\n\nv1\nv2\nv3\nv4\nh1\nh2\nh3\n\n0.525\n0.824\n0.389\n0.300\n0.260\n0.545\n0.127\n\n0.432\n0.843\n0.393\n0.264\n0.216\n0.549\n0.111\n\n0.375\n0.836\n0.399\n0.249\n0.207\n0.553\n0.108\n\n0.395\n0.819\n0.384\n0.261\n0.203\n0.551\n0.110\n\n0.367\n0.804\n0.393\n0.253\n0.196\n0.567\n0.102\n\n0.387\n0.845\n0.403\n0.267\n0.201\n0.543\n0.110\n\n0.359\n0.814\n0.392\n0.256\n0.208\n0.563\n0.104\n\n0.550\n0.813\n0.393\n0.309\n0.262\n0.555\n0.141\n\n0.453\n0.788\n0.371\n0.241\n0.216\n0.521\n0.109\n\n0.383\n0.781\n0.362\n0.223\n0.197\n0.508\n0.105\n\n0.424\n0.794\n0.380\n0.236\n0.216\n0.535\n0.121\n\n0.381\n0.847\n0.390\n0.226\n0.212\n0.572\n0.115\n\n0.414\n0.812\n0.377\n0.240\n0.207\n0.522\n0.121\n\n0.361\n0.838\n0.395\n0.241\n0.199\n0.551\n0.113\n\nG\nI\nE\n\n0.416\n0.291\n0.529\n\n0.386\n0.296\n0.468\n\n0.381\n0.300\n0.454\n\n0.367\n0.297\n0.430\n\n0.351\n0.293\n0.402\n\n0.357\n0.297\n0.411\n\n0.358\n0.304\n0.407\n\n0.421\n0.296\n0.536\n\n0.374\n0.296\n0.447\n\n0.371\n0.302\n0.436\n\n0.360\n0.296\n0.420\n\n0.348\n0.302\n0.391\n\n0.352\n0.287\n0.413\n\n0.338\n0.293\n0.380\n\nv1\nv2\nv3\nv4\nv5\nh1\nh2\n\n0.846\n0.566\n0.094\n0.649\n1.142\n0.561\n0.285\n\n0.687\n0.502\n0.092\n0.606\n0.964\n0.514\n0.203\n\n0.606\n0.552\n0.091\n0.687\n0.781\n0.441\n0.170\n\n0.578\n0.477\n0.091\n0.619\n0.817\n0.414\n0.197\n\n0.504\n0.468\n0.092\n0.583\n0.713\n0.370\n0.156\n\n0.549\n0.496\n0.090\n0.579\n0.769\n0.388\n0.168\n\n0.510\n0.459\n0.101\n0.570\n0.720\n0.373\n0.166\n\n0.872\n0.556\n0.099\n0.670\n1.193\n0.600\n0.300\n\n0.657\n0.423\n0.087\n0.593\n0.964\n0.494\n0.220\n\n0.565\n0.449\n0.087\n0.670\n0.799\n0.443\n0.190\n\n0.588\n0.491\n0.099\n0.571\n0.777\n0.435\n0.193\n\n0.514\n0.446\n0.095\n0.535\n0.682\n0.364\n0.168\n\n0.569\n0.462\n0.094\n0.550\n0.787\n0.426\n0.179\n\n0.495\n0.448\n0.085\n0.519\n0.674\n0.368\n0.154\n\nG\nI\nE\n\n0.052\n0.039\n0.065\n\n0.049\n0.039\n0.059\n\n0.049\n0.041\n0.057\n\n0.046\n0.039\n0.053\n\n0.045\n0.038\n0.052\n\n0.046\n0.038\n0.054\n\n0.044\n0.038\n0.050\n\n0.055\n0.039\n0.069\n\n0.048\n0.039\n0.057\n\n0.049\n0.041\n0.055\n\n0.047\n0.038\n0.056\n\n0.046\n0.039\n0.053\n\n0.047\n0.038\n0.055\n\n0.046\n0.038\n0.053\n\nv1\nv2\nv3\nh1\nh2\nh3\n\n0.053\n0.052\n0.180\n0.097\n0.044\n0.048\n\n0.050\n0.049\n0.150\n0.085\n0.043\n0.046\n\n0.045\n0.049\n0.122\n0.079\n0.044\n0.046\n\n0.042\n0.047\n0.138\n0.073\n0.040\n0.041\n\n0.040\n0.051\n0.121\n0.070\n0.038\n0.039\n\n0.042\n0.048\n0.137\n0.076\n0.038\n0.042\n\n0.039\n0.048\n0.112\n0.068\n0.040\n0.040\n\n0.057\n0.051\n0.191\n0.106\n0.046\n0.053\n\n0.044\n0.049\n0.148\n0.086\n0.045\n0.042\n\n0.042\n0.050\n0.124\n0.081\n0.045\n0.041\n\n0.046\n0.051\n0.135\n0.080\n0.038\n0.044\n\n0.044\n0.048\n0.119\n0.075\n0.043\n0.041\n\n0.043\n0.049\n0.134\n0.074\n0.042\n0.043\n\n0.044\n0.048\n0.120\n0.074\n0.041\n0.042\n\nTable C.2: Mean squared error for the lifting estimations for regular and clustered\npoints when γ = 0.5, 0.75. The rows denote the different spatial parts of the region,\nand the columns give the results for different weight methods. If the parameter γ <\n1 it indicates the incremental magnitudes of inhibition or repulsion, and clustering\nif γ > 1. Results are given for Dopper, Heavisine, Blocks, Bumps and Maartenfunc\ntest functions from top to bottom panel respectively. MSE calculated globally is\ndenoted as (G), interior (I), edge (E) of the region, and the vertical (v), horizontal\n(h), and diagonal (D) transects.\n172\n\nγ=1\n\nγ = 1.25\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nG\nI\nE\n\n0.046\n0.040\n0.052\n\n0.043\n0.040\n0.047\n\n0.045\n0.042\n0.048\n\n0.043\n0.038\n0.048\n\n0.043\n0.040\n0.045\n\n0.043\n0.038\n0.047\n\n0.043\n0.040\n0.047\n\n0.044\n0.037\n0.051\n\n0.044\n0.040\n0.047\n\n0.045\n0.042\n0.048\n\n0.043\n0.038\n0.047\n\n0.043\n0.039\n0.046\n\n0.042\n0.038\n0.047\n\n0.043\n0.039\n0.046\n\nv1\nv2\nh1\nh2\nD\n\n0.110\n0.016\n0.113\n0.015\n0.069\n\n0.088\n0.013\n0.096\n0.013\n0.062\n\n0.088\n0.012\n0.094\n0.012\n0.064\n\n0.085\n0.013\n0.090\n0.013\n0.065\n\n0.081\n0.012\n0.080\n0.012\n0.059\n\n0.084\n0.013\n0.086\n0.012\n0.062\n\n0.085\n0.012\n0.088\n0.013\n0.064\n\n0.111\n0.015\n0.106\n0.016\n0.064\n\n0.091\n0.014\n0.093\n0.013\n0.052\n\n0.088\n0.013\n0.090\n0.012\n0.054\n\n0.081\n0.013\n0.090\n0.013\n0.065\n\n0.081\n0.013\n0.088\n0.012\n0.066\n\n0.083\n0.013\n0.088\n0.013\n0.064\n\n0.083\n0.013\n0.085\n0.013\n0.065\n\nG\nI\nE\n\n0.346\n0.311\n0.379\n\n0.311\n0.314\n0.308\n\n0.312\n0.322\n0.302\n\n0.304\n0.308\n0.301\n\n0.296\n0.306\n0.285\n\n0.295\n0.302\n0.288\n\n0.292\n0.304\n0.280\n\n0.311\n0.281\n0.341\n\n0.285\n0.288\n0.283\n\n0.289\n0.297\n0.281\n\n0.274\n0.282\n0.266\n\n0.269\n0.283\n0.255\n\n0.265\n0.269\n0.260\n\n0.260\n0.273\n0.248\n\nv1\nv2\nh1\nh2\nD\n\n0.451\n0.683\n0.426\n0.682\n0.377\n\n0.356\n0.482\n0.367\n0.483\n0.307\n\n0.346\n0.425\n0.358\n0.420\n0.307\n\n0.330\n0.428\n0.333\n0.436\n0.307\n\n0.305\n0.373\n0.315\n0.370\n0.283\n\n0.317\n0.406\n0.317\n0.404\n0.297\n\n0.298\n0.362\n0.304\n0.366\n0.281\n\n0.401\n0.606\n0.382\n0.628\n0.357\n\n0.331\n0.432\n0.319\n0.439\n0.267\n\n0.328\n0.378\n0.318\n0.387\n0.269\n\n0.286\n0.393\n0.302\n0.377\n0.270\n\n0.265\n0.321\n0.272\n0.345\n0.280\n\n0.294\n0.350\n0.289\n0.379\n0.273\n\n0.264\n0.315\n0.276\n0.321\n0.274\n\nG\nI\nE\n\n0.535\n0.769\n0.311\n\n0.506\n0.746\n0.276\n\n0.509\n0.753\n0.275\n\n0.533\n0.801\n0.276\n\n0.538\n0.813\n0.274\n\n0.527\n0.791\n0.274\n\n0.523\n0.791\n0.266\n\n0.523\n0.739\n0.312\n\n0.489\n0.718\n0.265\n\n0.492\n0.728\n0.261\n\n0.506\n0.750\n0.268\n\n0.507\n0.752\n0.268\n\n0.510\n0.753\n0.272\n\n0.509\n0.755\n0.269\n\nv1\nv2\nv3\nv4\nh1\nh2\nh3\n\n0.594\n0.771\n0.390\n0.313\n0.284\n0.511\n0.132\n\n0.468\n0.774\n0.377\n0.277\n0.197\n0.507\n0.124\n\n0.415\n0.759\n0.388\n0.262\n0.193\n0.498\n0.121\n\n0.414\n0.787\n0.405\n0.261\n0.203\n0.547\n0.115\n\n0.382\n0.782\n0.396\n0.252\n0.220\n0.548\n0.107\n\n0.446\n0.746\n0.399\n0.246\n0.209\n0.543\n0.108\n\n0.378\n0.785\n0.382\n0.240\n0.204\n0.532\n0.105\n\n0.574\n0.762\n0.379\n0.297\n0.286\n0.566\n0.125\n\n0.435\n0.768\n0.367\n0.251\n0.193\n0.518\n0.108\n\n0.374\n0.756\n0.373\n0.241\n0.177\n0.518\n0.107\n\n0.436\n0.783\n0.367\n0.252\n0.218\n0.534\n0.105\n\n0.375\n0.789\n0.404\n0.239\n0.202\n0.551\n0.110\n\n0.430\n0.735\n0.401\n0.244\n0.217\n0.540\n0.110\n\n0.381\n0.771\n0.378\n0.248\n0.214\n0.539\n0.105\n\nG\nI\nE\n\n0.415\n0.295\n0.531\n\n0.370\n0.294\n0.443\n\n0.367\n0.301\n0.431\n\n0.351\n0.293\n0.408\n\n0.347\n0.303\n0.390\n\n0.345\n0.288\n0.399\n\n0.346\n0.301\n0.389\n\n0.406\n0.284\n0.525\n\n0.361\n0.279\n0.442\n\n0.362\n0.288\n0.434\n\n0.351\n0.286\n0.415\n\n0.342\n0.293\n0.389\n\n0.343\n0.278\n0.406\n\n0.339\n0.286\n0.391\n\nv1\nv2\nv3\nv4\nv5\nh1\nh2\n\n0.870\n0.532\n0.102\n0.740\n1.174\n0.637\n0.322\n\n0.644\n0.448\n0.098\n0.618\n0.959\n0.501\n0.243\n\n0.563\n0.464\n0.098\n0.675\n0.802\n0.456\n0.200\n\n0.559\n0.439\n0.101\n0.581\n0.784\n0.416\n0.198\n\n0.499\n0.431\n0.106\n0.601\n0.666\n0.395\n0.176\n\n0.548\n0.427\n0.094\n0.584\n0.791\n0.394\n0.203\n\n0.521\n0.443\n0.106\n0.577\n0.674\n0.375\n0.169\n\n0.896\n0.532\n0.101\n0.708\n1.160\n0.588\n0.350\n\n0.670\n0.460\n0.098\n0.578\n0.967\n0.515\n0.232\n\n0.603\n0.491\n0.096\n0.643\n0.799\n0.468\n0.201\n\n0.545\n0.463\n0.102\n0.617\n0.797\n0.436\n0.214\n\n0.490\n0.457\n0.101\n0.578\n0.649\n0.408\n0.193\n\n0.554\n0.455\n0.096\n0.591\n0.790\n0.408\n0.206\n\n0.520\n0.458\n0.103\n0.583\n0.672\n0.392\n0.189\n\nG\nI\nE\n\n0.054\n0.039\n0.068\n\n0.050\n0.040\n0.058\n\n0.050\n0.043\n0.057\n\n0.049\n0.039\n0.059\n\n0.048\n0.039\n0.055\n\n0.049\n0.040\n0.057\n\n0.047\n0.039\n0.054\n\n0.055\n0.041\n0.070\n\n0.050\n0.040\n0.059\n\n0.050\n0.043\n0.058\n\n0.047\n0.039\n0.054\n\n0.046\n0.039\n0.053\n\n0.047\n0.040\n0.055\n\n0.046\n0.039\n0.053\n\nv1\nv2\nv3\nh1\nh2\nh3\n\n0.062\n0.056\n0.185\n0.104\n0.043\n0.051\n\n0.046\n0.051\n0.155\n0.085\n0.047\n0.043\n\n0.042\n0.052\n0.131\n0.081\n0.047\n0.043\n\n0.049\n0.053\n0.146\n0.079\n0.045\n0.047\n\n0.043\n0.053\n0.127\n0.075\n0.041\n0.045\n\n0.046\n0.052\n0.141\n0.080\n0.041\n0.046\n\n0.043\n0.052\n0.120\n0.074\n0.041\n0.045\n\n0.062\n0.052\n0.192\n0.107\n0.045\n0.050\n\n0.049\n0.054\n0.158\n0.084\n0.044\n0.043\n\n0.046\n0.054\n0.134\n0.079\n0.044\n0.042\n\n0.046\n0.049\n0.136\n0.074\n0.043\n0.042\n\n0.043\n0.051\n0.126\n0.075\n0.042\n0.040\n\n0.046\n0.048\n0.137\n0.074\n0.043\n0.043\n\n0.044\n0.054\n0.119\n0.072\n0.041\n0.040\n\nTable C.3: Mean squared error for the lifting estimations for regular and clustered\npoints when γ = 1, 1.25. The rows denote the different spatial parts of the region,\nand the columns give the results for different weight methods. If the parameter γ <\n1 it indicates the incremental magnitudes of inhibition or repulsion, and clustering\nif γ > 1. Results are given for Dopper, Heavisine, Blocks, Bumps and Maartenfunc\ntest functions from top to bottom panel respectively. MSE calculated globally is\ndenoted as (G), interior (I), edge (E) of the region, and the vertical (v), horizontal\n(h), and diagonal (D) transects.\n173\n\nγ = 1.5\n\nγ=2\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nG\nI\nE\n\n0.045\n0.038\n0.051\n\n0.043\n0.040\n0.047\n\n0.044\n0.041\n0.047\n\n0.043\n0.039\n0.048\n\n0.044\n0.041\n0.047\n\n0.042\n0.038\n0.046\n\n0.043\n0.039\n0.046\n\n0.045\n0.038\n0.052\n\n0.043\n0.038\n0.048\n\n0.042\n0.040\n0.045\n\n0.046\n0.041\n0.050\n\n0.046\n0.043\n0.049\n\n0.042\n0.038\n0.046\n\n0.042\n0.038\n0.046\n\nv1\nv2\nh1\nh2\nD\n\n0.105\n0.015\n0.108\n0.015\n0.063\n\n0.088\n0.012\n0.097\n0.013\n0.065\n\n0.088\n0.013\n0.090\n0.012\n0.064\n\n0.088\n0.012\n0.093\n0.012\n0.063\n\n0.081\n0.012\n0.088\n0.013\n0.065\n\n0.078\n0.012\n0.089\n0.013\n0.055\n\n0.082\n0.012\n0.090\n0.013\n0.062\n\n0.097\n0.016\n0.107\n0.016\n0.074\n\n0.091\n0.014\n0.096\n0.013\n0.061\n\n0.089\n0.013\n0.091\n0.012\n0.061\n\n0.092\n0.014\n0.094\n0.014\n0.068\n\n0.087\n0.014\n0.088\n0.013\n0.070\n\n0.073\n0.013\n0.088\n0.013\n0.068\n\n0.076\n0.012\n0.088\n0.013\n0.061\n\nG\nI\nE\n\n0.298\n0.267\n0.329\n\n0.263\n0.270\n0.257\n\n0.269\n0.278\n0.261\n\n0.252\n0.258\n0.247\n\n0.251\n0.264\n0.238\n\n0.247\n0.252\n0.242\n\n0.249\n0.261\n0.237\n\n0.281\n0.249\n0.312\n\n0.244\n0.244\n0.243\n\n0.249\n0.257\n0.241\n\n0.265\n0.268\n0.262\n\n0.267\n0.270\n0.263\n\n0.233\n0.238\n0.228\n\n0.229\n0.244\n0.214\n\nv1\nv2\nh1\nh2\nD\n\n0.397\n0.606\n0.382\n0.601\n0.320\n\n0.315\n0.405\n0.292\n0.408\n0.259\n\n0.309\n0.368\n0.297\n0.357\n0.264\n\n0.280\n0.354\n0.254\n0.362\n0.252\n\n0.262\n0.306\n0.254\n0.313\n0.239\n\n0.261\n0.333\n0.265\n0.350\n0.247\n\n0.257\n0.294\n0.255\n0.300\n0.241\n\n0.380\n0.558\n0.364\n0.584\n0.311\n\n0.283\n0.385\n0.276\n0.404\n0.228\n\n0.285\n0.343\n0.269\n0.336\n0.240\n\n0.277\n0.351\n0.280\n0.360\n0.265\n\n0.270\n0.327\n0.272\n0.348\n0.256\n\n0.256\n0.318\n0.253\n0.326\n0.249\n\n0.223\n0.268\n0.223\n0.269\n0.242\n\nG\nI\nE\n\n0.530\n0.744\n0.319\n\n0.519\n0.761\n0.279\n\n0.495\n0.730\n0.263\n\n0.514\n0.761\n0.270\n\n0.517\n0.768\n0.269\n\n0.503\n0.738\n0.271\n\n0.509\n0.751\n0.270\n\n0.508\n0.703\n0.313\n\n0.498\n0.731\n0.264\n\n0.494\n0.725\n0.260\n\n0.494\n0.715\n0.273\n\n0.496\n0.726\n0.266\n\n0.491\n0.719\n0.263\n\n0.494\n0.722\n0.267\n\nv1\nv2\nv3\nv4\nh1\nh2\nh3\n\n0.592\n0.734\n0.379\n0.300\n0.300\n0.496\n0.123\n\n0.456\n0.746\n0.367\n0.262\n0.224\n0.503\n0.112\n\n0.393\n0.704\n0.341\n0.223\n0.185\n0.506\n0.118\n\n0.430\n0.747\n0.368\n0.231\n0.210\n0.522\n0.111\n\n0.387\n0.761\n0.384\n0.248\n0.210\n0.532\n0.109\n\n0.413\n0.742\n0.370\n0.246\n0.236\n0.479\n0.104\n\n0.398\n0.744\n0.364\n0.235\n0.233\n0.476\n0.107\n\n0.555\n0.761\n0.381\n0.288\n0.285\n0.489\n0.121\n\n0.444\n0.639\n0.360\n0.231\n0.209\n0.466\n0.102\n\n0.382\n0.659\n0.369\n0.229\n0.185\n0.518\n0.101\n\n0.407\n0.716\n0.373\n0.236\n0.224\n0.490\n0.115\n\n0.358\n0.709\n0.375\n0.230\n0.215\n0.481\n0.114\n\n0.388\n0.752\n0.344\n0.234\n0.210\n0.459\n0.109\n\n0.374\n0.776\n0.360\n0.223\n0.212\n0.457\n0.104\n\nG\nI\nE\n\n0.408\n0.283\n0.533\n\n0.366\n0.290\n0.441\n\n0.375\n0.311\n0.437\n\n0.354\n0.294\n0.414\n\n0.340\n0.290\n0.389\n\n0.350\n0.284\n0.416\n\n0.337\n0.285\n0.389\n\n0.400\n0.275\n0.524\n\n0.362\n0.299\n0.425\n\n0.360\n0.301\n0.419\n\n0.367\n0.305\n0.429\n\n0.370\n0.312\n0.427\n\n0.338\n0.280\n0.396\n\n0.334\n0.288\n0.380\n\nv1\nv2\nv3\nv4\nv5\nh1\nh2\n\n0.909\n0.539\n0.096\n0.713\n1.192\n0.603\n0.322\n\n0.675\n0.475\n0.097\n0.552\n0.992\n0.499\n0.212\n\n0.667\n0.470\n0.099\n0.637\n0.810\n0.486\n0.198\n\n0.615\n0.489\n0.102\n0.544\n0.801\n0.398\n0.206\n\n0.530\n0.473\n0.105\n0.545\n0.682\n0.382\n0.179\n\n0.546\n0.454\n0.104\n0.608\n0.849\n0.414\n0.223\n\n0.542\n0.459\n0.099\n0.567\n0.690\n0.365\n0.178\n\n0.931\n0.523\n0.109\n0.700\n1.243\n0.581\n0.333\n\n0.706\n0.439\n0.109\n0.556\n0.936\n0.545\n0.256\n\n0.596\n0.473\n0.103\n0.621\n0.819\n0.469\n0.218\n\n0.612\n0.480\n0.110\n0.551\n0.819\n0.444\n0.214\n\n0.592\n0.498\n0.124\n0.575\n0.750\n0.463\n0.212\n\n0.544\n0.427\n0.114\n0.543\n0.760\n0.399\n0.204\n\n0.532\n0.419\n0.106\n0.550\n0.653\n0.362\n0.184\n\nG\nI\nE\n\n0.056\n0.041\n0.071\n\n0.049\n0.039\n0.059\n\n0.051\n0.045\n0.057\n\n0.048\n0.040\n0.056\n\n0.047\n0.040\n0.054\n\n0.048\n0.041\n0.054\n\n0.047\n0.040\n0.054\n\n0.058\n0.042\n0.074\n\n0.050\n0.040\n0.059\n\n0.052\n0.045\n0.058\n\n0.051\n0.043\n0.059\n\n0.050\n0.043\n0.058\n\n0.048\n0.041\n0.055\n\n0.047\n0.040\n0.054\n\nv1\nv2\nv3\nh1\nh2\nh3\n\n0.064\n0.055\n0.195\n0.111\n0.047\n0.049\n\n0.047\n0.049\n0.158\n0.084\n0.043\n0.043\n\n0.045\n0.050\n0.127\n0.076\n0.050\n0.045\n\n0.045\n0.051\n0.140\n0.078\n0.043\n0.040\n\n0.042\n0.054\n0.133\n0.071\n0.043\n0.042\n\n0.047\n0.054\n0.133\n0.073\n0.047\n0.040\n\n0.045\n0.051\n0.128\n0.076\n0.046\n0.041\n\n0.067\n0.054\n0.210\n0.120\n0.047\n0.048\n\n0.050\n0.056\n0.164\n0.084\n0.046\n0.047\n\n0.048\n0.053\n0.139\n0.081\n0.050\n0.045\n\n0.052\n0.058\n0.157\n0.094\n0.047\n0.047\n\n0.048\n0.056\n0.150\n0.087\n0.045\n0.046\n\n0.045\n0.054\n0.145\n0.081\n0.044\n0.039\n\n0.044\n0.052\n0.128\n0.079\n0.041\n0.042\n\nTable C.4: Mean squared error for the lifting estimations for regular and clustered\npoints when γ = 1.5, 2. The rows denote the different spatial parts of the region, and\nthe columns give the results for different weight methods. If the parameter γ < 1\nit indicates the incremental magnitudes of inhibition or repulsion, and clustering if\nγ > 1. Results are given for Dopper, Heavisine, Blocks, Bumps and Maartenfunc\ntest functions from top to bottom panel respectively. MSE calculated globally is\ndenoted as (G), interior (I), edge (E) of the region, and the vertical (v), horizontal\n(h), and diagonal (D) transects.\n174\n\nγ=3\nConv.\n\nUnit\n\nDoub.\n\nB0\n\nA0\n\nB?\n\nA?\n\nG\nI\nE\n\n0.045\n0.037\n0.052\n\n0.042\n0.037\n0.047\n\n0.043\n0.039\n0.048\n\n0.046\n0.041\n0.051\n\n0.046\n0.042\n0.051\n\n0.042\n0.038\n0.047\n\n0.042\n0.037\n0.046\n\nv1\nv2\nh1\nh2\nD\n\n0.102\n0.016\n0.115\n0.016\n0.069\n\n0.087\n0.014\n0.090\n0.014\n0.059\n\n0.088\n0.013\n0.096\n0.012\n0.063\n\n0.092\n0.014\n0.106\n0.013\n0.075\n\n0.092\n0.013\n0.106\n0.014\n0.073\n\n0.091\n0.014\n0.091\n0.013\n0.058\n\n0.086\n0.013\n0.086\n0.013\n0.057\n\nG\nI\nE\n\n0.267\n0.237\n0.298\n\n0.217\n0.220\n0.213\n\n0.222\n0.230\n0.214\n\n0.260\n0.264\n0.256\n\n0.262\n0.267\n0.256\n\n0.223\n0.227\n0.218\n\n0.228\n0.237\n0.218\n\nv1\nv2\nh1\nh2\nD\n\n0.351\n0.548\n0.364\n0.565\n0.288\n\n0.234\n0.348\n0.255\n0.349\n0.212\n\n0.236\n0.320\n0.256\n0.316\n0.210\n\n0.261\n0.335\n0.277\n0.358\n0.254\n\n0.270\n0.317\n0.297\n0.350\n0.247\n\n0.239\n0.297\n0.251\n0.314\n0.220\n\n0.228\n0.263\n0.243\n0.277\n0.234\n\nG\nI\nE\n\n0.505\n0.699\n0.308\n\n0.509\n0.740\n0.270\n\n0.492\n0.709\n0.269\n\n0.495\n0.711\n0.277\n\n0.501\n0.722\n0.277\n\n0.491\n0.718\n0.263\n\n0.503\n0.741\n0.262\n\nv1\nv2\nv3\nv4\nh1\nh2\nh3\n\n0.544\n0.632\n0.379\n0.285\n0.264\n0.459\n0.118\n\n0.426\n0.752\n0.395\n0.238\n0.221\n0.491\n0.110\n\n0.394\n0.718\n0.380\n0.221\n0.211\n0.492\n0.116\n\n0.396\n0.689\n0.390\n0.267\n0.221\n0.505\n0.107\n\n0.380\n0.689\n0.405\n0.252\n0.219\n0.496\n0.111\n\n0.391\n0.653\n0.346\n0.244\n0.212\n0.444\n0.097\n\n0.378\n0.682\n0.372\n0.200\n0.191\n0.452\n0.098\n\nG\nI\nE\n\n0.408\n0.287\n0.530\n\n0.345\n0.278\n0.415\n\n0.360\n0.304\n0.418\n\n0.368\n0.300\n0.436\n\n0.370\n0.308\n0.432\n\n0.339\n0.279\n0.399\n\n0.331\n0.282\n0.381\n\nv1\nv2\nv3\nv4\nv5\nh1\nh2\n\n0.984\n0.528\n0.106\n0.752\n1.166\n0.560\n0.382\n\n0.624\n0.450\n0.096\n0.525\n0.955\n0.486\n0.257\n\n0.591\n0.456\n0.104\n0.570\n0.907\n0.454\n0.230\n\n0.689\n0.475\n0.101\n0.617\n0.854\n0.485\n0.279\n\n0.632\n0.505\n0.107\n0.625\n0.805\n0.462\n0.265\n\n0.578\n0.448\n0.104\n0.580\n0.763\n0.444\n0.196\n\n0.509\n0.425\n0.105\n0.572\n0.685\n0.404\n0.180\n\nG\nI\nE\n\n0.059\n0.042\n0.076\n\n0.050\n0.041\n0.059\n\n0.053\n0.046\n0.059\n\n0.053\n0.044\n0.062\n\n0.052\n0.044\n0.061\n\n0.048\n0.040\n0.056\n\n0.047\n0.040\n0.055\n\nv1\nv2\nv3\nh1\nh2\nh3\n\n0.064\n0.059\n0.207\n0.123\n0.052\n0.055\n\n0.050\n0.053\n0.168\n0.079\n0.045\n0.049\n\n0.046\n0.054\n0.152\n0.080\n0.047\n0.052\n\n0.060\n0.053\n0.169\n0.090\n0.050\n0.052\n\n0.056\n0.053\n0.157\n0.087\n0.049\n0.053\n\n0.046\n0.050\n0.146\n0.078\n0.045\n0.045\n\n0.045\n0.050\n0.127\n0.076\n0.045\n0.040\n\nTable C.5: Mean squared error for the lifting estimations for regular and clustered\npoints when γ = 3. The rows denote the different spatial parts of the region, and\nthe columns give the results for different weight methods. If the parameter γ < 1\nit indicates the incremental magnitudes of inhibition or repulsion, and clustering if\nγ > 1. Results are given for Dopper, Heavisine, Blocks, Bumps and Maartenfunc\ntest functions from top to bottom panel respectively. MSE calculated globally is\ndenoted as (G), interior (I), edge (E) of the region, and the vertical (v), horizontal\n(h), and diagonal (D) transects.\n175\n\nReferences\nAkaike, H. (1987). Factor analysis and AIC. In Selected Papers of Hirotugu\nAkaike, 371–386, Springer. 53\nAntoniadis, A., Bigot, J. & Sapatinas, T. (2001). Wavelet estimators in\nnonparametric regression: a comparative simulation study. Journal of Statistical\nSoftware, 6, 1–83. 116\nArvanitakis, G. (2014). Distribution of the number of Poisson points in Poisson\nVoronoi tessellation. Tech. Rep. RR-15-304 . 17\nBaccelli, F. & Blaszczyszyn, B. (2001). On a coverage process ranging from\nthe Boolean model to the Poisson–Voronoi tessellation with applications to wireless communications. Advances in Applied Probability, 33, 293–323. 14\nBaddeley, A., Rubak, E. & Turner, R. (2015). Spatial point patterns:\nmethodology and applications with R. CRC press, New York. 85, 86\nBesag, J. (1977). Discussion on Dr Ripley’s Paper. Journal of the Royal Statistical\nSociety: Series B (Methodological), 39, 192–212. 94\nBivand, R. & Rundel, C. (2020). rgeos: Interface to Geometry Engine - Open\nSource (‘GEOS’). R package version 0.5-5. 111\nBrakke, K.A. (1987). Statistics of random plane Voronoi tessellations. Department of Mathematical Sciences, Susquehanna University (Manuscript 1987a). 18\nCai, T.T. (1999). Adaptive wavelet estimation: a block thresholding and oracle\ninequality approach. Annals of Statistics, 27, 898–924. 115\nCai, T.T. (2002). On block thresholding in wavelet regression: Adaptivity, block\nsize, and threshold level. Statistica Sinica, 2, 1241–1273. 115\n\n176\n\nREFERENCES\n\nCai, T.T. & Silverman, B.W. (2001). Incorporating information on neighbouring coefficients into wavelet estimation. Sankhyā: The Indian Journal of Statistics, Series B , 63, 127–148. 115\nChristensen, R. (1991). Linear models for multivariate, time series, and spatial\ndata, vol. 1. Springer, New York. 147\nClaire, M. & Neocleous, T. (2019). Flexible regression lecture notes. Academy\nfor PhD training in Statistics, Oxford University. 52\nClaypoole, R.L., Baraniuk, R.G. & Nowak, R.D. (1998). Adaptive wavelet\ntransforms via lifting. In Proceedings of the 1998 IEEE International Conference\non Acoustics, Speech and Signal Processing, ICASSP’98 (Cat. No. 98CH36181),\nvol. 3, 1513–1516, IEEE. 102\nCox, D.R. & Isham, V. (1980). Point processes, vol. 12. CRC Press, Boca Raton.\n85\nCrain, I.K. (1978). The Monte-Carlo generation of random polygons. Computers\n& Geosciences, 4, 131–141. 15\nCressie, N. (2015). Statistics for Spatial Data. John Wiley & Sons, New York. 85,\n147\nCressie, N. & Wikle, C.K. (2015). Statistics for spatio-temporal data. John\nWiley & Sons. 85\nDalling, J., John, R., Harms, K., Stallard, R. & Yavitt, J. (2021).\nProject: Effects of soil-borne resources on the structure and dynamics of lowland tropical forests. Funding: NSF DEB021104,021115, 0212284,0212818, OISE\n0314581, STRI Soils Initiative and CTFS. Note: Thanks to Paolo Segre and Juan\nDi Trani for assistance in the field. 92\nDaubechies, I. (1992). Ten lectures on wavelets. SIAM, Philadelphia. 100\nDiggle, P. (1983). Statistical analysis of spatial point patterns. Mathematics in\nbiology, Academic Press, London. 85\nDiggle, P. (1985). A kernel method for smoothing point process data. Journal of\nthe Royal Statistical Society: Series C (Applied Statistics), 34, 138–147. 87\n\n177\n\nREFERENCES\n\nDonoho, D.L. & Johnstone, I.M. (1995). Adapting to unknown smoothness\nvia wavelet shrinkage. Journal of The American Statistical Association, 90, 1200–\n1224. 115\nDonoho, D.L. & Johnstone, J.M. (1994). Ideal spatial adaptation by wavelet\nshrinkage. Biometrika, 81, 425–455. 10, 112, 113, 114, 122\nDonoho, D.L., Johnstone, I.M., Kerkyacharian, G. & Picard, D. (1995).\nWavelet shrinkage: asymptopia? Journal of the Royal Statistical Society: Series\nB (Methodological), 57, 301–337. 10, 112, 113\nFinney, J. (1970). Random packings and the structure of simple liquids. I. The\ngeometry of random close packing. Proceedings of the Royal Society of London.\nSeries A, Mathematical and Physical Sciences, 479–493. 14\nFischer, R. & Miles, R. (1973). The role of spatial pattern in the competition\nbetween crop plants and weeds. A theoretical analysis. Mathematical Biosciences,\n18, 335–350. 14\nGarrett, R.C., Nar, A. & Fisher, T.J. (2021). ggvoronoi: Voronoi Diagrams\nand Heatmaps with ‘ggplot2’ . R package version 0.8.4. 116\nGebhardt, A., Eglen, S., Zuyev, S. & White, D. (2020). tripack: Triangulation of Irregularly Spaced Data. R package version 1.3-9.1. 111\nGelfand, A.E., Diggle, P., Guttorp, P. & Fuentes, M. (2010). Handbook\nof spatial statistics. CRC press. 85\nGeyer, C. (1999). Likelihood inference for spatial point processes. In Stochastic\nGeometry, 79–140, Routledge. 6, 85, 87, 137, 154\nGezer, F., Aykroyd, R.G. & Barber, S. (2021). Statistical properties of\nPoisson-Voronoi tessellation cells in bounded regions. Journal of Statistical Computation and Simulation, 91, 915–933. 13, 44\nGilbert, E. (1962). Random subdivisions of space into crystals. The Annals of\nMathematical Statistics, 33, 958–972. 15\nGoldstein, J., Haran, M., Simeonov, I., Fricks, J. & Chiaromonte,\nF. (2015). An attraction–repulsion point process model for respiratory syncytial\nvirus infections. Biometrics, 71, 376–385. 6\n\n178\n\nREFERENCES\n\nGräler, B., Pebesma, E. & Heuvelink, G. (2016). Spatio-temporal interpolation using gstat. The R Journal , 8, 204–218. 147\nHall, P., Kerkyacharian, G. & Picard, D. (1999). On the minimax optimality of block thresholded wavelet estimators. Statistica Sinica, 9, 33–49. 115\nHastie, T. (2020). gam: Generalized Additive Models. R package version 1.20. 53\nHastie, T.J. & Tibshirani, R.J. (1990). Generalized additive models. CRC\npress, Boca Raton. 50, 52\nHeaton, T. & Silverman, B. (2008). A wavelet-or lifting-scheme-based imputation method. Journal of the Royal Statistical Society: Series B (Statistical\nMethodology), 70, 567–587. 102, 152, 159\nHinde, A. & Miles, R. (1980). Monte Carlo estimates of the distributions of the\nrandom polygons of the Voronoi tessellation with respect to a Poisson process.\nJournal of Statistical Computation and Simulation, 10, 205–223. 16, 29, 32\nHo, T.K. (1995). Random decision forests. In Proceedings of 3rd International\nConference on Document Analysis and Recognition, vol. 1, 278–282, IEEE. 56\nHofner, B., Mayr, A., Robinzonov, N. & Schmid, M. (2014). Model-based\nboosting in R: A hands-on tutorial using the R package mboost. Computational\nstatistics, 29, 3–35. 56\nIcke, V. & Weygaert, R. (1987). Fragmenting the universe. Astronomy and\nAstrophysics, 184, 16. 14\nIllian, J., Penttinen, A., Stoyan, H. & Stoyan, D. (2008). Statistical analysis and modelling of spatial point patterns, vol. 70. John Wiley & Sons. 5, 12,\n14, 85\nJames, G., Witten, D., Hastie, T. & Tibshirani, R. (2013). An Introduction\nto Statistical Learning, vol. 112. Springer, New York. 52\nJansen, M. & Bultheel, A. (1999). Smooting irregularly sampled signals using\nwavelets and cross validation. Technical Report TW289 . 116\nJansen, M., Nason, G.P. & Silverman, B.W. (2001). Scattered data smoothing by empirical Bayesian shrinkage of second-generation wavelet coefficients. In\nWavelets: Applications in Signal and Image Processing IX , vol. 4478, 87–98,\nInternational Society for Optics and Photonics. 102\n\n179\n\nREFERENCES\n\nJansen, M., Nason, G.P. & Silverman, B.W. (2009). Multiscale methods for\ndata on graphs and irregular multidimensional situations. Journal of the Royal\nStatistical Society: Series B (Statistical Methodology), 71, 97–125. 1, 7, 10, 12,\n102, 106, 107, 122, 157\nJansen, M.H. & Oonincx, P.J. (2005). Second generation wavelets and applications. Springer Science & Business Media, London. 101\nJohnstone, I.M. & Silverman, B.W. (2004). Needles and straw in haystacks:\nEmpirical Bayes estimates of possibly sparse sequences. The Annals of Statistics,\n32, 1594–1649. 114\nJohnstone, I.M. & Silverman, B.W. (2005a). Empirical bayes selection of\nwavelet thresholds. The Annals of Statistics, 33, 1700–1752. 114\nJohnstone, I.M. & Silverman, B.W. (2005b). Ebayesthresh: R programs for\nempirical Bayes thresholding. Journal of Statistical Software, 12. 115\nKiang, T. (1966). Random fragmentation in two and three dimensions. Zeitschrift\nfur Astrophysik , 64, 433. 15, 16, 31\nKiskowski, M.A., Hancock, J.F. & Kenworthy, A.K. (2009). On the use\nof Ripley’s K-function and its derivatives to analyze domain size. Biophysical\nJournal , 97, 1095–1103. 93\nKnight, M. & Nunes, M. (2018). nlt: A Nondecimated Lifting Transform for\nSignal Denoising. R package version 2.2-1. 111\nKnight, M.I. & Nason, G.P. (2009). A ‘nondecimated’ lifting transform. Statistics and Computing, 19, 1–16. 102\nKoenker, R. & Machado, J.A. (1999). Goodness of fit and related inference\nprocesses for quantile regression. Journal of the American Statistical Association,\n94, 1296–1310. 34\nKoufos, K. & Dettmann, C.P. (2019). Distribution of cell area in bounded\nPoisson Voronoi tessellations with application to secure local connectivity. Journal of Statistical Physics, 176, 1–20. xii, 14, 18, 19, 31\nKumar, S. & Kurtz, S.K. (1993). Properties of a two-dimensional PoissonVoronoi tesselation: a Monte-Carlo study. Materials Characterization, 31, 55–68.\n16, 31\n\n180\n\nREFERENCES\n\nKumar, S., Kurtz, S.K., Banavar, J.R. & Sharma, M. (1992). Properties of\na three-dimensional Poisson-Voronoi tesselation: A Monte Carlo study. Journal\nof Statistical Physics, 67, 523–551. 158\nLazar, E.A., Mason, J.K., MacPherson, R.D. & Srolovitz, D.J. (2013).\nStatistical topology of three-dimensional Poisson-Voronoi cells and cell boundary\nnetworks. Physical Review E , 88, 063309. 158\nLiaw, A., Wiener, M. et al. (2002). Classification and regression by randomForest. R news, 2, 18–22. 56\nLuchnikov, V., Medvedev, N., Naberukhin, Y.I. & Schober, H. (2000).\nVoronoi-Delaunay analysis of normal modes in a simple model glass. Physical\nReview B , 62, 3181. 14\nMackay, A. (1972). Stereological characteristics of atomic arrangements in crystals. Journal of Microscopy, 95, 217–227. 14\nMallat, S. (1989). A theory for multiresolution signal decomposition: The wavelet\nrepresentation. IEEE Transactions on Pattern Analysis & Machine Intelligence,\n11, 674–693. 100, 101\nMarra, G. & Wood, S.N. (2011). Practical variable selection for generalized\nadditive models. Computational Statistics & Data Analysis, 55, 2372–2387. 53,\n54\nMeijering, J. (1953). Interface area, edge length, and number of vertices in crystal\naggregates with random nucleation. Philips Res. Rep., 8, 270–290. 15\nMøller, J. (2012). Lectures on random Voronoi tessellations, vol. 87. Springer\nScience & Business Media. 2, 13\nMorlini, I. (2006). On multicollinearity and concurvity in some nonlinear multivariate models. Statistical Methods and Applications, 15, 3–26. 48\nMuche, L. (1996). Distributional properties of the three-dimensional Poisson Delaunay cell. Journal of Statistical Physics, 84, 147–167. 158\nNason, G. (2008). Wavelet methods in statistics with R. Springer Science & Business Media, New York. 101\n\n181\n\nREFERENCES\n\nNason, G., Jansen, M. & Silverman, B. (2004). Simulations and examples for\nmultivariate nonparametric regression using lifting. Technical Report, Department of Mathematics, University of Bristol. 122\nNason, G.P. (1996). Wavelet shrinkage using cross-validation. Journal of the Royal\nStatistical Society: Series B (Methodological), 58, 463–479. 116\nNunes, M. & Knight, M. (2018). adlift: An Adaptive Lifting Scheme Algorithm.\nR package version 1.4-1. 111\nNunes, M.A., Knight, M.I. & Nason, G.P. (2006). Adaptive lifting for nonparametric regression. Statistics and Computing, 16, 143–159. 102\nOkabe, A., Boots, B., Sugihara, K. & Chiu, S.N. (2000). Spatial tessellations: concepts and applications of Voronoi diagrams, vol. 501. John Wiley &\nSons, New York. 2, 13, 14\nPebesma, E.J. (2004). Multivariable geostatistics in S: the gstat package. Computers & geosciences, 30, 683–691. 147\nPeck, S.J. (2010). Multiscale spatial imputation applied to crop infestation modelling. Ph.D. thesis, University of Leeds. 152, 159\nPope, C.A., Gosling, J.P., Barber, S., Johnson, J.S., Yamaguchi, T.,\nFeingold, G. & Blackwell, P.G. (2021). Gaussian process modeling of\nheterogeneity and discontinuities using Voronoi tessellations. Technometrics, 63,\n53–63. 100\nR Core Team (2021). R: A Language and Environment for Statistical Computing.\nR Foundation for Statistical Computing, Vienna, Austria. 111\nRamella, M., Boschin, W., Fadda, D. & Nonino, M. (2001). Finding galaxy\nclusters using Voronoi tessellations. Astronomy & Astrophysics, 368, 776–786. 14\nRipley, B.D. (1976). The second-order analysis of stationary point processes.\nJournal of Applied Probability, 13, 255–266. 93\nRipley, B.D. (1977). Modelling spatial patterns. Journal of the Royal Statistical\nSociety: Series B (Methodological), 39, 172–192. 93\nRipley, B.D. (1988). Statistical inference for spatial processes. Cambridge University Press, Cambridge. 85\n\n182\n\nREFERENCES\n\nRipley, B.D. (2005). Spatial statistics, vol. 575. John Wiley & Sons, New York.\n12, 14, 85\nSchlather, M., Ribeiro Jr, P.J. & Diggle, P.J. (2004). Detecting dependence between marks and locations of marked point processes. Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology), 66, 79–93. 157\nSchoenberg, F.P., Barr, C. & Seo, J. (2009). The distribution of Voronoi\ncells generated by Southern California earthquake epicenters. Environmetrics:\nThe Official Journal of the International Environmetrics Society, 20, 159–171.\n14, 44\nStacy, E.W. (1962). A generalization of the gamma distribution. The Annals of\nMathematical Statistics, 33, 1187–1192. 16\nStacy, E.W. & Mihram, G.A. (1965). Parameter estimation for a generalized\ngamma distribution. Technometrics, 7, 349–358. 16\nStein, C.M. (1981). Estimation of the mean of a multivariate normal distribution.\nThe Annals of Statistics, 9, 1135–1151. 115\nStrauss, D.J. (1975). A model for clustering. Biometrika, 62, 467–475. 6\nSweldens, W. (1995). Lifting scheme: a new philosophy in biorthogonal wavelet\nconstructions. In Wavelet Applications in Signal and Image Processing III , vol.\n2569, 68–79, International Society for Optics and Photonics. 100\nSweldens, W. (1996). The lifting scheme: A custom-design construction of\nbiorthogonal wavelets. Applied and Computational Harmonic Analysis, 3, 186–\n200. 100\nSweldens, W. (1998). The lifting scheme: A construction of second generation\nwavelets. SIAM Journal on Mathematical Analysis, 29, 511–546. 10, 100, 102\nTanemura, M. (2003). Statistical distributions of Poisson Voronoi cells in two\nand three dimensions. Forma, 12, 221–247. xii, 16, 17, 29, 32, 158\nTanemura, M. (2005). Statistical distributions of shape of Poisson Voronoi cells.\nVoronoi’s Impact on Modern Science. Book III. Proceedings of the 3rd Voronoi\nConference on Analytic Number Theory and Spatial Tessellations. 18\n\n183\n\nREFERENCES\n\nTanemura, M. & Hasegawa, M. (1980). Geometrical models of territory I.\nModels for synchronous and asynchronous settlement of territories. Journal of\nTheoretical Biology, 82, 477–496. 14\nTurner, R. (2021). deldir: Delaunay Triangulation and Dirichlet (Voronoi) Tessellation. R package version 0.2-10. 111\nVidakovic, B. (1999). Statistical modeling by wavelets, vol. 503. John Wiley &\nSons, New York. 100\nWeaire, D., Kermode, J. & Wejchert, J. (1986). On the distribution of cell\nareas in a Voronoi network. Philosophical Magazine B , 53, 101–105. 16, 31\nWickham, H. (2016). ggplot2: Elegant Graphics for Data Analysis. SpringerVerlag New York. 21\nWood, S. (2015). Package ‘mgcv’. R package version, 1, 29. 54\nWood, S.N. (2011). Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models. Journal of the\nRoyal Statistical Society: Series B (Statistical Methodology), 73, 3–36. 52\nWood, S.N. (2017). Generalized additive models: an introduction with R. CRC\npress, Boca Raton. 50, 52\nXu, T. & Li, M. (2009). Topological and statistical properties of a constrained\nvoronoi tessellation. Philosophical Magazine, 89, 349–374. 14\nYoshioka, S. & Ikeuchi, S. (1989). The large-scale structure of the universe and\nthe division of space. The Astrophysical Journal , 341, 16–25. 14\nYu, K. & Moyeed, R.A. (2001). Bayesian quantile regression. Statistics & Probability Letters, 54, 437–447. 34\n\n184\n\n","pages":{"startPosition":[0,5000,9999,15001,20001,25000,29994,34999,39995,44996,49999,54998,59997,65001,69999,74997,80001,85000,89998,94986,99993,105001,110000,114997,120000,125000,129993,134997,140001,145000,150000,154998,159999,164998,170000,175000,179998,184993,190000,194998,199998,204992,210001,214987,219998,225000,229992,235001,239997,245000,249994,254995,259992,264992,269992,274998,280001,285000,290001,294994,300000,304998,310001,314998,319992,324992,330000,335000,340000,345000,349992,355000,360000,365000]}},"html":{"comparison":{"identical":{"groupId":[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],"source":{"chars":{"starts":[3794672,3794689,3794697,3794714,3794722,3794739,3794747,3794764,3794772,3794789,3794797,3794814,3794822,3794839,3794847,3794864,3794872,3794889,3794897,3794914,3794922,3794939,3794947,3794964,3794972,3794989,3794997,3839244,3839261,3839269,3839286,3839294,3839311,3839319,3839336,3839344,3839361,3839369,3839386,3839394,3839411,3839419,3839436,3839444,3839461,3839469,3839486,3839494,3839511,3839519,3839536,3839544,3874994,3875011,3875019,3875036,3875044,3875061,3875069,3875086,3875094,3875111,3875119,3875136,3875144,3875161,3875169,3875186,3875194,3875211,3875219,3875236,3875244,3875261,3875269,4027458,4027475,4027483,4027500,4027508,4027525,4027533,4027550,4027558,4027575,4027583,4027600,4027608,4027625,4027633,4027650,4027658,4027675,4027683,4027700,4027708,4027725,4027733,4064367,4064384,4064392,4064409,4064417,4064434,4064442,4064459,4064467,4064484,4064492,4064509,4064517,4064534,4064542,4064559,4064567,4064584,4064592,4064609,4064617,4064634,4064642,4074420,4074437,4074445,4074462,4074470,4074487,4074495,4074512,4074520,4074537,4074545,4074562,4074570,4074587,4074595,4074612,4074620,4074637,4074645,4074662,4074670,4074687,4074695,4074712,4074720,4074737,4074745,4074762,4074770,4074787,4074795,4081529,4081546,4081554,4081571,4081579,4081596,4081604,4081621,4081629,4081646,4081654,4081671,4081679,4081696,4081704,4081721,4081729,4081746,4081754,4081771,4081779,4081796,4081804,4081821,4081829,4081846,4081854,4081871,4081879,4081896,4081904],"lengths":[1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1]},"words":{"starts":[8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,8511,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,9545,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,10298,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,13735,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14573,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,14914,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144,15144],"lengths":[13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15]}},"suspected":{"chars":{"starts":[5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5361,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5751,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,5787,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6015,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6244,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6333,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707,6707],"lengths":[27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,27,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,25,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,23,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31,31]},"words":{"starts":[854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,854,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,982,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1000,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1071,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1144,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1182,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268,1268],"lengths":[13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,13,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,12,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,11,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15,15]}}},"minorChanges":{"groupId":[],"source":{"chars":{"starts":[],"lengths":[]},"words":{"starts":[],"lengths":[]}},"suspected":{"chars":{"starts":[],"lengths":[]},"words":{"starts":[],"lengths":[]}}},"relatedMeaning":{"groupId":[7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8,8],"source":{"chars":{"starts":[3908900,3908927,3908935,3908959,3908967,3909064,3909091,3909099,3909126,3909134,3909166,3909174,3909206,3909214,3909231,3909239,3909256,3909264,3909281,3909289,3909306,3909314,3909331,3909339,3909356,3909364,3909381,3909389,3909406,3909414,3909431,3909439,3909456,3909464,3909481,3909489,3909506,3909514,3909531,3909539,3909556,3909564,3909581,3909589,3909606,3909614,3909631,3909639,3909656,3909664,3909684,3909692,3909757,3909784,3909792,3909819,3909827,3909859,3909867,3909889,3909897,3909923,3909931,3909959,3909967,3909984,3909992,3910009,3910017,3910034,3910042,3910059,3910067,3910084,3910092,3910109,3910117,3910134,3910142,3910159,3910167,3910184,3910192,3910209,3910217,3910234,3910242,3910259,3910267,3910284,3910292,3910309,3910317,3910334,3910342,3910362,3910370],"lengths":[7,1,8,1,35,7,1,7,1,13,1,12,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,3,7,1,7,1,12,1,2,1,6,1,8,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,3]},"words":{"starts":[11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11002,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029,11029],"lengths":[26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22,22]}},"suspected":{"chars":{"starts":[5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,5405,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451,6451],"lengths":[83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,83,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74,74]},"words":{"starts":[875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,875,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220,1220],"lengths":[30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,30,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14,14]}}}}},"version":3}